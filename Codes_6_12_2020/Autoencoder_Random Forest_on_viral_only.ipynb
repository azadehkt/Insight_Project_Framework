{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing the host information including;age, location,gender and status for the possible prediction of outcome of recovery vs Death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "record_date\n",
      "virus_name\n",
      "accession_id\n",
      "type\n",
      "lineage\n",
      "passage_details_history\n",
      "collection_date\n",
      "location\n",
      "host\n",
      "additional_location_info\n",
      "gender\n",
      "age\n",
      "status\n",
      "specimen_source\n",
      "additional_host_information\n",
      "outbreak\n",
      "last_vaccinated\n",
      "treatment\n",
      "sequencing_technology\n",
      "assembly_method\n",
      "coverage\n",
      "comment\n",
      "originating_lab\n",
      "originating_lab_address\n",
      "sample_id_given_by_sample_provider\n",
      "submitting_lab\n",
      "submitting_lab_address\n",
      "sample_id_given_by_submitting_lab\n",
      "authors\n",
      "submitter\n",
      "submission_date\n",
      "submitter_address\n",
      "Query\n",
      "Strand\n",
      "%N\n",
      "Length(nt)\n",
      "Length(aa)\n",
      "#Muts\n",
      "%Muts\n",
      "#UniqueMuts\n",
      "%UniqueMuts\n",
      "#ExistingMuts\n",
      "%ExistingMuts\n",
      "Comment\n",
      "Symbol\n",
      "Reference\n",
      "UniqueMutList\n",
      "ExistingMutList\n",
      "Clade\n",
      "IfExistSpecialChar\n"
     ]
    }
   ],
   "source": [
    "# iterating the columns \n",
    "for col in data.columns: \n",
    "    print(col) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#important features\n",
    "start_data_row = 76\n",
    "Final_data_row = 255\n",
    "Data = data.loc[ start_data_row:Final_data_row , ['status','%N','Length(nt)','Length(aa)',\n",
    "                                                   '%Muts','%UniqueMuts','%ExistingMuts','ExistingMutList','Clade']]\n",
    "#change the index of the data according to the length of the new data\n",
    "Data.index = range(len(Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.04\n",
       "1      0.05\n",
       "2      0.13\n",
       "3      0.03\n",
       "4      0.03\n",
       "       ... \n",
       "173    0.04\n",
       "174    0.05\n",
       "175    0.05\n",
       "176    0.05\n",
       "177    0.01\n",
       "Name: %ExistingMuts, Length: 178, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing the % from the data\n",
    "Data['%N'] = Data['%N'].str.replace('%', ' ') \n",
    "Data['%Muts'] = Data['%Muts'].str.replace('%', ' ') \n",
    "Data['%UniqueMuts'] = Data['%UniqueMuts'].str.replace('%', ' ') \n",
    "Data['%ExistingMuts'] = Data['%ExistingMuts'].str.replace('%', ' ') \n",
    "Data['%N'].astype(float)\n",
    "Data['%Muts'].astype(float)\n",
    "Data['%UniqueMuts'].astype(float)\n",
    "Data['%ExistingMuts'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting rid of NaN data\n",
    "Data.dropna(subset = ['status'], inplace=True)\n",
    "Data.dropna(subset = ['ExistingMutList'], inplace=True)\n",
    "Data.index = range(len(Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chacking if there is any null data in ExistingMutList\n",
    "for i in range(len(Data)):\n",
    "    if pd.isnull(Data.ExistingMutList[i]) is True:\n",
    "        print('True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting rid of NaN data\n",
    "Data.drop(Data.loc[Data['status']=='unknown'].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#Labeling\n",
    "Data.replace(['Deceased'],value= [1], inplace=True)\n",
    "Data.status[Data['status'] != 1]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ExistingMutList_(NS3_A23V,NS3_G251V)</th>\n",
       "      <th>ExistingMutList_(NS3_G251V)</th>\n",
       "      <th>ExistingMutList_(NS8_L84S)</th>\n",
       "      <th>ExistingMutList_(NSP12_P323L)</th>\n",
       "      <th>ExistingMutList_(NSP12_P323L,NS8_L84S)</th>\n",
       "      <th>ExistingMutList_(NSP12_P323L,NSP12_A185V,Spike_D614G)</th>\n",
       "      <th>ExistingMutList_(NSP12_P323L,NSP12_A449V,Spike_D614G,N_G204R,N_R203K)</th>\n",
       "      <th>ExistingMutList_(NSP12_P323L,NSP12_T252N,Spike_D614G)</th>\n",
       "      <th>ExistingMutList_(NSP12_P323L,NSP12_T26I,Spike_D614G,M_D3G)</th>\n",
       "      <th>ExistingMutList_(NSP12_P323L,NSP12_V880I,Spike_D614G,Spike_E583D,NS3_Q57H,N_S194L)</th>\n",
       "      <th>...</th>\n",
       "      <th>Clade_G</th>\n",
       "      <th>Clade_Other</th>\n",
       "      <th>Clade_S</th>\n",
       "      <th>Clade_V</th>\n",
       "      <th>status</th>\n",
       "      <th>%N</th>\n",
       "      <th>Length(nt)</th>\n",
       "      <th>Length(aa)</th>\n",
       "      <th>%Muts</th>\n",
       "      <th>%UniqueMuts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29890.0</td>\n",
       "      <td>9710.0</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29849.0</td>\n",
       "      <td>9710.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.41</td>\n",
       "      <td>29899.0</td>\n",
       "      <td>9685.0</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29845.0</td>\n",
       "      <td>9710.0</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29834.0</td>\n",
       "      <td>9710.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 116 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ExistingMutList_(NS3_A23V,NS3_G251V)  ExistingMutList_(NS3_G251V)  \\\n",
       "0                                     0                            0   \n",
       "1                                     0                            0   \n",
       "2                                     0                            0   \n",
       "3                                     0                            0   \n",
       "4                                     0                            0   \n",
       "\n",
       "   ExistingMutList_(NS8_L84S)  ExistingMutList_(NSP12_P323L)  \\\n",
       "0                           0                              0   \n",
       "1                           0                              0   \n",
       "2                           0                              0   \n",
       "3                           0                              0   \n",
       "4                           0                              0   \n",
       "\n",
       "   ExistingMutList_(NSP12_P323L,NS8_L84S)  \\\n",
       "0                                       0   \n",
       "1                                       0   \n",
       "2                                       0   \n",
       "3                                       0   \n",
       "4                                       0   \n",
       "\n",
       "   ExistingMutList_(NSP12_P323L,NSP12_A185V,Spike_D614G)  \\\n",
       "0                                                  0       \n",
       "1                                                  0       \n",
       "2                                                  0       \n",
       "3                                                  0       \n",
       "4                                                  0       \n",
       "\n",
       "   ExistingMutList_(NSP12_P323L,NSP12_A449V,Spike_D614G,N_G204R,N_R203K)  \\\n",
       "0                                                  0                       \n",
       "1                                                  0                       \n",
       "2                                                  0                       \n",
       "3                                                  0                       \n",
       "4                                                  0                       \n",
       "\n",
       "   ExistingMutList_(NSP12_P323L,NSP12_T252N,Spike_D614G)  \\\n",
       "0                                                  0       \n",
       "1                                                  0       \n",
       "2                                                  0       \n",
       "3                                                  0       \n",
       "4                                                  0       \n",
       "\n",
       "   ExistingMutList_(NSP12_P323L,NSP12_T26I,Spike_D614G,M_D3G)  \\\n",
       "0                                                  0            \n",
       "1                                                  0            \n",
       "2                                                  0            \n",
       "3                                                  0            \n",
       "4                                                  0            \n",
       "\n",
       "   ExistingMutList_(NSP12_P323L,NSP12_V880I,Spike_D614G,Spike_E583D,NS3_Q57H,N_S194L)  \\\n",
       "0                                                  0                                    \n",
       "1                                                  0                                    \n",
       "2                                                  0                                    \n",
       "3                                                  0                                    \n",
       "4                                                  0                                    \n",
       "\n",
       "   ...  Clade_G  Clade_Other  Clade_S  Clade_V  status     %N  Length(nt)  \\\n",
       "0  ...        1            0        0        0       0  0.00      29890.0   \n",
       "1  ...        1            0        0        0       0  0.00      29849.0   \n",
       "2  ...        0            1        0        0       0  2.41      29899.0   \n",
       "3  ...        1            0        0        0       0  0.00      29845.0   \n",
       "4  ...        1            0        0        0       0  0.00      29834.0   \n",
       "\n",
       "   Length(aa)  %Muts  %UniqueMuts  \n",
       "0      9710.0  0.04         0.00   \n",
       "1      9710.0  0.05         0.00   \n",
       "2      9685.0  0.19         0.05   \n",
       "3      9710.0  0.04         0.01   \n",
       "4      9710.0  0.03         0.00   \n",
       "\n",
       "[5 rows x 116 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using dummies instead of location data for ML input\n",
    "Data_model = pd.concat([pd.get_dummies(Data[['ExistingMutList']]),pd.get_dummies(Data[['Clade']]), Data[['status','%N','Length(nt)','Length(aa)',\n",
    "                                                   '%Muts','%UniqueMuts']]], axis=1)\n",
    "Data_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    137\n",
       "1     32\n",
       "Name: status, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the balance in the data\n",
    "Data_model['status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    137\n",
       "1     32\n",
       "Name: status, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Separating target (label) data\n",
    "X = Data_model.drop('status',axis=1).astype(float)\n",
    "y = Data_model.status\n",
    "y=y.astype(int)\n",
    "y.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data to standarize them\n",
    "#Here caling reduced the iteration number from 200 to 50\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "sc = StandardScaler()\n",
    "#sc = MinMaxScaler()\n",
    "sc.fit(X)\n",
    "X_scaled = sc.transform(X)\n",
    "X_normal_scaled = X_scaled[y == 0] \n",
    "X_deseased_scaled = X_scaled[y == 1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 115)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 812       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 115)               920       \n",
      "=================================================================\n",
      "Total params: 1,732\n",
      "Trainable params: 1,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 109 samples, validate on 28 samples\n",
      "Epoch 1/300\n",
      "109/109 [==============================] - 0s 1ms/step - loss: 0.7149 - val_loss: 0.7071\n",
      "Epoch 2/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.7118 - val_loss: 0.7051\n",
      "Epoch 3/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.7091 - val_loss: 0.7028\n",
      "Epoch 4/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.7060 - val_loss: 0.7007\n",
      "Epoch 5/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.7032 - val_loss: 0.6984\n",
      "Epoch 6/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.7002 - val_loss: 0.6962\n",
      "Epoch 7/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.6973 - val_loss: 0.6939\n",
      "Epoch 8/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.6944 - val_loss: 0.6917\n",
      "Epoch 9/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.6916 - val_loss: 0.6894\n",
      "Epoch 10/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.6887 - val_loss: 0.6872\n",
      "Epoch 11/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.6859 - val_loss: 0.6850\n",
      "Epoch 12/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.6832 - val_loss: 0.6827\n",
      "Epoch 13/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.6804 - val_loss: 0.6805\n",
      "Epoch 14/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.6776 - val_loss: 0.6783\n",
      "Epoch 15/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.6749 - val_loss: 0.6761\n",
      "Epoch 16/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.6721 - val_loss: 0.6739\n",
      "Epoch 17/300\n",
      "109/109 [==============================] - 0s 73us/step - loss: 0.6694 - val_loss: 0.6717\n",
      "Epoch 18/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.6666 - val_loss: 0.6695\n",
      "Epoch 19/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.6639 - val_loss: 0.6673\n",
      "Epoch 20/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.6611 - val_loss: 0.6651\n",
      "Epoch 21/300\n",
      "109/109 [==============================] - 0s 156us/step - loss: 0.6584 - val_loss: 0.6629\n",
      "Epoch 22/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.6556 - val_loss: 0.6607\n",
      "Epoch 23/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.6529 - val_loss: 0.6585\n",
      "Epoch 24/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.6501 - val_loss: 0.6563\n",
      "Epoch 25/300\n",
      "109/109 [==============================] - 0s 64us/step - loss: 0.6472 - val_loss: 0.6541\n",
      "Epoch 26/300\n",
      "109/109 [==============================] - 0s 64us/step - loss: 0.6444 - val_loss: 0.6519\n",
      "Epoch 27/300\n",
      "109/109 [==============================] - 0s 73us/step - loss: 0.6416 - val_loss: 0.6497\n",
      "Epoch 28/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.6387 - val_loss: 0.6475\n",
      "Epoch 29/300\n",
      "109/109 [==============================] - 0s 64us/step - loss: 0.6358 - val_loss: 0.6453\n",
      "Epoch 30/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.6328 - val_loss: 0.6431\n",
      "Epoch 31/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.6299 - val_loss: 0.6409\n",
      "Epoch 32/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.6269 - val_loss: 0.6387\n",
      "Epoch 33/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.6239 - val_loss: 0.6364\n",
      "Epoch 34/300\n",
      "109/109 [==============================] - 0s 73us/step - loss: 0.6209 - val_loss: 0.6342\n",
      "Epoch 35/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.6178 - val_loss: 0.6320\n",
      "Epoch 36/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.6147 - val_loss: 0.6297\n",
      "Epoch 37/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.6115 - val_loss: 0.6275\n",
      "Epoch 38/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.6083 - val_loss: 0.6252\n",
      "Epoch 39/300\n",
      "109/109 [==============================] - 0s 82us/step - loss: 0.6051 - val_loss: 0.6230\n",
      "Epoch 40/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.6018 - val_loss: 0.6208\n",
      "Epoch 41/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.5986 - val_loss: 0.6185\n",
      "Epoch 42/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.5952 - val_loss: 0.6163\n",
      "Epoch 43/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.5919 - val_loss: 0.6140\n",
      "Epoch 44/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.5884 - val_loss: 0.6118\n",
      "Epoch 45/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.5850 - val_loss: 0.6095\n",
      "Epoch 46/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.5815 - val_loss: 0.6072\n",
      "Epoch 47/300\n",
      "109/109 [==============================] - 0s 73us/step - loss: 0.5779 - val_loss: 0.6050\n",
      "Epoch 48/300\n",
      "109/109 [==============================] - 0s 73us/step - loss: 0.5744 - val_loss: 0.6027\n",
      "Epoch 49/300\n",
      "109/109 [==============================] - 0s 73us/step - loss: 0.5708 - val_loss: 0.6004\n",
      "Epoch 50/300\n",
      "109/109 [==============================] - 0s 73us/step - loss: 0.5672 - val_loss: 0.5981\n",
      "Epoch 51/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.5635 - val_loss: 0.5959\n",
      "Epoch 52/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.5598 - val_loss: 0.5936\n",
      "Epoch 53/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.5560 - val_loss: 0.5913\n",
      "Epoch 54/300\n",
      "109/109 [==============================] - 0s 64us/step - loss: 0.5522 - val_loss: 0.5890\n",
      "Epoch 55/300\n",
      "109/109 [==============================] - 0s 82us/step - loss: 0.5483 - val_loss: 0.5867\n",
      "Epoch 56/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.5444 - val_loss: 0.5844\n",
      "Epoch 57/300\n",
      "109/109 [==============================] - 0s 110us/step - loss: 0.5405 - val_loss: 0.5821\n",
      "Epoch 58/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.5366 - val_loss: 0.5798\n",
      "Epoch 59/300\n",
      "109/109 [==============================] - 0s 73us/step - loss: 0.5326 - val_loss: 0.5775\n",
      "Epoch 60/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.5286 - val_loss: 0.5752\n",
      "Epoch 61/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.5245 - val_loss: 0.5729\n",
      "Epoch 62/300\n",
      "109/109 [==============================] - 0s 64us/step - loss: 0.5204 - val_loss: 0.5706\n",
      "Epoch 63/300\n",
      "109/109 [==============================] - 0s 44us/step - loss: 0.5163 - val_loss: 0.5683\n",
      "Epoch 64/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.5121 - val_loss: 0.5660\n",
      "Epoch 65/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.5080 - val_loss: 0.5637\n",
      "Epoch 66/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.5038 - val_loss: 0.5614\n",
      "Epoch 67/300\n",
      "109/109 [==============================] - 0s 82us/step - loss: 0.4995 - val_loss: 0.5591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.4952 - val_loss: 0.5568\n",
      "Epoch 69/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.4909 - val_loss: 0.5545\n",
      "Epoch 70/300\n",
      "109/109 [==============================] - 0s 36us/step - loss: 0.4866 - val_loss: 0.5522\n",
      "Epoch 71/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.4822 - val_loss: 0.5499\n",
      "Epoch 72/300\n",
      "109/109 [==============================] - 0s 83us/step - loss: 0.4778 - val_loss: 0.5476\n",
      "Epoch 73/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.4734 - val_loss: 0.5453\n",
      "Epoch 74/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.4690 - val_loss: 0.5430\n",
      "Epoch 75/300\n",
      "109/109 [==============================] - 0s 82us/step - loss: 0.4646 - val_loss: 0.5407\n",
      "Epoch 76/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.4601 - val_loss: 0.5385\n",
      "Epoch 77/300\n",
      "109/109 [==============================] - 0s 110us/step - loss: 0.4556 - val_loss: 0.5362\n",
      "Epoch 78/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.4511 - val_loss: 0.5340\n",
      "Epoch 79/300\n",
      "109/109 [==============================] - 0s 36us/step - loss: 0.4465 - val_loss: 0.5318\n",
      "Epoch 80/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.4420 - val_loss: 0.5295\n",
      "Epoch 81/300\n",
      "109/109 [==============================] - 0s 101us/step - loss: 0.4375 - val_loss: 0.5273\n",
      "Epoch 82/300\n",
      "109/109 [==============================] - 0s 147us/step - loss: 0.4329 - val_loss: 0.5251\n",
      "Epoch 83/300\n",
      "109/109 [==============================] - 0s 64us/step - loss: 0.4283 - val_loss: 0.5229\n",
      "Epoch 84/300\n",
      "109/109 [==============================] - 0s 64us/step - loss: 0.4237 - val_loss: 0.5207\n",
      "Epoch 85/300\n",
      "109/109 [==============================] - 0s 110us/step - loss: 0.4191 - val_loss: 0.5185\n",
      "Epoch 86/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.4144 - val_loss: 0.5163\n",
      "Epoch 87/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.4098 - val_loss: 0.5141\n",
      "Epoch 88/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.4052 - val_loss: 0.5119\n",
      "Epoch 89/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.4005 - val_loss: 0.5097\n",
      "Epoch 90/300\n",
      "109/109 [==============================] - 0s 128us/step - loss: 0.3958 - val_loss: 0.5076\n",
      "Epoch 91/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.3911 - val_loss: 0.5054\n",
      "Epoch 92/300\n",
      "109/109 [==============================] - 0s 110us/step - loss: 0.3865 - val_loss: 0.5032\n",
      "Epoch 93/300\n",
      "109/109 [==============================] - 0s 128us/step - loss: 0.3818 - val_loss: 0.5011\n",
      "Epoch 94/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.3771 - val_loss: 0.4989\n",
      "Epoch 95/300\n",
      "109/109 [==============================] - 0s 64us/step - loss: 0.3724 - val_loss: 0.4968\n",
      "Epoch 96/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.3677 - val_loss: 0.4946\n",
      "Epoch 97/300\n",
      "109/109 [==============================] - 0s 82us/step - loss: 0.3630 - val_loss: 0.4925\n",
      "Epoch 98/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.3582 - val_loss: 0.4904\n",
      "Epoch 99/300\n",
      "109/109 [==============================] - 0s 82us/step - loss: 0.3535 - val_loss: 0.4883\n",
      "Epoch 100/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.3488 - val_loss: 0.4863\n",
      "Epoch 101/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.3442 - val_loss: 0.4842\n",
      "Epoch 102/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.3395 - val_loss: 0.4821\n",
      "Epoch 103/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.3348 - val_loss: 0.4801\n",
      "Epoch 104/300\n",
      "109/109 [==============================] - 0s 64us/step - loss: 0.3301 - val_loss: 0.4780\n",
      "Epoch 105/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.3254 - val_loss: 0.4759\n",
      "Epoch 106/300\n",
      "109/109 [==============================] - 0s 73us/step - loss: 0.3207 - val_loss: 0.4739\n",
      "Epoch 107/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.3160 - val_loss: 0.4718\n",
      "Epoch 108/300\n",
      "109/109 [==============================] - 0s 110us/step - loss: 0.3113 - val_loss: 0.4698\n",
      "Epoch 109/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.3066 - val_loss: 0.4678\n",
      "Epoch 110/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.3019 - val_loss: 0.4658\n",
      "Epoch 111/300\n",
      "109/109 [==============================] - 0s 82us/step - loss: 0.2972 - val_loss: 0.4639\n",
      "Epoch 112/300\n",
      "109/109 [==============================] - 0s 64us/step - loss: 0.2925 - val_loss: 0.4619\n",
      "Epoch 113/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.2879 - val_loss: 0.4600\n",
      "Epoch 114/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.2832 - val_loss: 0.4581\n",
      "Epoch 115/300\n",
      "109/109 [==============================] - 0s 82us/step - loss: 0.2786 - val_loss: 0.4562\n",
      "Epoch 116/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.2740 - val_loss: 0.4543\n",
      "Epoch 117/300\n",
      "109/109 [==============================] - 0s 110us/step - loss: 0.2694 - val_loss: 0.4525\n",
      "Epoch 118/300\n",
      "109/109 [==============================] - 0s 64us/step - loss: 0.2648 - val_loss: 0.4507\n",
      "Epoch 119/300\n",
      "109/109 [==============================] - 0s 101us/step - loss: 0.2602 - val_loss: 0.4489\n",
      "Epoch 120/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.2556 - val_loss: 0.4471\n",
      "Epoch 121/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.2510 - val_loss: 0.4454\n",
      "Epoch 122/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.2465 - val_loss: 0.4436\n",
      "Epoch 123/300\n",
      "109/109 [==============================] - 0s 36us/step - loss: 0.2419 - val_loss: 0.4419\n",
      "Epoch 124/300\n",
      "109/109 [==============================] - 0s 137us/step - loss: 0.2374 - val_loss: 0.4402\n",
      "Epoch 125/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.2329 - val_loss: 0.4385\n",
      "Epoch 126/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.2284 - val_loss: 0.4369\n",
      "Epoch 127/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.2239 - val_loss: 0.4352\n",
      "Epoch 128/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.2194 - val_loss: 0.4336\n",
      "Epoch 129/300\n",
      "109/109 [==============================] - 0s 82us/step - loss: 0.2149 - val_loss: 0.4319\n",
      "Epoch 130/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.2105 - val_loss: 0.4303\n",
      "Epoch 131/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.2061 - val_loss: 0.4287\n",
      "Epoch 132/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.2017 - val_loss: 0.4271\n",
      "Epoch 133/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.1973 - val_loss: 0.4256\n",
      "Epoch 134/300\n",
      "109/109 [==============================] - 0s 82us/step - loss: 0.1929 - val_loss: 0.4240\n",
      "Epoch 135/300\n",
      "109/109 [==============================] - 0s 64us/step - loss: 0.1885 - val_loss: 0.4225\n",
      "Epoch 136/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.1842 - val_loss: 0.4210\n",
      "Epoch 137/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.1798 - val_loss: 0.4195\n",
      "Epoch 138/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.1755 - val_loss: 0.4180\n",
      "Epoch 139/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.1712 - val_loss: 0.4165\n",
      "Epoch 140/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.1669 - val_loss: 0.4150\n",
      "Epoch 141/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.1626 - val_loss: 0.4136\n",
      "Epoch 142/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.1584 - val_loss: 0.4122\n",
      "Epoch 143/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.1541 - val_loss: 0.4108\n",
      "Epoch 144/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.1499 - val_loss: 0.4094\n",
      "Epoch 145/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.1457 - val_loss: 0.4080\n",
      "Epoch 146/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.1415 - val_loss: 0.4066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 147/300\n",
      "109/109 [==============================] - 0s 73us/step - loss: 0.1373 - val_loss: 0.4053\n",
      "Epoch 148/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.1331 - val_loss: 0.4040\n",
      "Epoch 149/300\n",
      "109/109 [==============================] - 0s 64us/step - loss: 0.1290 - val_loss: 0.4027\n",
      "Epoch 150/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.1248 - val_loss: 0.4014\n",
      "Epoch 151/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.1207 - val_loss: 0.4001\n",
      "Epoch 152/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.1166 - val_loss: 0.3989\n",
      "Epoch 153/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.1125 - val_loss: 0.3976\n",
      "Epoch 154/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.1084 - val_loss: 0.3964\n",
      "Epoch 155/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.1044 - val_loss: 0.3952\n",
      "Epoch 156/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.1003 - val_loss: 0.3940\n",
      "Epoch 157/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.0963 - val_loss: 0.3928\n",
      "Epoch 158/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.0922 - val_loss: 0.3916\n",
      "Epoch 159/300\n",
      "109/109 [==============================] - 0s 92us/step - loss: 0.0882 - val_loss: 0.3904\n",
      "Epoch 160/300\n",
      "109/109 [==============================] - 0s 64us/step - loss: 0.0842 - val_loss: 0.3893\n",
      "Epoch 161/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.0802 - val_loss: 0.3881\n",
      "Epoch 162/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.0762 - val_loss: 0.3870\n",
      "Epoch 163/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.0723 - val_loss: 0.3858\n",
      "Epoch 164/300\n",
      "109/109 [==============================] - 0s 64us/step - loss: 0.0683 - val_loss: 0.3847\n",
      "Epoch 165/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.0643 - val_loss: 0.3836\n",
      "Epoch 166/300\n",
      "109/109 [==============================] - 0s 87us/step - loss: 0.0604 - val_loss: 0.3825\n",
      "Epoch 167/300\n",
      "109/109 [==============================] - 0s 73us/step - loss: 0.0565 - val_loss: 0.3815\n",
      "Epoch 168/300\n",
      "109/109 [==============================] - 0s 73us/step - loss: 0.0525 - val_loss: 0.3804\n",
      "Epoch 169/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.0486 - val_loss: 0.3794\n",
      "Epoch 170/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.0447 - val_loss: 0.3784\n",
      "Epoch 171/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.0408 - val_loss: 0.3773\n",
      "Epoch 172/300\n",
      "109/109 [==============================] - 0s 82us/step - loss: 0.0369 - val_loss: 0.3763\n",
      "Epoch 173/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.0330 - val_loss: 0.3753\n",
      "Epoch 174/300\n",
      "109/109 [==============================] - 0s 82us/step - loss: 0.0292 - val_loss: 0.3743\n",
      "Epoch 175/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.0253 - val_loss: 0.3733\n",
      "Epoch 176/300\n",
      "109/109 [==============================] - 0s 67us/step - loss: 0.0214 - val_loss: 0.3723\n",
      "Epoch 177/300\n",
      "109/109 [==============================] - 0s 64us/step - loss: 0.0176 - val_loss: 0.3714\n",
      "Epoch 178/300\n",
      "109/109 [==============================] - 0s 101us/step - loss: 0.0137 - val_loss: 0.3704\n",
      "Epoch 179/300\n",
      "109/109 [==============================] - 0s 64us/step - loss: 0.0099 - val_loss: 0.3694\n",
      "Epoch 180/300\n",
      "109/109 [==============================] - 0s 101us/step - loss: 0.0061 - val_loss: 0.3685\n",
      "Epoch 181/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.0022 - val_loss: 0.3675\n",
      "Epoch 182/300\n",
      "109/109 [==============================] - 0s 101us/step - loss: -0.0016 - val_loss: 0.3666\n",
      "Epoch 183/300\n",
      "109/109 [==============================] - 0s 64us/step - loss: -0.0054 - val_loss: 0.3656\n",
      "Epoch 184/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: -0.0092 - val_loss: 0.3647\n",
      "Epoch 185/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: -0.0129 - val_loss: 0.3638\n",
      "Epoch 186/300\n",
      "109/109 [==============================] - 0s 36us/step - loss: -0.0167 - val_loss: 0.3629\n",
      "Epoch 187/300\n",
      "109/109 [==============================] - 0s 119us/step - loss: -0.0205 - val_loss: 0.3620\n",
      "Epoch 188/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: -0.0243 - val_loss: 0.3611\n",
      "Epoch 189/300\n",
      "109/109 [==============================] - 0s 82us/step - loss: -0.0280 - val_loss: 0.3602\n",
      "Epoch 190/300\n",
      "109/109 [==============================] - 0s 64us/step - loss: -0.0318 - val_loss: 0.3593\n",
      "Epoch 191/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: -0.0355 - val_loss: 0.3585\n",
      "Epoch 192/300\n",
      "109/109 [==============================] - 0s 64us/step - loss: -0.0393 - val_loss: 0.3576\n",
      "Epoch 193/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: -0.0430 - val_loss: 0.3567\n",
      "Epoch 194/300\n",
      "109/109 [==============================] - 0s 124us/step - loss: -0.0467 - val_loss: 0.3559\n",
      "Epoch 195/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: -0.0505 - val_loss: 0.3550\n",
      "Epoch 196/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: -0.0542 - val_loss: 0.3541\n",
      "Epoch 197/300\n",
      "109/109 [==============================] - 0s 64us/step - loss: -0.0579 - val_loss: 0.3533\n",
      "Epoch 198/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: -0.0616 - val_loss: 0.3524\n",
      "Epoch 199/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: -0.0653 - val_loss: 0.3516\n",
      "Epoch 200/300\n",
      "109/109 [==============================] - 0s 82us/step - loss: -0.0690 - val_loss: 0.3508\n",
      "Epoch 201/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: -0.0727 - val_loss: 0.3500\n",
      "Epoch 202/300\n",
      "109/109 [==============================] - 0s 82us/step - loss: -0.0764 - val_loss: 0.3492\n",
      "Epoch 203/300\n",
      "109/109 [==============================] - 0s 64us/step - loss: -0.0801 - val_loss: 0.3483\n",
      "Epoch 204/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: -0.0838 - val_loss: 0.3475\n",
      "Epoch 205/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: -0.0875 - val_loss: 0.3468\n",
      "Epoch 206/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: -0.0912 - val_loss: 0.3460\n",
      "Epoch 207/300\n",
      "109/109 [==============================] - 0s 64us/step - loss: -0.0949 - val_loss: 0.3452\n",
      "Epoch 208/300\n",
      "109/109 [==============================] - 0s 82us/step - loss: -0.0985 - val_loss: 0.3444\n",
      "Epoch 209/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: -0.1022 - val_loss: 0.3437\n",
      "Epoch 210/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: -0.1059 - val_loss: 0.3429\n",
      "Epoch 211/300\n",
      "109/109 [==============================] - 0s 101us/step - loss: -0.1096 - val_loss: 0.3421\n",
      "Epoch 212/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: -0.1132 - val_loss: 0.3414\n",
      "Epoch 213/300\n",
      "109/109 [==============================] - 0s 73us/step - loss: -0.1169 - val_loss: 0.3406\n",
      "Epoch 214/300\n",
      "109/109 [==============================] - 0s 64us/step - loss: -0.1206 - val_loss: 0.3399\n",
      "Epoch 215/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: -0.1242 - val_loss: 0.3392\n",
      "Epoch 216/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: -0.1279 - val_loss: 0.3384\n",
      "Epoch 217/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: -0.1316 - val_loss: 0.3377\n",
      "Epoch 218/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: -0.1352 - val_loss: 0.3370\n",
      "Epoch 219/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: -0.1389 - val_loss: 0.3362\n",
      "Epoch 220/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: -0.1425 - val_loss: 0.3355\n",
      "Epoch 221/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: -0.1462 - val_loss: 0.3347\n",
      "Epoch 222/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: -0.1499 - val_loss: 0.3340\n",
      "Epoch 223/300\n",
      "109/109 [==============================] - 0s 92us/step - loss: -0.1535 - val_loss: 0.3332\n",
      "Epoch 224/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: -0.1572 - val_loss: 0.3324\n",
      "Epoch 225/300\n",
      "109/109 [==============================] - 0s 82us/step - loss: -0.1608 - val_loss: 0.3317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 226/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: -0.1645 - val_loss: 0.3309\n",
      "Epoch 227/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: -0.1682 - val_loss: 0.3302\n",
      "Epoch 228/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: -0.1718 - val_loss: 0.3295\n",
      "Epoch 229/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: -0.1756 - val_loss: 0.3287\n",
      "Epoch 230/300\n",
      "109/109 [==============================] - 0s 64us/step - loss: -0.1792 - val_loss: 0.3280\n",
      "Epoch 231/300\n",
      "109/109 [==============================] - 0s 64us/step - loss: -0.1829 - val_loss: 0.3273\n",
      "Epoch 232/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: -0.1866 - val_loss: 0.3265\n",
      "Epoch 233/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: -0.1903 - val_loss: 0.3258\n",
      "Epoch 234/300\n",
      "109/109 [==============================] - 0s 91us/step - loss: -0.1939 - val_loss: 0.3251\n",
      "Epoch 235/300\n",
      "109/109 [==============================] - 0s 101us/step - loss: -0.1977 - val_loss: 0.3244\n",
      "Epoch 236/300\n",
      "109/109 [==============================] - 0s 64us/step - loss: -0.2014 - val_loss: 0.3237\n",
      "Epoch 237/300\n",
      "109/109 [==============================] - 0s 92us/step - loss: -0.2051 - val_loss: 0.3229\n",
      "Epoch 238/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: -0.2087 - val_loss: 0.3222\n",
      "Epoch 239/300\n",
      "109/109 [==============================] - 0s 110us/step - loss: -0.2125 - val_loss: 0.3215\n",
      "Epoch 240/300\n",
      "109/109 [==============================] - 0s 110us/step - loss: -0.2163 - val_loss: 0.3208\n",
      "Epoch 241/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: -0.2199 - val_loss: 0.3201\n",
      "Epoch 242/300\n",
      "109/109 [==============================] - 0s 92us/step - loss: -0.2236 - val_loss: 0.3194\n",
      "Epoch 243/300\n",
      "109/109 [==============================] - 0s 64us/step - loss: -0.2275 - val_loss: 0.3187\n",
      "Epoch 244/300\n",
      "109/109 [==============================] - 0s 64us/step - loss: -0.2311 - val_loss: 0.3180\n",
      "Epoch 245/300\n",
      "109/109 [==============================] - 0s 119us/step - loss: -0.2348 - val_loss: 0.3173\n",
      "Epoch 246/300\n",
      "109/109 [==============================] - 0s 64us/step - loss: -0.2386 - val_loss: 0.3167\n",
      "Epoch 247/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: -0.2424 - val_loss: 0.3160\n",
      "Epoch 248/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: -0.2459 - val_loss: 0.3153\n",
      "Epoch 249/300\n",
      "109/109 [==============================] - 0s 73us/step - loss: -0.2495 - val_loss: 0.3146\n",
      "Epoch 250/300\n",
      "109/109 [==============================] - 0s 82us/step - loss: -0.2535 - val_loss: 0.3139\n",
      "Epoch 251/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: -0.2575 - val_loss: 0.3132\n",
      "Epoch 252/300\n",
      "109/109 [==============================] - 0s 82us/step - loss: -0.2606 - val_loss: 0.3126\n",
      "Epoch 253/300\n",
      "109/109 [==============================] - 0s 119us/step - loss: -0.2644 - val_loss: 0.3119\n",
      "Epoch 254/300\n",
      "109/109 [==============================] - 0s 64us/step - loss: -0.2679 - val_loss: 0.3112\n",
      "Epoch 255/300\n",
      "109/109 [==============================] - 0s 82us/step - loss: -0.2717 - val_loss: 0.3105\n",
      "Epoch 256/300\n",
      "109/109 [==============================] - 0s 73us/step - loss: -0.2754 - val_loss: 0.3098\n",
      "Epoch 257/300\n",
      "109/109 [==============================] - 0s 82us/step - loss: -0.2791 - val_loss: 0.3091\n",
      "Epoch 258/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: -0.2822 - val_loss: 0.3084\n",
      "Epoch 259/300\n",
      "109/109 [==============================] - 0s 73us/step - loss: -0.2858 - val_loss: 0.3077\n",
      "Epoch 260/300\n",
      "109/109 [==============================] - 0s 73us/step - loss: -0.2897 - val_loss: 0.3070\n",
      "Epoch 261/300\n",
      "109/109 [==============================] - 0s 73us/step - loss: -0.2930 - val_loss: 0.3063\n",
      "Epoch 262/300\n",
      "109/109 [==============================] - 0s 64us/step - loss: -0.2968 - val_loss: 0.3056\n",
      "Epoch 263/300\n",
      "109/109 [==============================] - 0s 101us/step - loss: -0.3005 - val_loss: 0.3048\n",
      "Epoch 264/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: -0.3039 - val_loss: 0.3041\n",
      "Epoch 265/300\n",
      "109/109 [==============================] - 0s 101us/step - loss: -0.3073 - val_loss: 0.3034\n",
      "Epoch 266/300\n",
      "109/109 [==============================] - 0s 64us/step - loss: -0.3108 - val_loss: 0.3026\n",
      "Epoch 267/300\n",
      "109/109 [==============================] - 0s 73us/step - loss: -0.3148 - val_loss: 0.3019\n",
      "Epoch 268/300\n",
      "109/109 [==============================] - 0s 73us/step - loss: -0.3182 - val_loss: 0.3011\n",
      "Epoch 269/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: -0.3213 - val_loss: 0.3004\n",
      "Epoch 270/300\n",
      "109/109 [==============================] - 0s 119us/step - loss: -0.3247 - val_loss: 0.2996\n",
      "Epoch 271/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: -0.3285 - val_loss: 0.2989\n",
      "Epoch 272/300\n",
      "109/109 [==============================] - 0s 73us/step - loss: -0.3319 - val_loss: 0.2981\n",
      "Epoch 273/300\n",
      "109/109 [==============================] - 0s 73us/step - loss: -0.3354 - val_loss: 0.2973\n",
      "Epoch 274/300\n",
      "109/109 [==============================] - 0s 73us/step - loss: -0.3389 - val_loss: 0.2965\n",
      "Epoch 275/300\n",
      "109/109 [==============================] - 0s 73us/step - loss: -0.3424 - val_loss: 0.2957\n",
      "Epoch 276/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: -0.3455 - val_loss: 0.2950\n",
      "Epoch 277/300\n",
      "109/109 [==============================] - 0s 156us/step - loss: -0.3494 - val_loss: 0.2942\n",
      "Epoch 278/300\n",
      "109/109 [==============================] - 0s 110us/step - loss: -0.3529 - val_loss: 0.2934\n",
      "Epoch 279/300\n",
      "109/109 [==============================] - 0s 101us/step - loss: -0.3564 - val_loss: 0.2926\n",
      "Epoch 280/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: -0.3600 - val_loss: 0.2918\n",
      "Epoch 281/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: -0.3635 - val_loss: 0.2909\n",
      "Epoch 282/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: -0.3671 - val_loss: 0.2901\n",
      "Epoch 283/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: -0.3706 - val_loss: 0.2893\n",
      "Epoch 284/300\n",
      "109/109 [==============================] - 0s 73us/step - loss: -0.3742 - val_loss: 0.2885\n",
      "Epoch 285/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: -0.3778 - val_loss: 0.2877\n",
      "Epoch 286/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: -0.3814 - val_loss: 0.2869\n",
      "Epoch 287/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: -0.3849 - val_loss: 0.2861\n",
      "Epoch 288/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: -0.3886 - val_loss: 0.2853\n",
      "Epoch 289/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: -0.3922 - val_loss: 0.2845\n",
      "Epoch 290/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: -0.3958 - val_loss: 0.2837\n",
      "Epoch 291/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: -0.3994 - val_loss: 0.2829\n",
      "Epoch 292/300\n",
      "109/109 [==============================] - 0s 110us/step - loss: -0.4030 - val_loss: 0.2820\n",
      "Epoch 293/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: -0.4067 - val_loss: 0.2812\n",
      "Epoch 294/300\n",
      "109/109 [==============================] - 0s 82us/step - loss: -0.4103 - val_loss: 0.2804\n",
      "Epoch 295/300\n",
      "109/109 [==============================] - 0s 73us/step - loss: -0.4140 - val_loss: 0.2795\n",
      "Epoch 296/300\n",
      "109/109 [==============================] - 0s 73us/step - loss: -0.4177 - val_loss: 0.2786\n",
      "Epoch 297/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: -0.4213 - val_loss: 0.2778\n",
      "Epoch 298/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: -0.4250 - val_loss: 0.2769\n",
      "Epoch 299/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: -0.4287 - val_loss: 0.2761\n",
      "Epoch 300/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: -0.4324 - val_loss: 0.2752\n"
     ]
    }
   ],
   "source": [
    "## Simple feedforward autoencoder\n",
    "\n",
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "window_length = 115\n",
    "encoding_dim = 7\n",
    "epochs = 300\n",
    "\n",
    "X_simple = X.values.reshape((len(X), np.prod(X.shape[1:])))\n",
    "\n",
    "# this is our input placeholder\n",
    "input_window = Input(shape=(window_length,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_window)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(window_length, activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_window, decoded)\n",
    "\n",
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_window, encoded)\n",
    "\n",
    "\n",
    "autoencoder.summary()\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "history = autoencoder.fit(X_normal_scaled, X_normal_scaled,\n",
    "                epochs=epochs,\n",
    "                batch_size=1024,\n",
    "                shuffle=True,\n",
    "                validation_split = 0.20)\n",
    "\n",
    "# Separating the points encoded by the Auto-encoder as normal and fraud \n",
    "decoded_X_normal = autoencoder.predict(X_normal_scaled)\n",
    "decoded_X_deseased = autoencoder.predict(X_deseased_scaled)\n",
    "# Combining the encoded points into a single table  \n",
    "encoded_X = np.append(decoded_X_normal, decoded_X_deseased, axis = 0) \n",
    "y_normal = np.zeros(decoded_X_normal.shape[0]) \n",
    "y_deceased = np.ones(decoded_X_deseased.shape[0]) \n",
    "encoded_y = np.append(y_normal, y_deceased) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting features and the label: 20% test data and 80% assigned to training data\n",
    "# split into train/test sets with same class ratio\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.bagging module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#balancing the data\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "# define oversampling strategy\n",
    "over = RandomOverSampler(sampling_strategy='minority')\n",
    "# fit and apply the transform\n",
    "X_train, y_train = over.fit_resample(X_train, y_train)\n",
    "# define undersampling strategy\n",
    "under = RandomUnderSampler(sampling_strategy='majority')\n",
    "# fit and apply the transform\n",
    "X_train, y_train = under.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 _train= 0.9908256880733946\n",
      "R^2 _test= 0.9411764705882353\n"
     ]
    }
   ],
   "source": [
    "#fitting the model and get the conversion probabilities. \n",
    "#predit_proba() function of our model assigns probability for each row:\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create the model with 100 trees\n",
    "model = RandomForestClassifier(n_estimators=30,max_depth = 60, max_features = 'log2')\n",
    "# Fit on training data\n",
    "model.fit(X_train,y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "RF_probs = model.predict_proba(X_test)[:,1]\n",
    "#Return the mean accuracy on the given test data and taraining data to see if we have overfitting.score clculates R^2\n",
    "print('R^2 _train=',model.score(X_train, y_train))\n",
    "print('R^2 _test=',model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy Scores [1.         1.         1.         0.95454545 0.90909091 0.95454545\n",
      " 0.95454545 1.         1.         0.95238095]\n",
      "CV-scores_min =  0.9090909090909091\n",
      "CV_scores_mean = 0.9725108225108225\n",
      "CV_scores_max = 1.0\n"
     ]
    }
   ],
   "source": [
    "#Cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(model, X_train, y_train, cv=10)\n",
    "print('Cross-Validation Accuracy Scores', scores)\n",
    "scores = pd.Series(scores)\n",
    "print('CV-scores_min = ',scores.min())\n",
    "print('CV_scores_mean =', scores.mean())\n",
    "print('CV_scores_max =', scores.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Actual  Predicted\n",
       "113       1          0\n",
       "1         0          0\n",
       "91        0          0\n",
       "77        1          1\n",
       "28        0          0\n",
       "51        0          0\n",
       "153       1          1\n",
       "127       0          0\n",
       "95        0          0\n",
       "151       1          1\n",
       "88        0          0\n",
       "130       0          0\n",
       "100       0          0\n",
       "86        0          0\n",
       "44        1          1\n",
       "123       0          0\n",
       "137       0          0\n",
       "43        1          0\n",
       "78        0          0\n",
       "31        0          0\n",
       "90        0          0\n",
       "97        0          0\n",
       "72        0          0\n",
       "57        0          0\n",
       "93        0          0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'Actual': y_test, 'Predicted': y_hat})\n",
    "df.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance measurement metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.058823529411764705\n",
      "Mean Squared Error: 0.058823529411764705\n",
      "Root Mean Squared Error: 0.24253562503633297\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_hat))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_hat))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc = 0.9404761904761905\n"
     ]
    }
   ],
   "source": [
    "#Area Under ROC Curve (AUROC) metric\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# Keep only the positive class\n",
    "#lr_probs = [p[1] for p in lr_probs]\n",
    "print( 'roc_auc =', roc_auc_score(y_test, RF_probs) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Skill: ROC AUC=0.500\n",
      "Logistic: ROC AUC=0.940\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "ns_probs = [0 for _ in range(len(y_test))]\n",
    "# predict probabilities\n",
    "RF_probs = model.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "RF_probs = RF_probs[:, 1]\n",
    "# calculate scores\n",
    "ns_auc = roc_auc_score(y_test, ns_probs)\n",
    "RF_auc = roc_auc_score(y_test, RF_probs)\n",
    "# summarize scores\n",
    "print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "print('Logistic: ROC AUC=%.3f' % (RF_auc))\n",
    "\n",
    "\n",
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
    "RF_fpr, RF_tpr, _ = roc_curve(y_test, RF_probs)\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill',  linewidth=2,  color = '#333F4B')\n",
    "pyplot.plot(RF_fpr, RF_tpr, marker='.', label='Random Forest', linewidth=2, color = 'darkred')\n",
    "# axis labels\n",
    "pyplot.xlabel('False Positive Rate',fontsize=15)\n",
    "pyplot.ylabel('True Positive Rate',fontsize=15)\n",
    "# show the legend\n",
    "pyplot.legend(fontsize=15)\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

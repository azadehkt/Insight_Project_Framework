{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing the host information including;age, location,gender and status for the possible prediction of outcome of recovery vs Death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "virus_name\n",
      "accession_id\n",
      "type\n",
      "lineage\n",
      "passage_details_history\n",
      "collection_date\n",
      "location\n",
      "host\n",
      "additional_location_information\n",
      "gender\n",
      "age\n",
      "status\n",
      "specimen_source\n",
      "additional_host_information\n",
      "outbreak\n",
      "last_vaccinated\n",
      "treatment\n",
      "sequencing_technology\n",
      "assembly_method\n",
      "coverage\n",
      "comment\n",
      "originating_lab\n",
      "address\n",
      "sample_id_given_by_the_sample_provider\n",
      "submitting_lab\n",
      "sample_id_given_by_the_submitting_laboratory\n",
      "authors\n",
      "submitter\n",
      "submission_date\n",
      "query\n",
      "strand\n",
      "%n\n",
      "length(nt)\n",
      "length(aa)\n",
      "#muts\n",
      "%muts\n",
      "#uniquemuts\n",
      "%uniquemuts\n",
      "#existingmuts\n",
      "%existingmuts\n",
      "symbol\n",
      "reference\n",
      "uniquemutlist\n",
      "existingmutlist\n",
      "clade\n",
      "ifexistspecialchar\n"
     ]
    }
   ],
   "source": [
    "# iterating the columns \n",
    "for col in data.columns: \n",
    "    print(col) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#important features\n",
    "start_data_row = 0\n",
    "Final_data_row = 1129\n",
    "Data = data.loc[start_data_row:Final_data_row, ['status','%n','length(nt)','length(aa)',\n",
    "                                                   '%muts','%uniquemuts','%existingmuts','existingmutlist','clade']]\n",
    "#change the index of the data according to the length of the new data\n",
    "Data.index = range(len(Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.05\n",
       "1       0.04\n",
       "2       0.06\n",
       "3       0.03\n",
       "4       0.02\n",
       "        ... \n",
       "1123    0.05\n",
       "1124    0.05\n",
       "1125    0.05\n",
       "1126    0.07\n",
       "1127    0.06\n",
       "Name: %existingmuts, Length: 1128, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing the % from the data\n",
    "Data['%n'] = Data['%n'].str.replace('%', ' ') \n",
    "Data['%muts'] = Data['%muts'].str.replace('%', ' ') \n",
    "Data['%uniquemuts'] = Data['%uniquemuts'].str.replace('%', ' ') \n",
    "Data['%existingmuts'] = Data['%existingmuts'].str.replace('%', ' ') \n",
    "Data['%n'].astype(float)\n",
    "Data['%muts'].astype(float)\n",
    "Data['%uniquemuts'].astype(float)\n",
    "Data['%existingmuts'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting rid of NaN data\n",
    "Data.dropna(subset = ['status'], inplace=True)\n",
    "Data.dropna(subset = ['existingmutlist'], inplace=True)\n",
    "Data.drop(Data.loc[Data['status']=='unknown'].index, inplace=True)\n",
    "\n",
    "Data.drop(Data.loc[Data['status']=='n/a'].index, inplace=True)\n",
    "\n",
    "Data.drop(Data.loc[Data['status']=='NA'].index, inplace=True)\n",
    "\n",
    "Data.drop(Data.loc[Data['status']=='-'].index, inplace=True)\n",
    "\n",
    "Data.drop(Data.loc[Data['status']=='Unknown'].index, inplace=True)\n",
    "\n",
    "Data.index = range(len(Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chacking if there is any null data in ExistingMutList\n",
    "for i in range(len(Data)):\n",
    "    if pd.isnull(Data.existingmutlist[i]) is True:\n",
    "        print('True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting rid of NaN data\n",
    "Data.drop(Data.loc[Data['status']=='unknown'].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#Labeling\n",
    "Data.replace(['Deceased'],value= [1], inplace=True)\n",
    "Data.status[Data['status'] != 1]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>existingmutlist_(E_T30I)</th>\n",
       "      <th>existingmutlist_(NS3_G196V,NS8_G8R,NS8_L84S)</th>\n",
       "      <th>existingmutlist_(NS3_G251V)</th>\n",
       "      <th>existingmutlist_(NS7b_T40I,NS8_L84S)</th>\n",
       "      <th>existingmutlist_(NS8_L84S)</th>\n",
       "      <th>existingmutlist_(NS8_L84S,N_G238C)</th>\n",
       "      <th>existingmutlist_(NS8_L84S,N_S202N)</th>\n",
       "      <th>existingmutlist_(NSP10_T111I,NSP12_P323L,Spike_D614G,N_N140T,N_G97S)</th>\n",
       "      <th>existingmutlist_(NSP10_T51A)</th>\n",
       "      <th>existingmutlist_(NSP12_A449T,NSP12_P323L,Spike_D614G,NS3_Q57H,NS7a_V24F)</th>\n",
       "      <th>...</th>\n",
       "      <th>clade_G</th>\n",
       "      <th>clade_Other</th>\n",
       "      <th>clade_S</th>\n",
       "      <th>clade_V</th>\n",
       "      <th>status</th>\n",
       "      <th>%n</th>\n",
       "      <th>length(nt)</th>\n",
       "      <th>length(aa)</th>\n",
       "      <th>%muts</th>\n",
       "      <th>%uniquemuts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29890</td>\n",
       "      <td>9710</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29859</td>\n",
       "      <td>9710</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29917</td>\n",
       "      <td>9710</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29892</td>\n",
       "      <td>9710</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29828</td>\n",
       "      <td>9710</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 411 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   existingmutlist_(E_T30I)  existingmutlist_(NS3_G196V,NS8_G8R,NS8_L84S)  \\\n",
       "0                         0                                             0   \n",
       "1                         0                                             0   \n",
       "2                         0                                             0   \n",
       "3                         0                                             0   \n",
       "4                         0                                             0   \n",
       "\n",
       "   existingmutlist_(NS3_G251V)  existingmutlist_(NS7b_T40I,NS8_L84S)  \\\n",
       "0                            0                                     0   \n",
       "1                            0                                     0   \n",
       "2                            0                                     0   \n",
       "3                            0                                     0   \n",
       "4                            0                                     0   \n",
       "\n",
       "   existingmutlist_(NS8_L84S)  existingmutlist_(NS8_L84S,N_G238C)  \\\n",
       "0                           0                                   0   \n",
       "1                           0                                   0   \n",
       "2                           0                                   0   \n",
       "3                           0                                   0   \n",
       "4                           0                                   0   \n",
       "\n",
       "   existingmutlist_(NS8_L84S,N_S202N)  \\\n",
       "0                                   0   \n",
       "1                                   0   \n",
       "2                                   0   \n",
       "3                                   0   \n",
       "4                                   0   \n",
       "\n",
       "   existingmutlist_(NSP10_T111I,NSP12_P323L,Spike_D614G,N_N140T,N_G97S)  \\\n",
       "0                                                  0                      \n",
       "1                                                  0                      \n",
       "2                                                  0                      \n",
       "3                                                  0                      \n",
       "4                                                  0                      \n",
       "\n",
       "   existingmutlist_(NSP10_T51A)  \\\n",
       "0                             0   \n",
       "1                             0   \n",
       "2                             0   \n",
       "3                             0   \n",
       "4                             0   \n",
       "\n",
       "   existingmutlist_(NSP12_A449T,NSP12_P323L,Spike_D614G,NS3_Q57H,NS7a_V24F)  \\\n",
       "0                                                  0                          \n",
       "1                                                  0                          \n",
       "2                                                  0                          \n",
       "3                                                  0                          \n",
       "4                                                  0                          \n",
       "\n",
       "   ...  clade_G  clade_Other  clade_S  clade_V  status     %n  length(nt)  \\\n",
       "0  ...        0            0        1        0       0  0.00        29890   \n",
       "1  ...        0            0        1        0       0  0.00        29859   \n",
       "2  ...        0            0        1        0       0  0.00        29917   \n",
       "3  ...        1            0        0        0       0  0.00        29892   \n",
       "4  ...        1            0        0        0       0  0.00        29828   \n",
       "\n",
       "   length(aa)  %muts  %uniquemuts  \n",
       "0        9710  0.04         0.00   \n",
       "1        9710  0.04         0.00   \n",
       "2        9710  0.05         0.01   \n",
       "3        9710  0.08         0.01   \n",
       "4        9710  0.02         0.00   \n",
       "\n",
       "[5 rows x 411 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using dummies instead of location data for ML input\n",
    "Data_model = pd.concat([pd.get_dummies(Data[['existingmutlist']]),pd.get_dummies(Data[['clade']]), Data[['status','%n','length(nt)','length(aa)',\n",
    "                                                   '%muts','%uniquemuts']]], axis=1)\n",
    "Data_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    894\n",
       "1     39\n",
       "Name: status, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the balance in the data\n",
    "Data_model['status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>existingmutlist_(E_T30I)</th>\n",
       "      <th>existingmutlist_(NS3_G196V,NS8_G8R,NS8_L84S)</th>\n",
       "      <th>existingmutlist_(NS3_G251V)</th>\n",
       "      <th>existingmutlist_(NS7b_T40I,NS8_L84S)</th>\n",
       "      <th>existingmutlist_(NS8_L84S)</th>\n",
       "      <th>existingmutlist_(NS8_L84S,N_G238C)</th>\n",
       "      <th>existingmutlist_(NS8_L84S,N_S202N)</th>\n",
       "      <th>existingmutlist_(NSP10_T111I,NSP12_P323L,Spike_D614G,N_N140T,N_G97S)</th>\n",
       "      <th>existingmutlist_(NSP10_T51A)</th>\n",
       "      <th>existingmutlist_(NSP12_A449T,NSP12_P323L,Spike_D614G,NS3_Q57H,NS7a_V24F)</th>\n",
       "      <th>...</th>\n",
       "      <th>existingmutlist_(Spike_T307I,NS3_G251V)</th>\n",
       "      <th>clade_G</th>\n",
       "      <th>clade_Other</th>\n",
       "      <th>clade_S</th>\n",
       "      <th>clade_V</th>\n",
       "      <th>%n</th>\n",
       "      <th>length(nt)</th>\n",
       "      <th>length(aa)</th>\n",
       "      <th>%muts</th>\n",
       "      <th>%uniquemuts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29890.0</td>\n",
       "      <td>9710.0</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29859.0</td>\n",
       "      <td>9710.0</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29917.0</td>\n",
       "      <td>9710.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29892.0</td>\n",
       "      <td>9710.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29828.0</td>\n",
       "      <td>9710.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.44</td>\n",
       "      <td>29903.0</td>\n",
       "      <td>9710.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.45</td>\n",
       "      <td>29903.0</td>\n",
       "      <td>9710.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.44</td>\n",
       "      <td>29903.0</td>\n",
       "      <td>9710.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.45</td>\n",
       "      <td>29903.0</td>\n",
       "      <td>9710.0</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.04</td>\n",
       "      <td>29776.0</td>\n",
       "      <td>9682.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>933 rows × 410 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     existingmutlist_(E_T30I)  existingmutlist_(NS3_G196V,NS8_G8R,NS8_L84S)  \\\n",
       "0                         0.0                                           0.0   \n",
       "1                         0.0                                           0.0   \n",
       "2                         0.0                                           0.0   \n",
       "3                         0.0                                           0.0   \n",
       "4                         0.0                                           0.0   \n",
       "..                        ...                                           ...   \n",
       "928                       0.0                                           0.0   \n",
       "929                       0.0                                           0.0   \n",
       "930                       0.0                                           0.0   \n",
       "931                       0.0                                           0.0   \n",
       "932                       0.0                                           0.0   \n",
       "\n",
       "     existingmutlist_(NS3_G251V)  existingmutlist_(NS7b_T40I,NS8_L84S)  \\\n",
       "0                            0.0                                   0.0   \n",
       "1                            0.0                                   0.0   \n",
       "2                            0.0                                   0.0   \n",
       "3                            0.0                                   0.0   \n",
       "4                            0.0                                   0.0   \n",
       "..                           ...                                   ...   \n",
       "928                          0.0                                   0.0   \n",
       "929                          0.0                                   0.0   \n",
       "930                          0.0                                   0.0   \n",
       "931                          0.0                                   0.0   \n",
       "932                          0.0                                   0.0   \n",
       "\n",
       "     existingmutlist_(NS8_L84S)  existingmutlist_(NS8_L84S,N_G238C)  \\\n",
       "0                           0.0                                 0.0   \n",
       "1                           0.0                                 0.0   \n",
       "2                           0.0                                 0.0   \n",
       "3                           0.0                                 0.0   \n",
       "4                           0.0                                 0.0   \n",
       "..                          ...                                 ...   \n",
       "928                         0.0                                 0.0   \n",
       "929                         0.0                                 0.0   \n",
       "930                         0.0                                 0.0   \n",
       "931                         0.0                                 0.0   \n",
       "932                         0.0                                 0.0   \n",
       "\n",
       "     existingmutlist_(NS8_L84S,N_S202N)  \\\n",
       "0                                   0.0   \n",
       "1                                   0.0   \n",
       "2                                   0.0   \n",
       "3                                   0.0   \n",
       "4                                   0.0   \n",
       "..                                  ...   \n",
       "928                                 0.0   \n",
       "929                                 0.0   \n",
       "930                                 0.0   \n",
       "931                                 0.0   \n",
       "932                                 0.0   \n",
       "\n",
       "     existingmutlist_(NSP10_T111I,NSP12_P323L,Spike_D614G,N_N140T,N_G97S)  \\\n",
       "0                                                  0.0                      \n",
       "1                                                  0.0                      \n",
       "2                                                  0.0                      \n",
       "3                                                  0.0                      \n",
       "4                                                  0.0                      \n",
       "..                                                 ...                      \n",
       "928                                                0.0                      \n",
       "929                                                0.0                      \n",
       "930                                                0.0                      \n",
       "931                                                0.0                      \n",
       "932                                                0.0                      \n",
       "\n",
       "     existingmutlist_(NSP10_T51A)  \\\n",
       "0                             0.0   \n",
       "1                             0.0   \n",
       "2                             0.0   \n",
       "3                             0.0   \n",
       "4                             0.0   \n",
       "..                            ...   \n",
       "928                           0.0   \n",
       "929                           0.0   \n",
       "930                           0.0   \n",
       "931                           0.0   \n",
       "932                           0.0   \n",
       "\n",
       "     existingmutlist_(NSP12_A449T,NSP12_P323L,Spike_D614G,NS3_Q57H,NS7a_V24F)  \\\n",
       "0                                                  0.0                          \n",
       "1                                                  0.0                          \n",
       "2                                                  0.0                          \n",
       "3                                                  0.0                          \n",
       "4                                                  0.0                          \n",
       "..                                                 ...                          \n",
       "928                                                0.0                          \n",
       "929                                                0.0                          \n",
       "930                                                0.0                          \n",
       "931                                                0.0                          \n",
       "932                                                0.0                          \n",
       "\n",
       "     ...  existingmutlist_(Spike_T307I,NS3_G251V)  clade_G  clade_Other  \\\n",
       "0    ...                                      0.0      0.0          0.0   \n",
       "1    ...                                      0.0      0.0          0.0   \n",
       "2    ...                                      0.0      0.0          0.0   \n",
       "3    ...                                      0.0      1.0          0.0   \n",
       "4    ...                                      0.0      1.0          0.0   \n",
       "..   ...                                      ...      ...          ...   \n",
       "928  ...                                      0.0      1.0          0.0   \n",
       "929  ...                                      0.0      1.0          0.0   \n",
       "930  ...                                      0.0      1.0          0.0   \n",
       "931  ...                                      0.0      1.0          0.0   \n",
       "932  ...                                      0.0      1.0          0.0   \n",
       "\n",
       "     clade_S  clade_V    %n  length(nt)  length(aa)  %muts  %uniquemuts  \n",
       "0        1.0      0.0  0.00     29890.0      9710.0   0.04         0.00  \n",
       "1        1.0      0.0  0.00     29859.0      9710.0   0.04         0.00  \n",
       "2        1.0      0.0  0.00     29917.0      9710.0   0.05         0.01  \n",
       "3        0.0      0.0  0.00     29892.0      9710.0   0.08         0.01  \n",
       "4        0.0      0.0  0.00     29828.0      9710.0   0.02         0.00  \n",
       "..       ...      ...   ...         ...         ...    ...          ...  \n",
       "928      0.0      0.0  0.44     29903.0      9710.0   0.05         0.00  \n",
       "929      0.0      0.0  0.45     29903.0      9710.0   0.05         0.00  \n",
       "930      0.0      0.0  0.44     29903.0      9710.0   0.05         0.00  \n",
       "931      0.0      0.0  0.45     29903.0      9710.0   0.07         0.00  \n",
       "932      0.0      0.0  1.04     29776.0      9682.0   0.06         0.00  \n",
       "\n",
       "[933 rows x 410 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Separating target (label) data\n",
    "X = Data_model.drop('status',axis=1).astype(float)\n",
    "y = Data_model.status\n",
    "y=y.astype(int)\n",
    "y.value_counts()\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Scaling the data to standarize them\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "sc = StandardScaler()\n",
    "#sc = MinMaxScaler()\n",
    "sc.fit(X)\n",
    "X_scaled = sc.transform(X)\n",
    "X_normal_scaled = X_scaled[y == 0] \n",
    "X_deseased_scaled = X_scaled[y == 1] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_deseased_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple feedforward autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from keras.layers import Input, Dense, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras.layers import Input, Dense, BatchNormalization\n",
    "# Building the Encoder network \n",
    "window_length = 410\n",
    "encoding_dim = 30\n",
    "epochs = 300\n",
    "input_window = Input(shape=(window_length,))\n",
    "x = Dense(6, activation='tanh')(input_window)\n",
    "x = BatchNormalization()(x)\n",
    "encoded = Dense(encoding_dim, activation='tanh')(x)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "x = Dense(6, activation='tanh')(encoded)\n",
    "x = BatchNormalization()(x)\n",
    "decoded = Dense(window_length, activation='sigmoid')(x)\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_window, decoded)\n",
    "\n",
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_window, encoded)\n",
    "autoencoder.summary()\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "autoencoder.fit(X_normal_scaled, X_normal_scaled,\n",
    "                epochs=epochs,\n",
    "                batch_size=16,\n",
    "                shuffle=True,\n",
    "                validation_split = 0.20)\n",
    "# Separating the points encoded by the Auto-encoder as normal and fraud \n",
    "decoded_X_normal = autoencoder.predict(X_normal_scaled)\n",
    "decoded_X_deseased = autoencoder.predict(X_deseased_scaled)\n",
    "# Combining the encoded points into a single table  \n",
    "encoded_X = np.append(decoded_X_normal, decoded_X_deseased, axis = 0) \n",
    "y_normal = np.zeros(decoded_X_normal.shape[0]) \n",
    "y_deceased = np.ones(decoded_X_deseased.shape[0]) \n",
    "encoded_y = np.append(y_normal, y_deceased) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 410)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               82200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 410)               82410     \n",
      "=================================================================\n",
      "Total params: 164,610\n",
      "Trainable params: 164,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 715 samples, validate on 179 samples\n",
      "Epoch 1/500\n",
      "715/715 [==============================] - 0s 254us/step - loss: 0.7364 - val_loss: 0.7032\n",
      "Epoch 2/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: 0.6925 - val_loss: 0.6626\n",
      "Epoch 3/500\n",
      "715/715 [==============================] - 0s 63us/step - loss: 0.6510 - val_loss: 0.6244\n",
      "Epoch 4/500\n",
      "715/715 [==============================] - 0s 60us/step - loss: 0.6117 - val_loss: 0.5887\n",
      "Epoch 5/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: 0.5744 - val_loss: 0.5555\n",
      "Epoch 6/500\n",
      "715/715 [==============================] - 0s 73us/step - loss: 0.5389 - val_loss: 0.5247\n",
      "Epoch 7/500\n",
      "715/715 [==============================] - 0s 64us/step - loss: 0.5051 - val_loss: 0.4963\n",
      "Epoch 8/500\n",
      "715/715 [==============================] - 0s 60us/step - loss: 0.4726 - val_loss: 0.4701\n",
      "Epoch 9/500\n",
      "715/715 [==============================] - 0s 61us/step - loss: 0.4416 - val_loss: 0.4462\n",
      "Epoch 10/500\n",
      "715/715 [==============================] - 0s 57us/step - loss: 0.4119 - val_loss: 0.4239\n",
      "Epoch 11/500\n",
      "715/715 [==============================] - 0s 61us/step - loss: 0.3832 - val_loss: 0.4034\n",
      "Epoch 12/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: 0.3555 - val_loss: 0.3844\n",
      "Epoch 13/500\n",
      "715/715 [==============================] - 0s 57us/step - loss: 0.3287 - val_loss: 0.3669\n",
      "Epoch 14/500\n",
      "715/715 [==============================] - 0s 75us/step - loss: 0.3028 - val_loss: 0.3505\n",
      "Epoch 15/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: 0.2777 - val_loss: 0.3354\n",
      "Epoch 16/500\n",
      "715/715 [==============================] - 0s 66us/step - loss: 0.2534 - val_loss: 0.3212\n",
      "Epoch 17/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: 0.2298 - val_loss: 0.3080\n",
      "Epoch 18/500\n",
      "715/715 [==============================] - 0s 57us/step - loss: 0.2069 - val_loss: 0.2956\n",
      "Epoch 19/500\n",
      "715/715 [==============================] - 0s 63us/step - loss: 0.1845 - val_loss: 0.2839\n",
      "Epoch 20/500\n",
      "715/715 [==============================] - 0s 67us/step - loss: 0.1628 - val_loss: 0.2729\n",
      "Epoch 21/500\n",
      "715/715 [==============================] - 0s 59us/step - loss: 0.1416 - val_loss: 0.2625\n",
      "Epoch 22/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: 0.1208 - val_loss: 0.2526\n",
      "Epoch 23/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: 0.1006 - val_loss: 0.2433\n",
      "Epoch 24/500\n",
      "715/715 [==============================] - 0s 66us/step - loss: 0.0808 - val_loss: 0.2344\n",
      "Epoch 25/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: 0.0614 - val_loss: 0.2259\n",
      "Epoch 26/500\n",
      "715/715 [==============================] - 0s 61us/step - loss: 0.0423 - val_loss: 0.2179\n",
      "Epoch 27/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: 0.0237 - val_loss: 0.2103\n",
      "Epoch 28/500\n",
      "715/715 [==============================] - 0s 66us/step - loss: 0.0053 - val_loss: 0.2032\n",
      "Epoch 29/500\n",
      "715/715 [==============================] - 0s 61us/step - loss: -0.0125 - val_loss: 0.1964\n",
      "Epoch 30/500\n",
      "715/715 [==============================] - 0s 57us/step - loss: -0.0301 - val_loss: 0.1900\n",
      "Epoch 31/500\n",
      "715/715 [==============================] - 0s 57us/step - loss: -0.0473 - val_loss: 0.1841\n",
      "Epoch 32/500\n",
      "715/715 [==============================] - 0s 60us/step - loss: -0.0641 - val_loss: 0.1785\n",
      "Epoch 33/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -0.0805 - val_loss: 0.1732\n",
      "Epoch 34/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -0.0967 - val_loss: 0.1683\n",
      "Epoch 35/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -0.1126 - val_loss: 0.1638\n",
      "Epoch 36/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -0.1282 - val_loss: 0.1596\n",
      "Epoch 37/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -0.1436 - val_loss: 0.1557\n",
      "Epoch 38/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -0.1588 - val_loss: 0.1521\n",
      "Epoch 39/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -0.1738 - val_loss: 0.1487\n",
      "Epoch 40/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -0.1887 - val_loss: 0.1457\n",
      "Epoch 41/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -0.2034 - val_loss: 0.1429\n",
      "Epoch 42/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -0.2179 - val_loss: 0.1403\n",
      "Epoch 43/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -0.2324 - val_loss: 0.1380\n",
      "Epoch 44/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -0.2468 - val_loss: 0.1358\n",
      "Epoch 45/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -0.2612 - val_loss: 0.1339\n",
      "Epoch 46/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -0.2755 - val_loss: 0.1322\n",
      "Epoch 47/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -0.2898 - val_loss: 0.1306\n",
      "Epoch 48/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -0.3041 - val_loss: 0.1292\n",
      "Epoch 49/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -0.3184 - val_loss: 0.1279\n",
      "Epoch 50/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -0.3328 - val_loss: 0.1268\n",
      "Epoch 51/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -0.3472 - val_loss: 0.1259\n",
      "Epoch 52/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -0.3617 - val_loss: 0.1250\n",
      "Epoch 53/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -0.3763 - val_loss: 0.1243\n",
      "Epoch 54/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -0.3910 - val_loss: 0.1237\n",
      "Epoch 55/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -0.4058 - val_loss: 0.1232\n",
      "Epoch 56/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -0.4207 - val_loss: 0.1227\n",
      "Epoch 57/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -0.4358 - val_loss: 0.1224\n",
      "Epoch 58/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -0.4510 - val_loss: 0.1221\n",
      "Epoch 59/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -0.4662 - val_loss: 0.1219\n",
      "Epoch 60/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -0.4816 - val_loss: 0.1218\n",
      "Epoch 61/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -0.4973 - val_loss: 0.1217\n",
      "Epoch 62/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -0.5130 - val_loss: 0.1217\n",
      "Epoch 63/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -0.5288 - val_loss: 0.1218\n",
      "Epoch 64/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -0.5450 - val_loss: 0.1219\n",
      "Epoch 65/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -0.5611 - val_loss: 0.1220\n",
      "Epoch 66/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -0.5770 - val_loss: 0.1222\n",
      "Epoch 67/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715/715 [==============================] - 0s 53us/step - loss: -0.5925 - val_loss: 0.1223\n",
      "Epoch 68/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -0.6083 - val_loss: 0.1225\n",
      "Epoch 69/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -0.6233 - val_loss: 0.1227\n",
      "Epoch 70/500\n",
      "715/715 [==============================] - 0s 46us/step - loss: -0.6381 - val_loss: 0.1228\n",
      "Epoch 71/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -0.6528 - val_loss: 0.1229\n",
      "Epoch 72/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -0.6671 - val_loss: 0.1230\n",
      "Epoch 73/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -0.6810 - val_loss: 0.1231\n",
      "Epoch 74/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -0.6946 - val_loss: 0.1231\n",
      "Epoch 75/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -0.7074 - val_loss: 0.1230\n",
      "Epoch 76/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -0.7205 - val_loss: 0.1228\n",
      "Epoch 77/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -0.7330 - val_loss: 0.1226\n",
      "Epoch 78/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -0.7447 - val_loss: 0.1222\n",
      "Epoch 79/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -0.7559 - val_loss: 0.1217\n",
      "Epoch 80/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -0.7664 - val_loss: 0.1211\n",
      "Epoch 81/500\n",
      "715/715 [==============================] - 0s 46us/step - loss: -0.7769 - val_loss: 0.1205\n",
      "Epoch 82/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -0.7870 - val_loss: 0.1196\n",
      "Epoch 83/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -0.7966 - val_loss: 0.1187\n",
      "Epoch 84/500\n",
      "715/715 [==============================] - 0s 46us/step - loss: -0.8063 - val_loss: 0.1176\n",
      "Epoch 85/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -0.8156 - val_loss: 0.1164\n",
      "Epoch 86/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -0.8247 - val_loss: 0.1151\n",
      "Epoch 87/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -0.8336 - val_loss: 0.1137\n",
      "Epoch 88/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -0.8424 - val_loss: 0.1122\n",
      "Epoch 89/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -0.8512 - val_loss: 0.1106\n",
      "Epoch 90/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -0.8602 - val_loss: 0.1089\n",
      "Epoch 91/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -0.8687 - val_loss: 0.1071\n",
      "Epoch 92/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -0.8772 - val_loss: 0.1053\n",
      "Epoch 93/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -0.8856 - val_loss: 0.1034\n",
      "Epoch 94/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -0.8940 - val_loss: 0.1014\n",
      "Epoch 95/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -0.9022 - val_loss: 0.0994\n",
      "Epoch 96/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -0.9106 - val_loss: 0.0974\n",
      "Epoch 97/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -0.9189 - val_loss: 0.0953\n",
      "Epoch 98/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -0.9266 - val_loss: 0.0932\n",
      "Epoch 99/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -0.9349 - val_loss: 0.0911\n",
      "Epoch 100/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -0.9428 - val_loss: 0.0889\n",
      "Epoch 101/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -0.9509 - val_loss: 0.0867\n",
      "Epoch 102/500\n",
      "715/715 [==============================] - 0s 46us/step - loss: -0.9590 - val_loss: 0.0846\n",
      "Epoch 103/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -0.9670 - val_loss: 0.0825\n",
      "Epoch 104/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -0.9751 - val_loss: 0.0803\n",
      "Epoch 105/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -0.9831 - val_loss: 0.0782\n",
      "Epoch 106/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -0.9911 - val_loss: 0.0760\n",
      "Epoch 107/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -0.9991 - val_loss: 0.0739\n",
      "Epoch 108/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.0072 - val_loss: 0.0718\n",
      "Epoch 109/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.0152 - val_loss: 0.0697\n",
      "Epoch 110/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.0232 - val_loss: 0.0675\n",
      "Epoch 111/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.0313 - val_loss: 0.0655\n",
      "Epoch 112/500\n",
      "715/715 [==============================] - 0s 59us/step - loss: -1.0393 - val_loss: 0.0635\n",
      "Epoch 113/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.0473 - val_loss: 0.0616\n",
      "Epoch 114/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.0552 - val_loss: 0.0597\n",
      "Epoch 115/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.0632 - val_loss: 0.0578\n",
      "Epoch 116/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.0710 - val_loss: 0.0559\n",
      "Epoch 117/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.0786 - val_loss: 0.0541\n",
      "Epoch 118/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.0861 - val_loss: 0.0523\n",
      "Epoch 119/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.0936 - val_loss: 0.0504\n",
      "Epoch 120/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.1010 - val_loss: 0.0488\n",
      "Epoch 121/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.1082 - val_loss: 0.0470\n",
      "Epoch 122/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -1.1155 - val_loss: 0.0453\n",
      "Epoch 123/500\n",
      "715/715 [==============================] - 0s 57us/step - loss: -1.1225 - val_loss: 0.0436\n",
      "Epoch 124/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.1295 - val_loss: 0.0419\n",
      "Epoch 125/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.1364 - val_loss: 0.0403\n",
      "Epoch 126/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -1.1432 - val_loss: 0.0390\n",
      "Epoch 127/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -1.1497 - val_loss: 0.0373\n",
      "Epoch 128/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.1561 - val_loss: 0.0359\n",
      "Epoch 129/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.1624 - val_loss: 0.0345\n",
      "Epoch 130/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -1.1684 - val_loss: 0.0333\n",
      "Epoch 131/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -1.1743 - val_loss: 0.0319\n",
      "Epoch 132/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.1801 - val_loss: 0.0308\n",
      "Epoch 133/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.1857 - val_loss: 0.0297\n",
      "Epoch 134/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.1911 - val_loss: 0.0286\n",
      "Epoch 135/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.1963 - val_loss: 0.0275\n",
      "Epoch 136/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.2015 - val_loss: 0.0265\n",
      "Epoch 137/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.2063 - val_loss: 0.0257\n",
      "Epoch 138/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.2110 - val_loss: 0.0246\n",
      "Epoch 139/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.2156 - val_loss: 0.0238\n",
      "Epoch 140/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.2198 - val_loss: 0.0230\n",
      "Epoch 141/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.2241 - val_loss: 0.0223\n",
      "Epoch 142/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -1.2278 - val_loss: 0.0217\n",
      "Epoch 143/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.2313 - val_loss: 0.0210\n",
      "Epoch 144/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.2347 - val_loss: 0.0204\n",
      "Epoch 145/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.2377 - val_loss: 0.0198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 146/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -1.2407 - val_loss: 0.0191\n",
      "Epoch 147/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.2434 - val_loss: 0.0185\n",
      "Epoch 148/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.2460 - val_loss: 0.0177\n",
      "Epoch 149/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -1.2484 - val_loss: 0.0170\n",
      "Epoch 150/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.2509 - val_loss: 0.0164\n",
      "Epoch 151/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -1.2532 - val_loss: 0.0156\n",
      "Epoch 152/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -1.2553 - val_loss: 0.0149\n",
      "Epoch 153/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.2574 - val_loss: 0.0141\n",
      "Epoch 154/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.2594 - val_loss: 0.0133\n",
      "Epoch 155/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.2612 - val_loss: 0.0125\n",
      "Epoch 156/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.2630 - val_loss: 0.0117\n",
      "Epoch 157/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.2646 - val_loss: 0.0108\n",
      "Epoch 158/500\n",
      "715/715 [==============================] - 0s 46us/step - loss: -1.2662 - val_loss: 0.0099\n",
      "Epoch 159/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.2678 - val_loss: 0.0090\n",
      "Epoch 160/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.2693 - val_loss: 0.0081\n",
      "Epoch 161/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.2707 - val_loss: 0.0072\n",
      "Epoch 162/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.2721 - val_loss: 0.0064\n",
      "Epoch 163/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.2734 - val_loss: 0.0055\n",
      "Epoch 164/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.2746 - val_loss: 0.0046\n",
      "Epoch 165/500\n",
      "715/715 [==============================] - 0s 46us/step - loss: -1.2758 - val_loss: 0.0037\n",
      "Epoch 166/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.2769 - val_loss: 0.0028\n",
      "Epoch 167/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -1.2780 - val_loss: 0.0019\n",
      "Epoch 168/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.2790 - val_loss: 9.5138e-04\n",
      "Epoch 169/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.2801 - val_loss: -6.8659e-06\n",
      "Epoch 170/500\n",
      "715/715 [==============================] - 0s 46us/step - loss: -1.2812 - val_loss: -9.5808e-04\n",
      "Epoch 171/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.2821 - val_loss: -0.0019\n",
      "Epoch 172/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.2831 - val_loss: -0.0030\n",
      "Epoch 173/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.2840 - val_loss: -0.0040\n",
      "Epoch 174/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.2850 - val_loss: -0.0050\n",
      "Epoch 175/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.2859 - val_loss: -0.0061\n",
      "Epoch 176/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.2867 - val_loss: -0.0071\n",
      "Epoch 177/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.2877 - val_loss: -0.0082\n",
      "Epoch 178/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -1.2885 - val_loss: -0.0093\n",
      "Epoch 179/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.2894 - val_loss: -0.0104\n",
      "Epoch 180/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.2902 - val_loss: -0.0114\n",
      "Epoch 181/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.2910 - val_loss: -0.0126\n",
      "Epoch 182/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.2917 - val_loss: -0.0136\n",
      "Epoch 183/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.2924 - val_loss: -0.0147\n",
      "Epoch 184/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.2931 - val_loss: -0.0159\n",
      "Epoch 185/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.2938 - val_loss: -0.0170\n",
      "Epoch 186/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.2945 - val_loss: -0.0181\n",
      "Epoch 187/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.2952 - val_loss: -0.0193\n",
      "Epoch 188/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.2959 - val_loss: -0.0204\n",
      "Epoch 189/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.2966 - val_loss: -0.0216\n",
      "Epoch 190/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.2973 - val_loss: -0.0227\n",
      "Epoch 191/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.2980 - val_loss: -0.0238\n",
      "Epoch 192/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -1.2987 - val_loss: -0.0250\n",
      "Epoch 193/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.2994 - val_loss: -0.0261\n",
      "Epoch 194/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3001 - val_loss: -0.0273\n",
      "Epoch 195/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3008 - val_loss: -0.0284\n",
      "Epoch 196/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3014 - val_loss: -0.0295\n",
      "Epoch 197/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3021 - val_loss: -0.0306\n",
      "Epoch 198/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3028 - val_loss: -0.0319\n",
      "Epoch 199/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3035 - val_loss: -0.0331\n",
      "Epoch 200/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3041 - val_loss: -0.0342\n",
      "Epoch 201/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3047 - val_loss: -0.0353\n",
      "Epoch 202/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3054 - val_loss: -0.0366\n",
      "Epoch 203/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3059 - val_loss: -0.0377\n",
      "Epoch 204/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -1.3065 - val_loss: -0.0388\n",
      "Epoch 205/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3072 - val_loss: -0.0400\n",
      "Epoch 206/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -1.3078 - val_loss: -0.0412\n",
      "Epoch 207/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3083 - val_loss: -0.0423\n",
      "Epoch 208/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3090 - val_loss: -0.0434\n",
      "Epoch 209/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3094 - val_loss: -0.0446\n",
      "Epoch 210/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3099 - val_loss: -0.0455\n",
      "Epoch 211/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3104 - val_loss: -0.0470\n",
      "Epoch 212/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -1.3109 - val_loss: -0.0477\n",
      "Epoch 213/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -1.3113 - val_loss: -0.0487\n",
      "Epoch 214/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3118 - val_loss: -0.0496\n",
      "Epoch 215/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3123 - val_loss: -0.0505\n",
      "Epoch 216/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3128 - val_loss: -0.0513\n",
      "Epoch 217/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3132 - val_loss: -0.0522\n",
      "Epoch 218/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3137 - val_loss: -0.0531\n",
      "Epoch 219/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3142 - val_loss: -0.0539\n",
      "Epoch 220/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3147 - val_loss: -0.0548\n",
      "Epoch 221/500\n",
      "715/715 [==============================] - 0s 46us/step - loss: -1.3153 - val_loss: -0.0557\n",
      "Epoch 222/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3158 - val_loss: -0.0566\n",
      "Epoch 223/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3162 - val_loss: -0.0574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 224/500\n",
      "715/715 [==============================] - 0s 46us/step - loss: -1.3168 - val_loss: -0.0583\n",
      "Epoch 225/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3172 - val_loss: -0.0592\n",
      "Epoch 226/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3177 - val_loss: -0.0600\n",
      "Epoch 227/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3182 - val_loss: -0.0609\n",
      "Epoch 228/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3187 - val_loss: -0.0617\n",
      "Epoch 229/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3191 - val_loss: -0.0626\n",
      "Epoch 230/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3196 - val_loss: -0.0634\n",
      "Epoch 231/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -1.3200 - val_loss: -0.0642\n",
      "Epoch 232/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3204 - val_loss: -0.0651\n",
      "Epoch 233/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3209 - val_loss: -0.0660\n",
      "Epoch 234/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3213 - val_loss: -0.0668\n",
      "Epoch 235/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3217 - val_loss: -0.0676\n",
      "Epoch 236/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3221 - val_loss: -0.0684\n",
      "Epoch 237/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -1.3225 - val_loss: -0.0693\n",
      "Epoch 238/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3229 - val_loss: -0.0700\n",
      "Epoch 239/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -1.3233 - val_loss: -0.0708\n",
      "Epoch 240/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3237 - val_loss: -0.0717\n",
      "Epoch 241/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3241 - val_loss: -0.0724\n",
      "Epoch 242/500\n",
      "715/715 [==============================] - 0s 46us/step - loss: -1.3245 - val_loss: -0.0732\n",
      "Epoch 243/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3249 - val_loss: -0.0740\n",
      "Epoch 244/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3253 - val_loss: -0.0748\n",
      "Epoch 245/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3257 - val_loss: -0.0756\n",
      "Epoch 246/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3261 - val_loss: -0.0763\n",
      "Epoch 247/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3265 - val_loss: -0.0770\n",
      "Epoch 248/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3268 - val_loss: -0.0778\n",
      "Epoch 249/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3272 - val_loss: -0.0786\n",
      "Epoch 250/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3276 - val_loss: -0.0793\n",
      "Epoch 251/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3279 - val_loss: -0.0800\n",
      "Epoch 252/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3283 - val_loss: -0.0808\n",
      "Epoch 253/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -1.3287 - val_loss: -0.0816\n",
      "Epoch 254/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3291 - val_loss: -0.0823\n",
      "Epoch 255/500\n",
      "715/715 [==============================] - 0s 46us/step - loss: -1.3295 - val_loss: -0.0830\n",
      "Epoch 256/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3299 - val_loss: -0.0837\n",
      "Epoch 257/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -1.3303 - val_loss: -0.0845\n",
      "Epoch 258/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3307 - val_loss: -0.0852\n",
      "Epoch 259/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3310 - val_loss: -0.0859\n",
      "Epoch 260/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3314 - val_loss: -0.0866\n",
      "Epoch 261/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3317 - val_loss: -0.0873\n",
      "Epoch 262/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3320 - val_loss: -0.0880\n",
      "Epoch 263/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3323 - val_loss: -0.0886\n",
      "Epoch 264/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3326 - val_loss: -0.0893\n",
      "Epoch 265/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3329 - val_loss: -0.0899\n",
      "Epoch 266/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3332 - val_loss: -0.0906\n",
      "Epoch 267/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3335 - val_loss: -0.0912\n",
      "Epoch 268/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -1.3338 - val_loss: -0.0917\n",
      "Epoch 269/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3341 - val_loss: -0.0926\n",
      "Epoch 270/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3344 - val_loss: -0.0932\n",
      "Epoch 271/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3347 - val_loss: -0.0939\n",
      "Epoch 272/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3350 - val_loss: -0.0945\n",
      "Epoch 273/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3354 - val_loss: -0.0952\n",
      "Epoch 274/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3357 - val_loss: -0.0956\n",
      "Epoch 275/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3360 - val_loss: -0.0956\n",
      "Epoch 276/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -1.3363 - val_loss: -0.0962\n",
      "Epoch 277/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -1.3366 - val_loss: -0.0966\n",
      "Epoch 278/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3368 - val_loss: -0.0969\n",
      "Epoch 279/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3371 - val_loss: -0.0972\n",
      "Epoch 280/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3374 - val_loss: -0.0975\n",
      "Epoch 281/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3376 - val_loss: -0.0978\n",
      "Epoch 282/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3379 - val_loss: -0.0982\n",
      "Epoch 283/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3381 - val_loss: -0.0983\n",
      "Epoch 284/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3383 - val_loss: -0.0985\n",
      "Epoch 285/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -1.3384 - val_loss: -0.0986\n",
      "Epoch 286/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -1.3386 - val_loss: -0.0985\n",
      "Epoch 287/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3388 - val_loss: -0.0989\n",
      "Epoch 288/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3390 - val_loss: -0.0988\n",
      "Epoch 289/500\n",
      "715/715 [==============================] - 0s 46us/step - loss: -1.3392 - val_loss: -0.0992\n",
      "Epoch 290/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3393 - val_loss: -0.0993\n",
      "Epoch 291/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3395 - val_loss: -0.0994\n",
      "Epoch 292/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3397 - val_loss: -0.0995\n",
      "Epoch 293/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3399 - val_loss: -0.0997\n",
      "Epoch 294/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3401 - val_loss: -0.0999\n",
      "Epoch 295/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3403 - val_loss: -0.1000\n",
      "Epoch 296/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3405 - val_loss: -0.1001\n",
      "Epoch 297/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3407 - val_loss: -0.1002\n",
      "Epoch 298/500\n",
      "715/715 [==============================] - 0s 61us/step - loss: -1.3408 - val_loss: -0.1002\n",
      "Epoch 299/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3410 - val_loss: -0.1005\n",
      "Epoch 300/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3412 - val_loss: -0.1006\n",
      "Epoch 301/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3414 - val_loss: -0.1007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 302/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3416 - val_loss: -0.1008\n",
      "Epoch 303/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3417 - val_loss: -0.1009\n",
      "Epoch 304/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3418 - val_loss: -0.1008\n",
      "Epoch 305/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3420 - val_loss: -0.1011\n",
      "Epoch 306/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3422 - val_loss: -0.1011\n",
      "Epoch 307/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -1.3423 - val_loss: -0.1012\n",
      "Epoch 308/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3425 - val_loss: -0.1013\n",
      "Epoch 309/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3426 - val_loss: -0.1013\n",
      "Epoch 310/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3427 - val_loss: -0.1013\n",
      "Epoch 311/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3429 - val_loss: -0.1013\n",
      "Epoch 312/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3430 - val_loss: -0.1014\n",
      "Epoch 313/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3431 - val_loss: -0.1015\n",
      "Epoch 314/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -1.3433 - val_loss: -0.1015\n",
      "Epoch 315/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -1.3434 - val_loss: -0.1017\n",
      "Epoch 316/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3435 - val_loss: -0.1016\n",
      "Epoch 317/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3436 - val_loss: -0.1017\n",
      "Epoch 318/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3437 - val_loss: -0.1018\n",
      "Epoch 319/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -1.3438 - val_loss: -0.1018\n",
      "Epoch 320/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -1.3439 - val_loss: -0.1019\n",
      "Epoch 321/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3440 - val_loss: -0.1018\n",
      "Epoch 322/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -1.3441 - val_loss: -0.1019\n",
      "Epoch 323/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -1.3442 - val_loss: -0.1019\n",
      "Epoch 324/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3444 - val_loss: -0.1020\n",
      "Epoch 325/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3445 - val_loss: -0.1020\n",
      "Epoch 326/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -1.3446 - val_loss: -0.1022\n",
      "Epoch 327/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3447 - val_loss: -0.1021\n",
      "Epoch 328/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -1.3448 - val_loss: -0.1021\n",
      "Epoch 329/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3449 - val_loss: -0.1023\n",
      "Epoch 330/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3450 - val_loss: -0.1023\n",
      "Epoch 331/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3451 - val_loss: -0.1024\n",
      "Epoch 332/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3452 - val_loss: -0.1024\n",
      "Epoch 333/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -1.3452 - val_loss: -0.1024\n",
      "Epoch 334/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -1.3453 - val_loss: -0.1024\n",
      "Epoch 335/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3454 - val_loss: -0.1024\n",
      "Epoch 336/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3455 - val_loss: -0.1025\n",
      "Epoch 337/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3456 - val_loss: -0.1026\n",
      "Epoch 338/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3457 - val_loss: -0.1026\n",
      "Epoch 339/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3457 - val_loss: -0.1026\n",
      "Epoch 340/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3458 - val_loss: -0.1027\n",
      "Epoch 341/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3459 - val_loss: -0.1026\n",
      "Epoch 342/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3460 - val_loss: -0.1027\n",
      "Epoch 343/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -1.3460 - val_loss: -0.1027\n",
      "Epoch 344/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3461 - val_loss: -0.1028\n",
      "Epoch 345/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3462 - val_loss: -0.1028\n",
      "Epoch 346/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3462 - val_loss: -0.1028\n",
      "Epoch 347/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3463 - val_loss: -0.1029\n",
      "Epoch 348/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3464 - val_loss: -0.1028\n",
      "Epoch 349/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3464 - val_loss: -0.1029\n",
      "Epoch 350/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -1.3465 - val_loss: -0.1029\n",
      "Epoch 351/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -1.3465 - val_loss: -0.1029\n",
      "Epoch 352/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3466 - val_loss: -0.1030\n",
      "Epoch 353/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3467 - val_loss: -0.1030\n",
      "Epoch 354/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3467 - val_loss: -0.1030\n",
      "Epoch 355/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3467 - val_loss: -0.1031\n",
      "Epoch 356/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3468 - val_loss: -0.1031\n",
      "Epoch 357/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3469 - val_loss: -0.1031\n",
      "Epoch 358/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3469 - val_loss: -0.1031\n",
      "Epoch 359/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3470 - val_loss: -0.1032\n",
      "Epoch 360/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3470 - val_loss: -0.1032\n",
      "Epoch 361/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3471 - val_loss: -0.1032\n",
      "Epoch 362/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3471 - val_loss: -0.1032\n",
      "Epoch 363/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3472 - val_loss: -0.1033\n",
      "Epoch 364/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3472 - val_loss: -0.1033\n",
      "Epoch 365/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -1.3472 - val_loss: -0.1033\n",
      "Epoch 366/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3473 - val_loss: -0.1033\n",
      "Epoch 367/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3473 - val_loss: -0.1034\n",
      "Epoch 368/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -1.3474 - val_loss: -0.1033\n",
      "Epoch 369/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3474 - val_loss: -0.1034\n",
      "Epoch 370/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3474 - val_loss: -0.1034\n",
      "Epoch 371/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3475 - val_loss: -0.1033\n",
      "Epoch 372/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3475 - val_loss: -0.1033\n",
      "Epoch 373/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -1.3476 - val_loss: -0.1034\n",
      "Epoch 374/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3476 - val_loss: -0.1034\n",
      "Epoch 375/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -1.3476 - val_loss: -0.1034\n",
      "Epoch 376/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3476 - val_loss: -0.1035\n",
      "Epoch 377/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3477 - val_loss: -0.1035\n",
      "Epoch 378/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3477 - val_loss: -0.1036\n",
      "Epoch 379/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -1.3477 - val_loss: -0.1035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 380/500\n",
      "715/715 [==============================] - 0s 46us/step - loss: -1.3478 - val_loss: -0.1036\n",
      "Epoch 381/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3478 - val_loss: -0.1035\n",
      "Epoch 382/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3478 - val_loss: -0.1036\n",
      "Epoch 383/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3479 - val_loss: -0.1037\n",
      "Epoch 384/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -1.3479 - val_loss: -0.1036\n",
      "Epoch 385/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3479 - val_loss: -0.1037\n",
      "Epoch 386/500\n",
      "715/715 [==============================] - 0s 46us/step - loss: -1.3480 - val_loss: -0.1036\n",
      "Epoch 387/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3480 - val_loss: -0.1037\n",
      "Epoch 388/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3480 - val_loss: -0.1037\n",
      "Epoch 389/500\n",
      "715/715 [==============================] - 0s 46us/step - loss: -1.3480 - val_loss: -0.1037\n",
      "Epoch 390/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3481 - val_loss: -0.1038\n",
      "Epoch 391/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3481 - val_loss: -0.1039\n",
      "Epoch 392/500\n",
      "715/715 [==============================] - 0s 46us/step - loss: -1.3481 - val_loss: -0.1039\n",
      "Epoch 393/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3482 - val_loss: -0.1038\n",
      "Epoch 394/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3482 - val_loss: -0.1039\n",
      "Epoch 395/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -1.3482 - val_loss: -0.1039\n",
      "Epoch 396/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3482 - val_loss: -0.1040\n",
      "Epoch 397/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3483 - val_loss: -0.1039\n",
      "Epoch 398/500\n",
      "715/715 [==============================] - 0s 46us/step - loss: -1.3483 - val_loss: -0.1039\n",
      "Epoch 399/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3483 - val_loss: -0.1039\n",
      "Epoch 400/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3483 - val_loss: -0.1040\n",
      "Epoch 401/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -1.3484 - val_loss: -0.1039\n",
      "Epoch 402/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3484 - val_loss: -0.1039\n",
      "Epoch 403/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3484 - val_loss: -0.1039\n",
      "Epoch 404/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -1.3484 - val_loss: -0.1039\n",
      "Epoch 405/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3485 - val_loss: -0.1039\n",
      "Epoch 406/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -1.3485 - val_loss: -0.1040\n",
      "Epoch 407/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -1.3485 - val_loss: -0.1040\n",
      "Epoch 408/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3486 - val_loss: -0.1041\n",
      "Epoch 409/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -1.3486 - val_loss: -0.1041\n",
      "Epoch 410/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3486 - val_loss: -0.1041\n",
      "Epoch 411/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3486 - val_loss: -0.1041\n",
      "Epoch 412/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3486 - val_loss: -0.1040\n",
      "Epoch 413/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3487 - val_loss: -0.1040\n",
      "Epoch 414/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3487 - val_loss: -0.1041\n",
      "Epoch 415/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -1.3487 - val_loss: -0.1041\n",
      "Epoch 416/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3487 - val_loss: -0.1041\n",
      "Epoch 417/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3488 - val_loss: -0.1041\n",
      "Epoch 418/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -1.3488 - val_loss: -0.1041\n",
      "Epoch 419/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3488 - val_loss: -0.1041\n",
      "Epoch 420/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3488 - val_loss: -0.1041\n",
      "Epoch 421/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3488 - val_loss: -0.1041\n",
      "Epoch 422/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3489 - val_loss: -0.1041\n",
      "Epoch 423/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3489 - val_loss: -0.1042\n",
      "Epoch 424/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3489 - val_loss: -0.1042\n",
      "Epoch 425/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3489 - val_loss: -0.1043\n",
      "Epoch 426/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -1.3489 - val_loss: -0.1043\n",
      "Epoch 427/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3490 - val_loss: -0.1042\n",
      "Epoch 428/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3490 - val_loss: -0.1042\n",
      "Epoch 429/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3490 - val_loss: -0.1043\n",
      "Epoch 430/500\n",
      "715/715 [==============================] - 0s 57us/step - loss: -1.3490 - val_loss: -0.1043\n",
      "Epoch 431/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3490 - val_loss: -0.1043\n",
      "Epoch 432/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -1.3491 - val_loss: -0.1043\n",
      "Epoch 433/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3491 - val_loss: -0.1042\n",
      "Epoch 434/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -1.3491 - val_loss: -0.1042\n",
      "Epoch 435/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3491 - val_loss: -0.1043\n",
      "Epoch 436/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -1.3491 - val_loss: -0.1042\n",
      "Epoch 437/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3491 - val_loss: -0.1042\n",
      "Epoch 438/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -1.3491 - val_loss: -0.1042\n",
      "Epoch 439/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3491 - val_loss: -0.1042\n",
      "Epoch 440/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3492 - val_loss: -0.1043\n",
      "Epoch 441/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3492 - val_loss: -0.1042\n",
      "Epoch 442/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3492 - val_loss: -0.1043\n",
      "Epoch 443/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3492 - val_loss: -0.1042\n",
      "Epoch 444/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3492 - val_loss: -0.1042\n",
      "Epoch 445/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -1.3492 - val_loss: -0.1041\n",
      "Epoch 446/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3492 - val_loss: -0.1042\n",
      "Epoch 447/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3493 - val_loss: -0.1042\n",
      "Epoch 448/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3493 - val_loss: -0.1042\n",
      "Epoch 449/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -1.3493 - val_loss: -0.1042\n",
      "Epoch 450/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3493 - val_loss: -0.1042\n",
      "Epoch 451/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3493 - val_loss: -0.1041\n",
      "Epoch 452/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3493 - val_loss: -0.1041\n",
      "Epoch 453/500\n",
      "715/715 [==============================] - 0s 60us/step - loss: -1.3493 - val_loss: -0.1041\n",
      "Epoch 454/500\n",
      "715/715 [==============================] - 0s 71us/step - loss: -1.3493 - val_loss: -0.1041\n",
      "Epoch 455/500\n",
      "715/715 [==============================] - 0s 78us/step - loss: -1.3494 - val_loss: -0.1040\n",
      "Epoch 456/500\n",
      "715/715 [==============================] - 0s 73us/step - loss: -1.3494 - val_loss: -0.1041\n",
      "Epoch 457/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -1.3494 - val_loss: -0.1041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 458/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3494 - val_loss: -0.1041\n",
      "Epoch 459/500\n",
      "715/715 [==============================] - 0s 46us/step - loss: -1.3494 - val_loss: -0.1040\n",
      "Epoch 460/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3494 - val_loss: -0.1040\n",
      "Epoch 461/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3494 - val_loss: -0.1040\n",
      "Epoch 462/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -1.3494 - val_loss: -0.1041\n",
      "Epoch 463/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3494 - val_loss: -0.1040\n",
      "Epoch 464/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3495 - val_loss: -0.1039\n",
      "Epoch 465/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3495 - val_loss: -0.1040\n",
      "Epoch 466/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3495 - val_loss: -0.1040\n",
      "Epoch 467/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3495 - val_loss: -0.1039\n",
      "Epoch 468/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3495 - val_loss: -0.1039\n",
      "Epoch 469/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3495 - val_loss: -0.1039\n",
      "Epoch 470/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -1.3495 - val_loss: -0.1038\n",
      "Epoch 471/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3495 - val_loss: -0.1039\n",
      "Epoch 472/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3496 - val_loss: -0.1038\n",
      "Epoch 473/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3496 - val_loss: -0.1039\n",
      "Epoch 474/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3496 - val_loss: -0.1038\n",
      "Epoch 475/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -1.3496 - val_loss: -0.1037\n",
      "Epoch 476/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3496 - val_loss: -0.1039\n",
      "Epoch 477/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3496 - val_loss: -0.1037\n",
      "Epoch 478/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -1.3496 - val_loss: -0.1039\n",
      "Epoch 479/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -1.3496 - val_loss: -0.1038\n",
      "Epoch 480/500\n",
      "715/715 [==============================] - 0s 57us/step - loss: -1.3496 - val_loss: -0.1037\n",
      "Epoch 481/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -1.3497 - val_loss: -0.1038\n",
      "Epoch 482/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3497 - val_loss: -0.1038\n",
      "Epoch 483/500\n",
      "715/715 [==============================] - 0s 59us/step - loss: -1.3497 - val_loss: -0.1038\n",
      "Epoch 484/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3497 - val_loss: -0.1038\n",
      "Epoch 485/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3497 - val_loss: -0.1037\n",
      "Epoch 486/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3497 - val_loss: -0.1037\n",
      "Epoch 487/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3497 - val_loss: -0.1037\n",
      "Epoch 488/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -1.3497 - val_loss: -0.1037\n",
      "Epoch 489/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3497 - val_loss: -0.1038\n",
      "Epoch 490/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3497 - val_loss: -0.1038\n",
      "Epoch 491/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -1.3497 - val_loss: -0.1038\n",
      "Epoch 492/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3498 - val_loss: -0.1037\n",
      "Epoch 493/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3498 - val_loss: -0.1037\n",
      "Epoch 494/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3498 - val_loss: -0.1037\n",
      "Epoch 495/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3498 - val_loss: -0.1036\n",
      "Epoch 496/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3498 - val_loss: -0.1037\n",
      "Epoch 497/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3498 - val_loss: -0.1036\n",
      "Epoch 498/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3498 - val_loss: -0.1036\n",
      "Epoch 499/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -1.3498 - val_loss: -0.1037\n",
      "Epoch 500/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3498 - val_loss: -0.1037\n"
     ]
    }
   ],
   "source": [
    "# import keras\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "window_length = 410\n",
    "encoding_dim = 200\n",
    "epochs = 500\n",
    "\n",
    "\n",
    "# this is our input placeholder\n",
    "input_window = Input(shape=(window_length,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_window)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(window_length, activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_window, decoded)\n",
    "\n",
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_window, encoded)\n",
    "\n",
    "\n",
    "autoencoder.summary()\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "history = autoencoder.fit(X_normal_scaled, X_normal_scaled,\n",
    "                epochs=epochs,\n",
    "                batch_size=1024,\n",
    "                shuffle=True,\n",
    "                validation_split = 0.2)\n",
    "\n",
    "# Separating the points encoded by the Auto-encoder as normal and fraud \n",
    "decoded_X_normal = autoencoder.predict(X_normal_scaled)\n",
    "decoded_X_deseased = autoencoder.predict(X_deseased_scaled)\n",
    "# Combining the encoded points into a single table  \n",
    "encoded_X = np.append(decoded_X_normal, decoded_X_deseased, axis = 0) \n",
    "y_normal = np.zeros(decoded_X_normal.shape[0]) \n",
    "y_deceased = np.ones(decoded_X_deseased.shape[0]) \n",
    "encoded_y = np.append(y_normal, y_deceased) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import keras\n",
    "from keras.layers import Input, Dense, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras import regularizers \n",
    "# Building the Input Layer \n",
    "window_length = 410\n",
    "input_layer =  Input(shape=(window_length,))\n",
    "  \n",
    "# Building the Encoder network \n",
    "encoded = Dense(200, activation ='tanh', \n",
    "                activity_regularizer = regularizers.l1(10e-5))(input_layer) \n",
    "encoded = BatchNormalization()(encoded)\n",
    "encoded = Dense(100, activation ='tanh', \n",
    "                activity_regularizer = regularizers.l1(10e-5))(input_layer) \n",
    "encoded = BatchNormalization()(encoded)\n",
    "encoded = Dense(50, activation ='tanh', \n",
    "                activity_regularizer = regularizers.l1(10e-5))(encoded) \n",
    "encoded = BatchNormalization()(encoded)\n",
    "encoded = Dense(25, activation ='tanh', \n",
    "                activity_regularizer = regularizers.l1(10e-5))(encoded) \n",
    "\n",
    "\n",
    "  \n",
    "# Building the Decoder network \n",
    "decoded = Dense(50, activation ='tanh')(encoded) \n",
    "dcoded = BatchNormalization()(decoded)\n",
    "decoded = Dense(100, activation ='tanh')(decoded) \n",
    "decoded  = BatchNormalization()(decoded)\n",
    "decoded = Dense(200, activation ='tanh')(decoded) \n",
    "decoded  = BatchNormalization()(decoded)\n",
    "# Building the Output Layer \n",
    "output_layer = Dense(X.shape[1], activation ='relu')(decoded) \n",
    "#Step 8: Defining and Training the Auto-encoder\n",
    "# Defining the parameters of the Auto-encoder network \n",
    "autoencoder = Model(input_layer, output_layer) \n",
    "autoencoder.compile(optimizer =\"adam\", loss =\"binary_crossentropy\") \n",
    "# Training the Auto-encoder network \n",
    "autoencoder.fit(X_normal_scaled, X_normal_scaled,  \n",
    "                batch_size = 16, epochs = 100,  \n",
    "                shuffle = True, validation_split = 0.20) \n",
    "\n",
    "hidden_representation = Sequential() \n",
    "hidden_representation.add(autoencoder.layers[0]) \n",
    "hidden_representation.add(autoencoder.layers[1]) \n",
    "hidden_representation.add(autoencoder.layers[2]) \n",
    "hidden_representation.add(autoencoder.layers[3]) \n",
    "\n",
    "# Separating the points encoded by the Auto-encoder as alive and deseased \n",
    "decoded_X_normal = hidden_representation.predict(X_normal_scaled)\n",
    "decoded_X_deseased = hidden_representation.predict(X_deseased_scaled)\n",
    "# Combining the encoded points into a single table  \n",
    "encoded_X = np.append(decoded_X_normal, decoded_X_deseased, axis = 0) \n",
    "y_normal = np.zeros(decoded_X_normal.shape[0]) \n",
    "y_deceased = np.ones(decoded_X_deseased.shape[0]) \n",
    "encoded_y = np.append(y_normal, y_deceased) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting features and the label: 20% test data and 80% assigned to training data\n",
    "# split into train/test sets with same class ratio\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(encoded_X, encoded_y, test_size=0.2, random_state=5, stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# balancing the data\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "# define oversampling strategy\n",
    "over = RandomOverSampler(sampling_strategy='minority')\n",
    "# fit and apply the transform\n",
    "X_train, y_train = over.fit_resample(X_train, y_train)\n",
    "# define undersampling strategy\n",
    "under = RandomUnderSampler(sampling_strategy='majority')\n",
    "# fit and apply the transform\n",
    "X_train, y_train = under.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.bagging module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "# transform the dataset\n",
    "oversample = SMOTE()\n",
    "X_train, y_train = oversample.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data\n",
    "\n",
    "#Scaling the data to standarize them\n",
    "#Here caling reduced the iteration number from 200 to 50\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train = sc.transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 _train= 0.8723702664796634\n",
      "R^2 _test= 0.7165775401069518\n"
     ]
    }
   ],
   "source": [
    "#fitting the model and get the conversion probabilities. \n",
    "#predit_proba() function of our model assigns probability for each row:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(solver='lbfgs', penalty='l2',max_iter=300)\n",
    "model.fit(X_train,y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "lr_probs = model.predict_proba(X_test)[:,1]\n",
    "#Return the mean accuracy on the given test data and taraining data to see if we have overfitting.score clculates R^2\n",
    "print('R^2 _train=',model.score(X_train, y_train))\n",
    "print('R^2 _test=',model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy Scores [0.86713287 0.87412587 0.88811189 0.87412587 0.86713287 0.82517483\n",
      " 0.85211268 0.85211268 0.82394366 0.88028169]\n",
      "CV-scores_min =  0.823943661971831\n",
      "CV_scores_mean = 0.8604254900029547\n",
      "CV_scores_max = 0.8881118881118881\n"
     ]
    }
   ],
   "source": [
    "#Cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(model, X_train, y_train, cv=10)\n",
    "print('Cross-Validation Accuracy Scores', scores)\n",
    "scores = pd.Series(scores)\n",
    "print('CV-scores_min = ',scores.min())\n",
    "print('CV_scores_mean =', scores.mean())\n",
    "print('CV_scores_max =', scores.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Actual  Predicted\n",
       "0      0.0        1.0\n",
       "1      0.0        1.0\n",
       "2      0.0        1.0\n",
       "3      0.0        1.0\n",
       "4      0.0        0.0\n",
       "5      0.0        0.0\n",
       "6      0.0        0.0\n",
       "7      0.0        1.0\n",
       "8      1.0        1.0\n",
       "9      0.0        0.0\n",
       "10     0.0        0.0\n",
       "11     0.0        1.0\n",
       "12     0.0        0.0\n",
       "13     0.0        0.0\n",
       "14     0.0        0.0\n",
       "15     0.0        0.0\n",
       "16     0.0        0.0\n",
       "17     0.0        0.0\n",
       "18     0.0        1.0\n",
       "19     0.0        0.0\n",
       "20     0.0        0.0\n",
       "21     0.0        0.0\n",
       "22     0.0        0.0\n",
       "23     0.0        0.0\n",
       "24     0.0        0.0\n",
       "25     0.0        1.0\n",
       "26     0.0        0.0\n",
       "27     0.0        0.0\n",
       "28     0.0        0.0\n",
       "29     0.0        0.0\n",
       "30     0.0        0.0\n",
       "31     0.0        1.0\n",
       "32     0.0        1.0\n",
       "33     0.0        0.0\n",
       "34     0.0        0.0\n",
       "35     0.0        0.0\n",
       "36     0.0        0.0\n",
       "37     0.0        1.0\n",
       "38     0.0        0.0\n",
       "39     0.0        0.0\n",
       "40     0.0        0.0\n",
       "41     0.0        0.0\n",
       "42     0.0        1.0\n",
       "43     0.0        0.0\n",
       "44     0.0        0.0\n",
       "45     0.0        1.0\n",
       "46     0.0        0.0\n",
       "47     0.0        0.0\n",
       "48     0.0        1.0\n",
       "49     0.0        0.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'Actual': y_test, 'Predicted': y_hat})\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance measurement metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.28342245989304815\n",
      "Mean Squared Error: 0.28342245989304815\n",
      "Root Mean Squared Error: 0.5323743606646062\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_hat))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_hat))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc = 0.9622467771639043\n"
     ]
    }
   ],
   "source": [
    "#Area Under ROC Curve (AUROC) metric\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# Keep only the positive class\n",
    "#lr_probs = [p[1] for p in lr_probs]\n",
    "lr_probs\n",
    "print( 'roc_auc =', roc_auc_score(y_test, lr_probs) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Skill: ROC AUC=0.500\n",
      "Logistic: ROC AUC=0.962\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "ns_probs = [0 for _ in range(len(y_test))]\n",
    "# predict probabilities\n",
    "lr_probs = model.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 1]\n",
    "# calculate scores\n",
    "ns_auc = roc_auc_score(y_test, ns_probs)\n",
    "lr_auc = roc_auc_score(y_test, lr_probs)\n",
    "# summarize scores\n",
    "print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "print('Logistic: ROC AUC=%.3f' % (lr_auc))\n",
    "\n",
    "\n",
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
    "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill',  linewidth=2,  color = '#333F4B')\n",
    "pyplot.plot(lr_fpr, lr_tpr, marker='.', label='Logistic regression', linewidth=2, color = 'darkred')\n",
    "# axis labels\n",
    "pyplot.xlabel('False Positive Rate',fontsize=15)\n",
    "pyplot.ylabel('True Positive Rate',fontsize=15)\n",
    "pyplot.title('Features: Viral Sequences',fontsize=15)\n",
    "# show the legend\n",
    "pyplot.legend(fontsize=15)\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7165775401069518"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "recall_score(y_test, y_hat, average='weighted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

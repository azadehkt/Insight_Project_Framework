{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing the host information including;age, location,gender and status for the possible prediction of outcome of recovery vs Death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "virus_name\n",
      "accession_id\n",
      "type\n",
      "lineage\n",
      "passage_details_history\n",
      "collection_date\n",
      "location\n",
      "host\n",
      "additional_location_information\n",
      "gender\n",
      "age\n",
      "status\n",
      "specimen_source\n",
      "additional_host_information\n",
      "outbreak\n",
      "last_vaccinated\n",
      "treatment\n",
      "sequencing_technology\n",
      "assembly_method\n",
      "coverage\n",
      "comment\n",
      "originating_lab\n",
      "address\n",
      "sample_id_given_by_the_sample_provider\n",
      "submitting_lab\n",
      "sample_id_given_by_the_submitting_laboratory\n",
      "authors\n",
      "submitter\n",
      "submission_date\n",
      "query\n",
      "strand\n",
      "%n\n",
      "length(nt)\n",
      "length(aa)\n",
      "#muts\n",
      "%muts\n",
      "#uniquemuts\n",
      "%uniquemuts\n",
      "#existingmuts\n",
      "%existingmuts\n",
      "symbol\n",
      "reference\n",
      "uniquemutlist\n",
      "existingmutlist\n",
      "clade\n",
      "ifexistspecialchar\n"
     ]
    }
   ],
   "source": [
    "# iterating the columns \n",
    "for col in data.columns: \n",
    "    print(col) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#important features\n",
    "start_data_row = 0\n",
    "Final_data_row = 1129\n",
    "Data = data.loc[start_data_row:Final_data_row, ['status','%n','length(nt)','length(aa)',\n",
    "                                                   '%muts','%uniquemuts','%existingmuts','existingmutlist','clade']]\n",
    "#change the index of the data according to the length of the new data\n",
    "Data.index = range(len(Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.05\n",
       "1       0.04\n",
       "2       0.06\n",
       "3       0.03\n",
       "4       0.02\n",
       "        ... \n",
       "1123    0.05\n",
       "1124    0.05\n",
       "1125    0.05\n",
       "1126    0.07\n",
       "1127    0.06\n",
       "Name: %existingmuts, Length: 1128, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing the % from the data\n",
    "Data['%n'] = Data['%n'].str.replace('%', ' ') \n",
    "Data['%muts'] = Data['%muts'].str.replace('%', ' ') \n",
    "Data['%uniquemuts'] = Data['%uniquemuts'].str.replace('%', ' ') \n",
    "Data['%existingmuts'] = Data['%existingmuts'].str.replace('%', ' ') \n",
    "Data['%n'].astype(float)\n",
    "Data['%muts'].astype(float)\n",
    "Data['%uniquemuts'].astype(float)\n",
    "Data['%existingmuts'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting rid of NaN data\n",
    "Data.dropna(subset = ['status'], inplace=True)\n",
    "Data.dropna(subset = ['existingmutlist'], inplace=True)\n",
    "Data.drop(Data.loc[Data['status']=='unknown'].index, inplace=True)\n",
    "\n",
    "Data.drop(Data.loc[Data['status']=='n/a'].index, inplace=True)\n",
    "\n",
    "Data.drop(Data.loc[Data['status']=='NA'].index, inplace=True)\n",
    "\n",
    "Data.drop(Data.loc[Data['status']=='-'].index, inplace=True)\n",
    "\n",
    "Data.drop(Data.loc[Data['status']=='Unknown'].index, inplace=True)\n",
    "\n",
    "Data.index = range(len(Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chacking if there is any null data in ExistingMutList\n",
    "for i in range(len(Data)):\n",
    "    if pd.isnull(Data.existingmutlist[i]) is True:\n",
    "        print('True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting rid of NaN data\n",
    "Data.drop(Data.loc[Data['status']=='unknown'].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#Labeling\n",
    "Data.replace(['Deceased'],value= [1], inplace=True)\n",
    "Data.status[Data['status'] != 1]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>existingmutlist_(E_T30I)</th>\n",
       "      <th>existingmutlist_(NS3_G196V,NS8_G8R,NS8_L84S)</th>\n",
       "      <th>existingmutlist_(NS3_G251V)</th>\n",
       "      <th>existingmutlist_(NS7b_T40I,NS8_L84S)</th>\n",
       "      <th>existingmutlist_(NS8_L84S)</th>\n",
       "      <th>existingmutlist_(NS8_L84S,N_G238C)</th>\n",
       "      <th>existingmutlist_(NS8_L84S,N_S202N)</th>\n",
       "      <th>existingmutlist_(NSP10_T111I,NSP12_P323L,Spike_D614G,N_N140T,N_G97S)</th>\n",
       "      <th>existingmutlist_(NSP10_T51A)</th>\n",
       "      <th>existingmutlist_(NSP12_A449T,NSP12_P323L,Spike_D614G,NS3_Q57H,NS7a_V24F)</th>\n",
       "      <th>...</th>\n",
       "      <th>clade_G</th>\n",
       "      <th>clade_Other</th>\n",
       "      <th>clade_S</th>\n",
       "      <th>clade_V</th>\n",
       "      <th>status</th>\n",
       "      <th>%n</th>\n",
       "      <th>length(nt)</th>\n",
       "      <th>length(aa)</th>\n",
       "      <th>%muts</th>\n",
       "      <th>%uniquemuts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29890</td>\n",
       "      <td>9710</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29859</td>\n",
       "      <td>9710</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29917</td>\n",
       "      <td>9710</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29892</td>\n",
       "      <td>9710</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29828</td>\n",
       "      <td>9710</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 411 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   existingmutlist_(E_T30I)  existingmutlist_(NS3_G196V,NS8_G8R,NS8_L84S)  \\\n",
       "0                         0                                             0   \n",
       "1                         0                                             0   \n",
       "2                         0                                             0   \n",
       "3                         0                                             0   \n",
       "4                         0                                             0   \n",
       "\n",
       "   existingmutlist_(NS3_G251V)  existingmutlist_(NS7b_T40I,NS8_L84S)  \\\n",
       "0                            0                                     0   \n",
       "1                            0                                     0   \n",
       "2                            0                                     0   \n",
       "3                            0                                     0   \n",
       "4                            0                                     0   \n",
       "\n",
       "   existingmutlist_(NS8_L84S)  existingmutlist_(NS8_L84S,N_G238C)  \\\n",
       "0                           0                                   0   \n",
       "1                           0                                   0   \n",
       "2                           0                                   0   \n",
       "3                           0                                   0   \n",
       "4                           0                                   0   \n",
       "\n",
       "   existingmutlist_(NS8_L84S,N_S202N)  \\\n",
       "0                                   0   \n",
       "1                                   0   \n",
       "2                                   0   \n",
       "3                                   0   \n",
       "4                                   0   \n",
       "\n",
       "   existingmutlist_(NSP10_T111I,NSP12_P323L,Spike_D614G,N_N140T,N_G97S)  \\\n",
       "0                                                  0                      \n",
       "1                                                  0                      \n",
       "2                                                  0                      \n",
       "3                                                  0                      \n",
       "4                                                  0                      \n",
       "\n",
       "   existingmutlist_(NSP10_T51A)  \\\n",
       "0                             0   \n",
       "1                             0   \n",
       "2                             0   \n",
       "3                             0   \n",
       "4                             0   \n",
       "\n",
       "   existingmutlist_(NSP12_A449T,NSP12_P323L,Spike_D614G,NS3_Q57H,NS7a_V24F)  \\\n",
       "0                                                  0                          \n",
       "1                                                  0                          \n",
       "2                                                  0                          \n",
       "3                                                  0                          \n",
       "4                                                  0                          \n",
       "\n",
       "   ...  clade_G  clade_Other  clade_S  clade_V  status     %n  length(nt)  \\\n",
       "0  ...        0            0        1        0       0  0.00        29890   \n",
       "1  ...        0            0        1        0       0  0.00        29859   \n",
       "2  ...        0            0        1        0       0  0.00        29917   \n",
       "3  ...        1            0        0        0       0  0.00        29892   \n",
       "4  ...        1            0        0        0       0  0.00        29828   \n",
       "\n",
       "   length(aa)  %muts  %uniquemuts  \n",
       "0        9710  0.04         0.00   \n",
       "1        9710  0.04         0.00   \n",
       "2        9710  0.05         0.01   \n",
       "3        9710  0.08         0.01   \n",
       "4        9710  0.02         0.00   \n",
       "\n",
       "[5 rows x 411 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using dummies instead of location data for ML input\n",
    "Data_model = pd.concat([pd.get_dummies(Data[['existingmutlist']]),pd.get_dummies(Data[['clade']]), Data[['status','%n','length(nt)','length(aa)',\n",
    "                                                   '%muts','%uniquemuts']]], axis=1)\n",
    "Data_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    894\n",
       "1     39\n",
       "Name: status, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the balance in the data\n",
    "Data_model['status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>existingmutlist_(E_T30I)</th>\n",
       "      <th>existingmutlist_(NS3_G196V,NS8_G8R,NS8_L84S)</th>\n",
       "      <th>existingmutlist_(NS3_G251V)</th>\n",
       "      <th>existingmutlist_(NS7b_T40I,NS8_L84S)</th>\n",
       "      <th>existingmutlist_(NS8_L84S)</th>\n",
       "      <th>existingmutlist_(NS8_L84S,N_G238C)</th>\n",
       "      <th>existingmutlist_(NS8_L84S,N_S202N)</th>\n",
       "      <th>existingmutlist_(NSP10_T111I,NSP12_P323L,Spike_D614G,N_N140T,N_G97S)</th>\n",
       "      <th>existingmutlist_(NSP10_T51A)</th>\n",
       "      <th>existingmutlist_(NSP12_A449T,NSP12_P323L,Spike_D614G,NS3_Q57H,NS7a_V24F)</th>\n",
       "      <th>...</th>\n",
       "      <th>existingmutlist_(Spike_T307I,NS3_G251V)</th>\n",
       "      <th>clade_G</th>\n",
       "      <th>clade_Other</th>\n",
       "      <th>clade_S</th>\n",
       "      <th>clade_V</th>\n",
       "      <th>%n</th>\n",
       "      <th>length(nt)</th>\n",
       "      <th>length(aa)</th>\n",
       "      <th>%muts</th>\n",
       "      <th>%uniquemuts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29890.0</td>\n",
       "      <td>9710.0</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29859.0</td>\n",
       "      <td>9710.0</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29917.0</td>\n",
       "      <td>9710.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29892.0</td>\n",
       "      <td>9710.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29828.0</td>\n",
       "      <td>9710.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.44</td>\n",
       "      <td>29903.0</td>\n",
       "      <td>9710.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.45</td>\n",
       "      <td>29903.0</td>\n",
       "      <td>9710.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.44</td>\n",
       "      <td>29903.0</td>\n",
       "      <td>9710.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.45</td>\n",
       "      <td>29903.0</td>\n",
       "      <td>9710.0</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.04</td>\n",
       "      <td>29776.0</td>\n",
       "      <td>9682.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>933 rows × 410 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     existingmutlist_(E_T30I)  existingmutlist_(NS3_G196V,NS8_G8R,NS8_L84S)  \\\n",
       "0                         0.0                                           0.0   \n",
       "1                         0.0                                           0.0   \n",
       "2                         0.0                                           0.0   \n",
       "3                         0.0                                           0.0   \n",
       "4                         0.0                                           0.0   \n",
       "..                        ...                                           ...   \n",
       "928                       0.0                                           0.0   \n",
       "929                       0.0                                           0.0   \n",
       "930                       0.0                                           0.0   \n",
       "931                       0.0                                           0.0   \n",
       "932                       0.0                                           0.0   \n",
       "\n",
       "     existingmutlist_(NS3_G251V)  existingmutlist_(NS7b_T40I,NS8_L84S)  \\\n",
       "0                            0.0                                   0.0   \n",
       "1                            0.0                                   0.0   \n",
       "2                            0.0                                   0.0   \n",
       "3                            0.0                                   0.0   \n",
       "4                            0.0                                   0.0   \n",
       "..                           ...                                   ...   \n",
       "928                          0.0                                   0.0   \n",
       "929                          0.0                                   0.0   \n",
       "930                          0.0                                   0.0   \n",
       "931                          0.0                                   0.0   \n",
       "932                          0.0                                   0.0   \n",
       "\n",
       "     existingmutlist_(NS8_L84S)  existingmutlist_(NS8_L84S,N_G238C)  \\\n",
       "0                           0.0                                 0.0   \n",
       "1                           0.0                                 0.0   \n",
       "2                           0.0                                 0.0   \n",
       "3                           0.0                                 0.0   \n",
       "4                           0.0                                 0.0   \n",
       "..                          ...                                 ...   \n",
       "928                         0.0                                 0.0   \n",
       "929                         0.0                                 0.0   \n",
       "930                         0.0                                 0.0   \n",
       "931                         0.0                                 0.0   \n",
       "932                         0.0                                 0.0   \n",
       "\n",
       "     existingmutlist_(NS8_L84S,N_S202N)  \\\n",
       "0                                   0.0   \n",
       "1                                   0.0   \n",
       "2                                   0.0   \n",
       "3                                   0.0   \n",
       "4                                   0.0   \n",
       "..                                  ...   \n",
       "928                                 0.0   \n",
       "929                                 0.0   \n",
       "930                                 0.0   \n",
       "931                                 0.0   \n",
       "932                                 0.0   \n",
       "\n",
       "     existingmutlist_(NSP10_T111I,NSP12_P323L,Spike_D614G,N_N140T,N_G97S)  \\\n",
       "0                                                  0.0                      \n",
       "1                                                  0.0                      \n",
       "2                                                  0.0                      \n",
       "3                                                  0.0                      \n",
       "4                                                  0.0                      \n",
       "..                                                 ...                      \n",
       "928                                                0.0                      \n",
       "929                                                0.0                      \n",
       "930                                                0.0                      \n",
       "931                                                0.0                      \n",
       "932                                                0.0                      \n",
       "\n",
       "     existingmutlist_(NSP10_T51A)  \\\n",
       "0                             0.0   \n",
       "1                             0.0   \n",
       "2                             0.0   \n",
       "3                             0.0   \n",
       "4                             0.0   \n",
       "..                            ...   \n",
       "928                           0.0   \n",
       "929                           0.0   \n",
       "930                           0.0   \n",
       "931                           0.0   \n",
       "932                           0.0   \n",
       "\n",
       "     existingmutlist_(NSP12_A449T,NSP12_P323L,Spike_D614G,NS3_Q57H,NS7a_V24F)  \\\n",
       "0                                                  0.0                          \n",
       "1                                                  0.0                          \n",
       "2                                                  0.0                          \n",
       "3                                                  0.0                          \n",
       "4                                                  0.0                          \n",
       "..                                                 ...                          \n",
       "928                                                0.0                          \n",
       "929                                                0.0                          \n",
       "930                                                0.0                          \n",
       "931                                                0.0                          \n",
       "932                                                0.0                          \n",
       "\n",
       "     ...  existingmutlist_(Spike_T307I,NS3_G251V)  clade_G  clade_Other  \\\n",
       "0    ...                                      0.0      0.0          0.0   \n",
       "1    ...                                      0.0      0.0          0.0   \n",
       "2    ...                                      0.0      0.0          0.0   \n",
       "3    ...                                      0.0      1.0          0.0   \n",
       "4    ...                                      0.0      1.0          0.0   \n",
       "..   ...                                      ...      ...          ...   \n",
       "928  ...                                      0.0      1.0          0.0   \n",
       "929  ...                                      0.0      1.0          0.0   \n",
       "930  ...                                      0.0      1.0          0.0   \n",
       "931  ...                                      0.0      1.0          0.0   \n",
       "932  ...                                      0.0      1.0          0.0   \n",
       "\n",
       "     clade_S  clade_V    %n  length(nt)  length(aa)  %muts  %uniquemuts  \n",
       "0        1.0      0.0  0.00     29890.0      9710.0   0.04         0.00  \n",
       "1        1.0      0.0  0.00     29859.0      9710.0   0.04         0.00  \n",
       "2        1.0      0.0  0.00     29917.0      9710.0   0.05         0.01  \n",
       "3        0.0      0.0  0.00     29892.0      9710.0   0.08         0.01  \n",
       "4        0.0      0.0  0.00     29828.0      9710.0   0.02         0.00  \n",
       "..       ...      ...   ...         ...         ...    ...          ...  \n",
       "928      0.0      0.0  0.44     29903.0      9710.0   0.05         0.00  \n",
       "929      0.0      0.0  0.45     29903.0      9710.0   0.05         0.00  \n",
       "930      0.0      0.0  0.44     29903.0      9710.0   0.05         0.00  \n",
       "931      0.0      0.0  0.45     29903.0      9710.0   0.07         0.00  \n",
       "932      0.0      0.0  1.04     29776.0      9682.0   0.06         0.00  \n",
       "\n",
       "[933 rows x 410 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Separating target (label) data\n",
    "X = Data_model.drop('status',axis=1).astype(float)\n",
    "y = Data_model.status\n",
    "y=y.astype(int)\n",
    "y.value_counts()\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Scaling the data to standarize them\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "sc = StandardScaler()\n",
    "#sc = MinMaxScaler()\n",
    "sc.fit(X)\n",
    "X_scaled = sc.transform(X)\n",
    "X_normal_scaled = X_scaled[y == 0] \n",
    "X_deseased_scaled = X_scaled[y == 1] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_deseased_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple feedforward autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from keras.layers import Input, Dense, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras.layers import Input, Dense, BatchNormalization\n",
    "# Building the Encoder network \n",
    "window_length = 410\n",
    "encoding_dim = 30\n",
    "epochs = 300\n",
    "input_window = Input(shape=(window_length,))\n",
    "x = Dense(100, activation='tanh')(input_window)\n",
    "x = BatchNormalization()(x)\n",
    "encoded = Dense(encoding_dim, activation='tanh')(x)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "x = Dense(100, activation='tanh')(encoded)\n",
    "x = BatchNormalization()(x)\n",
    "decoded = Dense(window_length, activation='tanh')(x)\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_window, decoded)\n",
    "\n",
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_window, encoded)\n",
    "autoencoder.summary()\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.fit(X_normal_scaled, X_normal_scaled,\n",
    "                epochs=epochs,\n",
    "                batch_size=16)\n",
    "# Separating the points encoded by the Auto-encoder as normal and fraud \n",
    "decoded_X_normal = autoencoder.predict(X_normal_scaled)\n",
    "decoded_X_deseased = autoencoder.predict(X_deseased_scaled)\n",
    "# Combining the encoded points into a single table  \n",
    "encoded_X = np.append(decoded_X_normal, decoded_X_deseased, axis = 0) \n",
    "y_normal = np.zeros(decoded_X_normal.shape[0]) \n",
    "y_deceased = np.ones(decoded_X_deseased.shape[0]) \n",
    "encoded_y = np.append(y_normal, y_deceased) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 410)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               82200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 410)               82410     \n",
      "=================================================================\n",
      "Total params: 164,610\n",
      "Trainable params: 164,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 715 samples, validate on 179 samples\n",
      "Epoch 1/500\n",
      "715/715 [==============================] - 0s 316us/step - loss: 0.7379 - val_loss: 0.6987\n",
      "Epoch 2/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: 0.6935 - val_loss: 0.6585\n",
      "Epoch 3/500\n",
      "715/715 [==============================] - 0s 46us/step - loss: 0.6516 - val_loss: 0.6207\n",
      "Epoch 4/500\n",
      "715/715 [==============================] - 0s 46us/step - loss: 0.6118 - val_loss: 0.5853\n",
      "Epoch 5/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: 0.5739 - val_loss: 0.5523\n",
      "Epoch 6/500\n",
      "715/715 [==============================] - 0s 45us/step - loss: 0.5379 - val_loss: 0.5217\n",
      "Epoch 7/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: 0.5034 - val_loss: 0.4934\n",
      "Epoch 8/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: 0.4704 - val_loss: 0.4672\n",
      "Epoch 9/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: 0.4387 - val_loss: 0.4430\n",
      "Epoch 10/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: 0.4082 - val_loss: 0.4207\n",
      "Epoch 11/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: 0.3788 - val_loss: 0.4000\n",
      "Epoch 12/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: 0.3505 - val_loss: 0.3809\n",
      "Epoch 13/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: 0.3231 - val_loss: 0.3632\n",
      "Epoch 14/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: 0.2967 - val_loss: 0.3467\n",
      "Epoch 15/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: 0.2711 - val_loss: 0.3314\n",
      "Epoch 16/500\n",
      "715/715 [==============================] - 0s 46us/step - loss: 0.2463 - val_loss: 0.3171\n",
      "Epoch 17/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: 0.2222 - val_loss: 0.3037\n",
      "Epoch 18/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: 0.1989 - val_loss: 0.2912\n",
      "Epoch 19/500\n",
      "715/715 [==============================] - 0s 57us/step - loss: 0.1762 - val_loss: 0.2794\n",
      "Epoch 20/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: 0.1542 - val_loss: 0.2684\n",
      "Epoch 21/500\n",
      "715/715 [==============================] - 0s 46us/step - loss: 0.1327 - val_loss: 0.2579\n",
      "Epoch 22/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: 0.1119 - val_loss: 0.2481\n",
      "Epoch 23/500\n",
      "715/715 [==============================] - 0s 46us/step - loss: 0.0916 - val_loss: 0.2389\n",
      "Epoch 24/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: 0.0718 - val_loss: 0.2301\n",
      "Epoch 25/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: 0.0525 - val_loss: 0.2219\n",
      "Epoch 26/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: 0.0336 - val_loss: 0.2141\n",
      "Epoch 27/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: 0.0150 - val_loss: 0.2067\n",
      "Epoch 28/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -0.0029 - val_loss: 0.1999\n",
      "Epoch 29/500\n",
      "715/715 [==============================] - 0s 46us/step - loss: -0.0204 - val_loss: 0.1934\n",
      "Epoch 30/500\n",
      "715/715 [==============================] - 0s 46us/step - loss: -0.0376 - val_loss: 0.1874\n",
      "Epoch 31/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -0.0543 - val_loss: 0.1817\n",
      "Epoch 32/500\n",
      "715/715 [==============================] - 0s 46us/step - loss: -0.0707 - val_loss: 0.1764\n",
      "Epoch 33/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -0.0868 - val_loss: 0.1715\n",
      "Epoch 34/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -0.1026 - val_loss: 0.1669\n",
      "Epoch 35/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -0.1180 - val_loss: 0.1626\n",
      "Epoch 36/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -0.1333 - val_loss: 0.1587\n",
      "Epoch 37/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -0.1482 - val_loss: 0.1551\n",
      "Epoch 38/500\n",
      "715/715 [==============================] - 0s 46us/step - loss: -0.1630 - val_loss: 0.1517\n",
      "Epoch 39/500\n",
      "715/715 [==============================] - 0s 45us/step - loss: -0.1776 - val_loss: 0.1486\n",
      "Epoch 40/500\n",
      "715/715 [==============================] - 0s 46us/step - loss: -0.1920 - val_loss: 0.1458\n",
      "Epoch 41/500\n",
      "715/715 [==============================] - 0s 46us/step - loss: -0.2063 - val_loss: 0.1432\n",
      "Epoch 42/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -0.2205 - val_loss: 0.1409\n",
      "Epoch 43/500\n",
      "715/715 [==============================] - 0s 46us/step - loss: -0.2346 - val_loss: 0.1388\n",
      "Epoch 44/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -0.2487 - val_loss: 0.1368\n",
      "Epoch 45/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -0.2627 - val_loss: 0.1351\n",
      "Epoch 46/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -0.2767 - val_loss: 0.1335\n",
      "Epoch 47/500\n",
      "715/715 [==============================] - 0s 46us/step - loss: -0.2906 - val_loss: 0.1321\n",
      "Epoch 48/500\n",
      "715/715 [==============================] - 0s 45us/step - loss: -0.3047 - val_loss: 0.1308\n",
      "Epoch 49/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -0.3187 - val_loss: 0.1297\n",
      "Epoch 50/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -0.3328 - val_loss: 0.1287\n",
      "Epoch 51/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -0.3470 - val_loss: 0.1279\n",
      "Epoch 52/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -0.3612 - val_loss: 0.1271\n",
      "Epoch 53/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -0.3756 - val_loss: 0.1265\n",
      "Epoch 54/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -0.3901 - val_loss: 0.1259\n",
      "Epoch 55/500\n",
      "715/715 [==============================] - 0s 46us/step - loss: -0.4046 - val_loss: 0.1255\n",
      "Epoch 56/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -0.4194 - val_loss: 0.1252\n",
      "Epoch 57/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -0.4343 - val_loss: 0.1249\n",
      "Epoch 58/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -0.4493 - val_loss: 0.1247\n",
      "Epoch 59/500\n",
      "715/715 [==============================] - 0s 66us/step - loss: -0.4644 - val_loss: 0.1245\n",
      "Epoch 60/500\n",
      "715/715 [==============================] - 0s 67us/step - loss: -0.4800 - val_loss: 0.1245\n",
      "Epoch 61/500\n",
      "715/715 [==============================] - 0s 68us/step - loss: -0.4954 - val_loss: 0.1245\n",
      "Epoch 62/500\n",
      "715/715 [==============================] - 0s 68us/step - loss: -0.5109 - val_loss: 0.1245\n",
      "Epoch 63/500\n",
      "715/715 [==============================] - 0s 59us/step - loss: -0.5264 - val_loss: 0.1246\n",
      "Epoch 64/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -0.5421 - val_loss: 0.1247\n",
      "Epoch 65/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -0.5581 - val_loss: 0.1249\n",
      "Epoch 66/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -0.5742 - val_loss: 0.1251\n",
      "Epoch 67/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715/715 [==============================] - 0s 56us/step - loss: -0.5902 - val_loss: 0.1253\n",
      "Epoch 68/500\n",
      "715/715 [==============================] - 0s 60us/step - loss: -0.6062 - val_loss: 0.1256\n",
      "Epoch 69/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -0.6217 - val_loss: 0.1259\n",
      "Epoch 70/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -0.6374 - val_loss: 0.1261\n",
      "Epoch 71/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -0.6523 - val_loss: 0.1264\n",
      "Epoch 72/500\n",
      "715/715 [==============================] - 0s 48us/step - loss: -0.6666 - val_loss: 0.1266\n",
      "Epoch 73/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -0.6805 - val_loss: 0.1267\n",
      "Epoch 74/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -0.6935 - val_loss: 0.1268\n",
      "Epoch 75/500\n",
      "715/715 [==============================] - 0s 44us/step - loss: -0.7063 - val_loss: 0.1268\n",
      "Epoch 76/500\n",
      "715/715 [==============================] - 0s 46us/step - loss: -0.7188 - val_loss: 0.1266\n",
      "Epoch 77/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -0.7306 - val_loss: 0.1264\n",
      "Epoch 78/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -0.7418 - val_loss: 0.1261\n",
      "Epoch 79/500\n",
      "715/715 [==============================] - 0s 46us/step - loss: -0.7531 - val_loss: 0.1256\n",
      "Epoch 80/500\n",
      "715/715 [==============================] - 0s 46us/step - loss: -0.7640 - val_loss: 0.1251\n",
      "Epoch 81/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -0.7747 - val_loss: 0.1244\n",
      "Epoch 82/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -0.7846 - val_loss: 0.1236\n",
      "Epoch 83/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -0.7945 - val_loss: 0.1227\n",
      "Epoch 84/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -0.8040 - val_loss: 0.1217\n",
      "Epoch 85/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -0.8132 - val_loss: 0.1205\n",
      "Epoch 86/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -0.8223 - val_loss: 0.1192\n",
      "Epoch 87/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -0.8311 - val_loss: 0.1179\n",
      "Epoch 88/500\n",
      "715/715 [==============================] - 0s 45us/step - loss: -0.8396 - val_loss: 0.1164\n",
      "Epoch 89/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -0.8483 - val_loss: 0.1148\n",
      "Epoch 90/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -0.8570 - val_loss: 0.1132\n",
      "Epoch 91/500\n",
      "715/715 [==============================] - 0s 46us/step - loss: -0.8652 - val_loss: 0.1114\n",
      "Epoch 92/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -0.8735 - val_loss: 0.1096\n",
      "Epoch 93/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -0.8819 - val_loss: 0.1077\n",
      "Epoch 94/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -0.8901 - val_loss: 0.1058\n",
      "Epoch 95/500\n",
      "715/715 [==============================] - 0s 47us/step - loss: -0.8983 - val_loss: 0.1039\n",
      "Epoch 96/500\n",
      "715/715 [==============================] - 0s 46us/step - loss: -0.9066 - val_loss: 0.1018\n",
      "Epoch 97/500\n",
      "715/715 [==============================] - 0s 46us/step - loss: -0.9148 - val_loss: 0.0998\n",
      "Epoch 98/500\n",
      "715/715 [==============================] - 0s 67us/step - loss: -0.9230 - val_loss: 0.0977\n",
      "Epoch 99/500\n",
      "715/715 [==============================] - 0s 82us/step - loss: -0.9311 - val_loss: 0.0956\n",
      "Epoch 100/500\n",
      "715/715 [==============================] - 0s 95us/step - loss: -0.9393 - val_loss: 0.0934\n",
      "Epoch 101/500\n",
      "715/715 [==============================] - 0s 82us/step - loss: -0.9475 - val_loss: 0.0913\n",
      "Epoch 102/500\n",
      "715/715 [==============================] - 0s 89us/step - loss: -0.9557 - val_loss: 0.0891\n",
      "Epoch 103/500\n",
      "715/715 [==============================] - 0s 112us/step - loss: -0.9639 - val_loss: 0.0870\n",
      "Epoch 104/500\n",
      "715/715 [==============================] - 0s 98us/step - loss: -0.9721 - val_loss: 0.0849\n",
      "Epoch 105/500\n",
      "715/715 [==============================] - 0s 92us/step - loss: -0.9805 - val_loss: 0.0828\n",
      "Epoch 106/500\n",
      "715/715 [==============================] - 0s 78us/step - loss: -0.9887 - val_loss: 0.0808\n",
      "Epoch 107/500\n",
      "715/715 [==============================] - 0s 68us/step - loss: -0.9967 - val_loss: 0.0788\n",
      "Epoch 108/500\n",
      "715/715 [==============================] - 0s 64us/step - loss: -1.0050 - val_loss: 0.0767\n",
      "Epoch 109/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.0130 - val_loss: 0.0747\n",
      "Epoch 110/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.0210 - val_loss: 0.0730\n",
      "Epoch 111/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.0292 - val_loss: 0.0712\n",
      "Epoch 112/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.0371 - val_loss: 0.0694\n",
      "Epoch 113/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.0450 - val_loss: 0.0677\n",
      "Epoch 114/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.0529 - val_loss: 0.0660\n",
      "Epoch 115/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.0607 - val_loss: 0.0643\n",
      "Epoch 116/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.0683 - val_loss: 0.0626\n",
      "Epoch 117/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -1.0760 - val_loss: 0.0610\n",
      "Epoch 118/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -1.0837 - val_loss: 0.0594\n",
      "Epoch 119/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -1.0912 - val_loss: 0.0578\n",
      "Epoch 120/500\n",
      "715/715 [==============================] - 0s 51us/step - loss: -1.0988 - val_loss: 0.0562\n",
      "Epoch 121/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.1061 - val_loss: 0.0547\n",
      "Epoch 122/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.1135 - val_loss: 0.0533\n",
      "Epoch 123/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.1207 - val_loss: 0.0518\n",
      "Epoch 124/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.1280 - val_loss: 0.0504\n",
      "Epoch 125/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -1.1348 - val_loss: 0.0490\n",
      "Epoch 126/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.1417 - val_loss: 0.0476\n",
      "Epoch 127/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.1483 - val_loss: 0.0463\n",
      "Epoch 128/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.1549 - val_loss: 0.0450\n",
      "Epoch 129/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.1612 - val_loss: 0.0438\n",
      "Epoch 130/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.1674 - val_loss: 0.0426\n",
      "Epoch 131/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.1735 - val_loss: 0.0415\n",
      "Epoch 132/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.1795 - val_loss: 0.0404\n",
      "Epoch 133/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.1852 - val_loss: 0.0393\n",
      "Epoch 134/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.1908 - val_loss: 0.0382\n",
      "Epoch 135/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -1.1962 - val_loss: 0.0371\n",
      "Epoch 136/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -1.2015 - val_loss: 0.0360\n",
      "Epoch 137/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -1.2062 - val_loss: 0.0350\n",
      "Epoch 138/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.2113 - val_loss: 0.0341\n",
      "Epoch 139/500\n",
      "715/715 [==============================] - 0s 57us/step - loss: -1.2158 - val_loss: 0.0331\n",
      "Epoch 140/500\n",
      "715/715 [==============================] - 0s 89us/step - loss: -1.2200 - val_loss: 0.0322\n",
      "Epoch 141/500\n",
      "715/715 [==============================] - 0s 64us/step - loss: -1.2242 - val_loss: 0.0312\n",
      "Epoch 142/500\n",
      "715/715 [==============================] - 0s 64us/step - loss: -1.2280 - val_loss: 0.0303\n",
      "Epoch 143/500\n",
      "715/715 [==============================] - 0s 61us/step - loss: -1.2317 - val_loss: 0.0294\n",
      "Epoch 144/500\n",
      "715/715 [==============================] - 0s 60us/step - loss: -1.2352 - val_loss: 0.0286\n",
      "Epoch 145/500\n",
      "715/715 [==============================] - 0s 64us/step - loss: -1.2385 - val_loss: 0.0275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 146/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.2415 - val_loss: 0.0267\n",
      "Epoch 147/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.2444 - val_loss: 0.0259\n",
      "Epoch 148/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.2470 - val_loss: 0.0249\n",
      "Epoch 149/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -1.2496 - val_loss: 0.0241\n",
      "Epoch 150/500\n",
      "715/715 [==============================] - 0s 59us/step - loss: -1.2519 - val_loss: 0.0232\n",
      "Epoch 151/500\n",
      "715/715 [==============================] - 0s 73us/step - loss: -1.2542 - val_loss: 0.0223\n",
      "Epoch 152/500\n",
      "715/715 [==============================] - 0s 68us/step - loss: -1.2564 - val_loss: 0.0214\n",
      "Epoch 153/500\n",
      "715/715 [==============================] - 0s 89us/step - loss: -1.2586 - val_loss: 0.0204\n",
      "Epoch 154/500\n",
      "715/715 [==============================] - 0s 110us/step - loss: -1.2606 - val_loss: 0.0194\n",
      "Epoch 155/500\n",
      "715/715 [==============================] - 0s 77us/step - loss: -1.2626 - val_loss: 0.0185\n",
      "Epoch 156/500\n",
      "715/715 [==============================] - 0s 78us/step - loss: -1.2643 - val_loss: 0.0174\n",
      "Epoch 157/500\n",
      "715/715 [==============================] - 0s 70us/step - loss: -1.2662 - val_loss: 0.0164\n",
      "Epoch 158/500\n",
      "715/715 [==============================] - 0s 60us/step - loss: -1.2680 - val_loss: 0.0153\n",
      "Epoch 159/500\n",
      "715/715 [==============================] - 0s 71us/step - loss: -1.2696 - val_loss: 0.0144\n",
      "Epoch 160/500\n",
      "715/715 [==============================] - 0s 91us/step - loss: -1.2712 - val_loss: 0.0135\n",
      "Epoch 161/500\n",
      "715/715 [==============================] - 0s 73us/step - loss: -1.2727 - val_loss: 0.0126\n",
      "Epoch 162/500\n",
      "715/715 [==============================] - 0s 69us/step - loss: -1.2741 - val_loss: 0.0116\n",
      "Epoch 163/500\n",
      "715/715 [==============================] - 0s 71us/step - loss: -1.2755 - val_loss: 0.0107\n",
      "Epoch 164/500\n",
      "715/715 [==============================] - 0s 66us/step - loss: -1.2768 - val_loss: 0.0098\n",
      "Epoch 165/500\n",
      "715/715 [==============================] - 0s 71us/step - loss: -1.2780 - val_loss: 0.0088\n",
      "Epoch 166/500\n",
      "715/715 [==============================] - 0s 74us/step - loss: -1.2792 - val_loss: 0.0079\n",
      "Epoch 167/500\n",
      "715/715 [==============================] - 0s 109us/step - loss: -1.2804 - val_loss: 0.0068\n",
      "Epoch 168/500\n",
      "715/715 [==============================] - 0s 75us/step - loss: -1.2815 - val_loss: 0.0059\n",
      "Epoch 169/500\n",
      "715/715 [==============================] - 0s 67us/step - loss: -1.2825 - val_loss: 0.0048\n",
      "Epoch 170/500\n",
      "715/715 [==============================] - 0s 101us/step - loss: -1.2836 - val_loss: 0.0038\n",
      "Epoch 171/500\n",
      "715/715 [==============================] - 0s 63us/step - loss: -1.2846 - val_loss: 0.0027\n",
      "Epoch 172/500\n",
      "715/715 [==============================] - 0s 66us/step - loss: -1.2856 - val_loss: 0.0016\n",
      "Epoch 173/500\n",
      "715/715 [==============================] - 0s 62us/step - loss: -1.2866 - val_loss: 5.1390e-04\n",
      "Epoch 174/500\n",
      "715/715 [==============================] - 0s 77us/step - loss: -1.2875 - val_loss: -6.1464e-04\n",
      "Epoch 175/500\n",
      "715/715 [==============================] - 0s 64us/step - loss: -1.2884 - val_loss: -0.0017\n",
      "Epoch 176/500\n",
      "715/715 [==============================] - 0s 67us/step - loss: -1.2893 - val_loss: -0.0029\n",
      "Epoch 177/500\n",
      "715/715 [==============================] - 0s 87us/step - loss: -1.2902 - val_loss: -0.0040\n",
      "Epoch 178/500\n",
      "715/715 [==============================] - 0s 64us/step - loss: -1.2911 - val_loss: -0.0051\n",
      "Epoch 179/500\n",
      "715/715 [==============================] - 0s 60us/step - loss: -1.2920 - val_loss: -0.0063\n",
      "Epoch 180/500\n",
      "715/715 [==============================] - 0s 60us/step - loss: -1.2928 - val_loss: -0.0075\n",
      "Epoch 181/500\n",
      "715/715 [==============================] - 0s 66us/step - loss: -1.2937 - val_loss: -0.0087\n",
      "Epoch 182/500\n",
      "715/715 [==============================] - 0s 60us/step - loss: -1.2945 - val_loss: -0.0100\n",
      "Epoch 183/500\n",
      "715/715 [==============================] - 0s 84us/step - loss: -1.2953 - val_loss: -0.0111\n",
      "Epoch 184/500\n",
      "715/715 [==============================] - 0s 57us/step - loss: -1.2962 - val_loss: -0.0122\n",
      "Epoch 185/500\n",
      "715/715 [==============================] - 0s 73us/step - loss: -1.2969 - val_loss: -0.0133\n",
      "Epoch 186/500\n",
      "715/715 [==============================] - 0s 57us/step - loss: -1.2977 - val_loss: -0.0144\n",
      "Epoch 187/500\n",
      "715/715 [==============================] - 0s 60us/step - loss: -1.2985 - val_loss: -0.0156\n",
      "Epoch 188/500\n",
      "715/715 [==============================] - 0s 60us/step - loss: -1.2993 - val_loss: -0.0167\n",
      "Epoch 189/500\n",
      "715/715 [==============================] - 0s 70us/step - loss: -1.3001 - val_loss: -0.0179\n",
      "Epoch 190/500\n",
      "715/715 [==============================] - 0s 63us/step - loss: -1.3007 - val_loss: -0.0190\n",
      "Epoch 191/500\n",
      "715/715 [==============================] - 0s 64us/step - loss: -1.3014 - val_loss: -0.0202\n",
      "Epoch 192/500\n",
      "715/715 [==============================] - 0s 62us/step - loss: -1.3021 - val_loss: -0.0214\n",
      "Epoch 193/500\n",
      "715/715 [==============================] - 0s 63us/step - loss: -1.3027 - val_loss: -0.0226\n",
      "Epoch 194/500\n",
      "715/715 [==============================] - 0s 67us/step - loss: -1.3034 - val_loss: -0.0238\n",
      "Epoch 195/500\n",
      "715/715 [==============================] - 0s 60us/step - loss: -1.3041 - val_loss: -0.0250\n",
      "Epoch 196/500\n",
      "715/715 [==============================] - 0s 60us/step - loss: -1.3047 - val_loss: -0.0262\n",
      "Epoch 197/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3054 - val_loss: -0.0274\n",
      "Epoch 198/500\n",
      "715/715 [==============================] - 0s 67us/step - loss: -1.3061 - val_loss: -0.0286\n",
      "Epoch 199/500\n",
      "715/715 [==============================] - 0s 60us/step - loss: -1.3068 - val_loss: -0.0299\n",
      "Epoch 200/500\n",
      "715/715 [==============================] - 0s 61us/step - loss: -1.3075 - val_loss: -0.0313\n",
      "Epoch 201/500\n",
      "715/715 [==============================] - 0s 57us/step - loss: -1.3082 - val_loss: -0.0324\n",
      "Epoch 202/500\n",
      "715/715 [==============================] - 0s 60us/step - loss: -1.3089 - val_loss: -0.0339\n",
      "Epoch 203/500\n",
      "715/715 [==============================] - 0s 74us/step - loss: -1.3097 - val_loss: -0.0349\n",
      "Epoch 204/500\n",
      "715/715 [==============================] - 0s 67us/step - loss: -1.3103 - val_loss: -0.0360\n",
      "Epoch 205/500\n",
      "715/715 [==============================] - 0s 57us/step - loss: -1.3109 - val_loss: -0.0370\n",
      "Epoch 206/500\n",
      "715/715 [==============================] - 0s 59us/step - loss: -1.3116 - val_loss: -0.0381\n",
      "Epoch 207/500\n",
      "715/715 [==============================] - 0s 59us/step - loss: -1.3123 - val_loss: -0.0392\n",
      "Epoch 208/500\n",
      "715/715 [==============================] - 0s 55us/step - loss: -1.3129 - val_loss: -0.0402\n",
      "Epoch 209/500\n",
      "715/715 [==============================] - 0s 66us/step - loss: -1.3135 - val_loss: -0.0413\n",
      "Epoch 210/500\n",
      "715/715 [==============================] - 0s 60us/step - loss: -1.3142 - val_loss: -0.0422\n",
      "Epoch 211/500\n",
      "715/715 [==============================] - 0s 59us/step - loss: -1.3147 - val_loss: -0.0432\n",
      "Epoch 212/500\n",
      "715/715 [==============================] - 0s 59us/step - loss: -1.3152 - val_loss: -0.0443\n",
      "Epoch 213/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -1.3157 - val_loss: -0.0453\n",
      "Epoch 214/500\n",
      "715/715 [==============================] - 0s 61us/step - loss: -1.3162 - val_loss: -0.0463\n",
      "Epoch 215/500\n",
      "715/715 [==============================] - 0s 59us/step - loss: -1.3167 - val_loss: -0.0473\n",
      "Epoch 216/500\n",
      "715/715 [==============================] - 0s 61us/step - loss: -1.3172 - val_loss: -0.0483\n",
      "Epoch 217/500\n",
      "715/715 [==============================] - 0s 68us/step - loss: -1.3177 - val_loss: -0.0494\n",
      "Epoch 218/500\n",
      "715/715 [==============================] - 0s 81us/step - loss: -1.3183 - val_loss: -0.0504\n",
      "Epoch 219/500\n",
      "715/715 [==============================] - 0s 61us/step - loss: -1.3188 - val_loss: -0.0514\n",
      "Epoch 220/500\n",
      "715/715 [==============================] - 0s 61us/step - loss: -1.3193 - val_loss: -0.0525\n",
      "Epoch 221/500\n",
      "715/715 [==============================] - 0s 59us/step - loss: -1.3199 - val_loss: -0.0534\n",
      "Epoch 222/500\n",
      "715/715 [==============================] - 0s 59us/step - loss: -1.3205 - val_loss: -0.0546\n",
      "Epoch 223/500\n",
      "715/715 [==============================] - 0s 59us/step - loss: -1.3210 - val_loss: -0.0555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 224/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -1.3215 - val_loss: -0.0566\n",
      "Epoch 225/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -1.3220 - val_loss: -0.0575\n",
      "Epoch 226/500\n",
      "715/715 [==============================] - 0s 59us/step - loss: -1.3226 - val_loss: -0.0586\n",
      "Epoch 227/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -1.3232 - val_loss: -0.0596\n",
      "Epoch 228/500\n",
      "715/715 [==============================] - 0s 60us/step - loss: -1.3237 - val_loss: -0.0607\n",
      "Epoch 229/500\n",
      "715/715 [==============================] - 0s 61us/step - loss: -1.3242 - val_loss: -0.0616\n",
      "Epoch 230/500\n",
      "715/715 [==============================] - 0s 71us/step - loss: -1.3247 - val_loss: -0.0624\n",
      "Epoch 231/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3252 - val_loss: -0.0634\n",
      "Epoch 232/500\n",
      "715/715 [==============================] - 0s 68us/step - loss: -1.3256 - val_loss: -0.0643\n",
      "Epoch 233/500\n",
      "715/715 [==============================] - 0s 66us/step - loss: -1.3261 - val_loss: -0.0651\n",
      "Epoch 234/500\n",
      "715/715 [==============================] - 0s 68us/step - loss: -1.3264 - val_loss: -0.0658\n",
      "Epoch 235/500\n",
      "715/715 [==============================] - 0s 67us/step - loss: -1.3268 - val_loss: -0.0668\n",
      "Epoch 236/500\n",
      "715/715 [==============================] - 0s 73us/step - loss: -1.3272 - val_loss: -0.0676\n",
      "Epoch 237/500\n",
      "715/715 [==============================] - 0s 71us/step - loss: -1.3276 - val_loss: -0.0684\n",
      "Epoch 238/500\n",
      "715/715 [==============================] - 0s 67us/step - loss: -1.3279 - val_loss: -0.0691\n",
      "Epoch 239/500\n",
      "715/715 [==============================] - 0s 63us/step - loss: -1.3283 - val_loss: -0.0699\n",
      "Epoch 240/500\n",
      "715/715 [==============================] - 0s 59us/step - loss: -1.3287 - val_loss: -0.0707\n",
      "Epoch 241/500\n",
      "715/715 [==============================] - 0s 60us/step - loss: -1.3291 - val_loss: -0.0713\n",
      "Epoch 242/500\n",
      "715/715 [==============================] - 0s 57us/step - loss: -1.3295 - val_loss: -0.0722\n",
      "Epoch 243/500\n",
      "715/715 [==============================] - 0s 61us/step - loss: -1.3299 - val_loss: -0.0729\n",
      "Epoch 244/500\n",
      "715/715 [==============================] - 0s 70us/step - loss: -1.3303 - val_loss: -0.0736\n",
      "Epoch 245/500\n",
      "715/715 [==============================] - 0s 71us/step - loss: -1.3306 - val_loss: -0.0743\n",
      "Epoch 246/500\n",
      "715/715 [==============================] - 0s 68us/step - loss: -1.3310 - val_loss: -0.0751\n",
      "Epoch 247/500\n",
      "715/715 [==============================] - 0s 74us/step - loss: -1.3314 - val_loss: -0.0757\n",
      "Epoch 248/500\n",
      "715/715 [==============================] - 0s 66us/step - loss: -1.3318 - val_loss: -0.0765\n",
      "Epoch 249/500\n",
      "715/715 [==============================] - 0s 67us/step - loss: -1.3322 - val_loss: -0.0771\n",
      "Epoch 250/500\n",
      "715/715 [==============================] - 0s 66us/step - loss: -1.3326 - val_loss: -0.0778\n",
      "Epoch 251/500\n",
      "715/715 [==============================] - 0s 70us/step - loss: -1.3330 - val_loss: -0.0785\n",
      "Epoch 252/500\n",
      "715/715 [==============================] - 0s 70us/step - loss: -1.3334 - val_loss: -0.0792\n",
      "Epoch 253/500\n",
      "715/715 [==============================] - 0s 67us/step - loss: -1.3337 - val_loss: -0.0798\n",
      "Epoch 254/500\n",
      "715/715 [==============================] - 0s 64us/step - loss: -1.3341 - val_loss: -0.0804\n",
      "Epoch 255/500\n",
      "715/715 [==============================] - 0s 64us/step - loss: -1.3344 - val_loss: -0.0811\n",
      "Epoch 256/500\n",
      "715/715 [==============================] - 0s 61us/step - loss: -1.3347 - val_loss: -0.0816\n",
      "Epoch 257/500\n",
      "715/715 [==============================] - 0s 78us/step - loss: -1.3349 - val_loss: -0.0822\n",
      "Epoch 258/500\n",
      "715/715 [==============================] - 0s 74us/step - loss: -1.3352 - val_loss: -0.0828\n",
      "Epoch 259/500\n",
      "715/715 [==============================] - 0s 67us/step - loss: -1.3354 - val_loss: -0.0833\n",
      "Epoch 260/500\n",
      "715/715 [==============================] - 0s 77us/step - loss: -1.3357 - val_loss: -0.0839\n",
      "Epoch 261/500\n",
      "715/715 [==============================] - 0s 74us/step - loss: -1.3359 - val_loss: -0.0844\n",
      "Epoch 262/500\n",
      "715/715 [==============================] - 0s 74us/step - loss: -1.3361 - val_loss: -0.0849\n",
      "Epoch 263/500\n",
      "715/715 [==============================] - 0s 67us/step - loss: -1.3363 - val_loss: -0.0854\n",
      "Epoch 264/500\n",
      "715/715 [==============================] - 0s 60us/step - loss: -1.3366 - val_loss: -0.0859\n",
      "Epoch 265/500\n",
      "715/715 [==============================] - 0s 79us/step - loss: -1.3368 - val_loss: -0.0864\n",
      "Epoch 266/500\n",
      "715/715 [==============================] - 0s 64us/step - loss: -1.3371 - val_loss: -0.0868\n",
      "Epoch 267/500\n",
      "715/715 [==============================] - 0s 74us/step - loss: -1.3373 - val_loss: -0.0873\n",
      "Epoch 268/500\n",
      "715/715 [==============================] - 0s 64us/step - loss: -1.3376 - val_loss: -0.0877\n",
      "Epoch 269/500\n",
      "715/715 [==============================] - 0s 78us/step - loss: -1.3379 - val_loss: -0.0883\n",
      "Epoch 270/500\n",
      "715/715 [==============================] - 0s 66us/step - loss: -1.3381 - val_loss: -0.0888\n",
      "Epoch 271/500\n",
      "715/715 [==============================] - 0s 82us/step - loss: -1.3384 - val_loss: -0.0892\n",
      "Epoch 272/500\n",
      "715/715 [==============================] - 0s 80us/step - loss: -1.3386 - val_loss: -0.0897\n",
      "Epoch 273/500\n",
      "715/715 [==============================] - 0s 68us/step - loss: -1.3389 - val_loss: -0.0901\n",
      "Epoch 274/500\n",
      "715/715 [==============================] - 0s 81us/step - loss: -1.3391 - val_loss: -0.0906\n",
      "Epoch 275/500\n",
      "715/715 [==============================] - 0s 66us/step - loss: -1.3394 - val_loss: -0.0910\n",
      "Epoch 276/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -1.3396 - val_loss: -0.0916\n",
      "Epoch 277/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -1.3398 - val_loss: -0.0919\n",
      "Epoch 278/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -1.3400 - val_loss: -0.0924\n",
      "Epoch 279/500\n",
      "715/715 [==============================] - 0s 57us/step - loss: -1.3402 - val_loss: -0.0929\n",
      "Epoch 280/500\n",
      "715/715 [==============================] - 0s 60us/step - loss: -1.3404 - val_loss: -0.0932\n",
      "Epoch 281/500\n",
      "715/715 [==============================] - 0s 75us/step - loss: -1.3406 - val_loss: -0.0938\n",
      "Epoch 282/500\n",
      "715/715 [==============================] - 0s 70us/step - loss: -1.3408 - val_loss: -0.0940\n",
      "Epoch 283/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3409 - val_loss: -0.0948\n",
      "Epoch 284/500\n",
      "715/715 [==============================] - 0s 59us/step - loss: -1.3411 - val_loss: -0.0951\n",
      "Epoch 285/500\n",
      "715/715 [==============================] - 0s 58us/step - loss: -1.3413 - val_loss: -0.0953\n",
      "Epoch 286/500\n",
      "715/715 [==============================] - 0s 73us/step - loss: -1.3414 - val_loss: -0.0955\n",
      "Epoch 287/500\n",
      "715/715 [==============================] - 0s 59us/step - loss: -1.3416 - val_loss: -0.0958\n",
      "Epoch 288/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -1.3417 - val_loss: -0.0960\n",
      "Epoch 289/500\n",
      "715/715 [==============================] - 0s 59us/step - loss: -1.3419 - val_loss: -0.0963\n",
      "Epoch 290/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3421 - val_loss: -0.0963\n",
      "Epoch 291/500\n",
      "715/715 [==============================] - 0s 69us/step - loss: -1.3422 - val_loss: -0.0963\n",
      "Epoch 292/500\n",
      "715/715 [==============================] - 0s 69us/step - loss: -1.3423 - val_loss: -0.0966\n",
      "Epoch 293/500\n",
      "715/715 [==============================] - 0s 59us/step - loss: -1.3425 - val_loss: -0.0968\n",
      "Epoch 294/500\n",
      "715/715 [==============================] - 0s 57us/step - loss: -1.3426 - val_loss: -0.0969\n",
      "Epoch 295/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3427 - val_loss: -0.0970\n",
      "Epoch 296/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3429 - val_loss: -0.0972\n",
      "Epoch 297/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3430 - val_loss: -0.0973\n",
      "Epoch 298/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3431 - val_loss: -0.0974\n",
      "Epoch 299/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3432 - val_loss: -0.0976\n",
      "Epoch 300/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3433 - val_loss: -0.0977\n",
      "Epoch 301/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3435 - val_loss: -0.0978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 302/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -1.3436 - val_loss: -0.0978\n",
      "Epoch 303/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3437 - val_loss: -0.0979\n",
      "Epoch 304/500\n",
      "715/715 [==============================] - 0s 55us/step - loss: -1.3438 - val_loss: -0.0979\n",
      "Epoch 305/500\n",
      "715/715 [==============================] - 0s 59us/step - loss: -1.3439 - val_loss: -0.0981\n",
      "Epoch 306/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3440 - val_loss: -0.0983\n",
      "Epoch 307/500\n",
      "715/715 [==============================] - 0s 55us/step - loss: -1.3440 - val_loss: -0.0983\n",
      "Epoch 308/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -1.3442 - val_loss: -0.0984\n",
      "Epoch 309/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3443 - val_loss: -0.0985\n",
      "Epoch 310/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3443 - val_loss: -0.0985\n",
      "Epoch 311/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3444 - val_loss: -0.0986\n",
      "Epoch 312/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3445 - val_loss: -0.0987\n",
      "Epoch 313/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -1.3446 - val_loss: -0.0988\n",
      "Epoch 314/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3447 - val_loss: -0.0988\n",
      "Epoch 315/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3448 - val_loss: -0.0989\n",
      "Epoch 316/500\n",
      "715/715 [==============================] - 0s 55us/step - loss: -1.3449 - val_loss: -0.0989\n",
      "Epoch 317/500\n",
      "715/715 [==============================] - 0s 55us/step - loss: -1.3450 - val_loss: -0.0990\n",
      "Epoch 318/500\n",
      "715/715 [==============================] - 0s 55us/step - loss: -1.3450 - val_loss: -0.0990\n",
      "Epoch 319/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3451 - val_loss: -0.0991\n",
      "Epoch 320/500\n",
      "715/715 [==============================] - 0s 55us/step - loss: -1.3452 - val_loss: -0.0992\n",
      "Epoch 321/500\n",
      "715/715 [==============================] - 0s 57us/step - loss: -1.3453 - val_loss: -0.0992\n",
      "Epoch 322/500\n",
      "715/715 [==============================] - 0s 67us/step - loss: -1.3453 - val_loss: -0.0993\n",
      "Epoch 323/500\n",
      "715/715 [==============================] - 0s 67us/step - loss: -1.3454 - val_loss: -0.0993\n",
      "Epoch 324/500\n",
      "715/715 [==============================] - 0s 59us/step - loss: -1.3455 - val_loss: -0.0994\n",
      "Epoch 325/500\n",
      "715/715 [==============================] - 0s 59us/step - loss: -1.3456 - val_loss: -0.0994\n",
      "Epoch 326/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3456 - val_loss: -0.0995\n",
      "Epoch 327/500\n",
      "715/715 [==============================] - 0s 59us/step - loss: -1.3457 - val_loss: -0.0994\n",
      "Epoch 328/500\n",
      "715/715 [==============================] - 0s 66us/step - loss: -1.3458 - val_loss: -0.0995\n",
      "Epoch 329/500\n",
      "715/715 [==============================] - 0s 57us/step - loss: -1.3458 - val_loss: -0.0995\n",
      "Epoch 330/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -1.3459 - val_loss: -0.0995\n",
      "Epoch 331/500\n",
      "715/715 [==============================] - 0s 59us/step - loss: -1.3460 - val_loss: -0.0996\n",
      "Epoch 332/500\n",
      "715/715 [==============================] - 0s 57us/step - loss: -1.3460 - val_loss: -0.0996\n",
      "Epoch 333/500\n",
      "715/715 [==============================] - 0s 60us/step - loss: -1.3461 - val_loss: -0.0997\n",
      "Epoch 334/500\n",
      "715/715 [==============================] - 0s 58us/step - loss: -1.3462 - val_loss: -0.0997\n",
      "Epoch 335/500\n",
      "715/715 [==============================] - 0s 62us/step - loss: -1.3462 - val_loss: -0.0997\n",
      "Epoch 336/500\n",
      "715/715 [==============================] - 0s 73us/step - loss: -1.3463 - val_loss: -0.0998\n",
      "Epoch 337/500\n",
      "715/715 [==============================] - 0s 68us/step - loss: -1.3464 - val_loss: -0.0999\n",
      "Epoch 338/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3464 - val_loss: -0.0999\n",
      "Epoch 339/500\n",
      "715/715 [==============================] - 0s 60us/step - loss: -1.3465 - val_loss: -0.1000\n",
      "Epoch 340/500\n",
      "715/715 [==============================] - 0s 70us/step - loss: -1.3465 - val_loss: -0.0998\n",
      "Epoch 341/500\n",
      "715/715 [==============================] - 0s 59us/step - loss: -1.3466 - val_loss: -0.1000\n",
      "Epoch 342/500\n",
      "715/715 [==============================] - 0s 70us/step - loss: -1.3466 - val_loss: -0.1000\n",
      "Epoch 343/500\n",
      "715/715 [==============================] - 0s 60us/step - loss: -1.3467 - val_loss: -0.1001\n",
      "Epoch 344/500\n",
      "715/715 [==============================] - 0s 73us/step - loss: -1.3467 - val_loss: -0.1001\n",
      "Epoch 345/500\n",
      "715/715 [==============================] - 0s 57us/step - loss: -1.3468 - val_loss: -0.1001\n",
      "Epoch 346/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3468 - val_loss: -0.1002\n",
      "Epoch 347/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -1.3469 - val_loss: -0.1001\n",
      "Epoch 348/500\n",
      "715/715 [==============================] - 0s 83us/step - loss: -1.3470 - val_loss: -0.1002\n",
      "Epoch 349/500\n",
      "715/715 [==============================] - 0s 59us/step - loss: -1.3470 - val_loss: -0.1001\n",
      "Epoch 350/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3471 - val_loss: -0.1003\n",
      "Epoch 351/500\n",
      "715/715 [==============================] - 0s 59us/step - loss: -1.3471 - val_loss: -0.1002\n",
      "Epoch 352/500\n",
      "715/715 [==============================] - 0s 63us/step - loss: -1.3472 - val_loss: -0.1003\n",
      "Epoch 353/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3472 - val_loss: -0.1003\n",
      "Epoch 354/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -1.3472 - val_loss: -0.1003\n",
      "Epoch 355/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3473 - val_loss: -0.1004\n",
      "Epoch 356/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -1.3473 - val_loss: -0.1005\n",
      "Epoch 357/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3474 - val_loss: -0.1005\n",
      "Epoch 358/500\n",
      "715/715 [==============================] - 0s 60us/step - loss: -1.3474 - val_loss: -0.1005\n",
      "Epoch 359/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -1.3475 - val_loss: -0.1005\n",
      "Epoch 360/500\n",
      "715/715 [==============================] - 0s 63us/step - loss: -1.3475 - val_loss: -0.1006\n",
      "Epoch 361/500\n",
      "715/715 [==============================] - 0s 68us/step - loss: -1.3476 - val_loss: -0.1006\n",
      "Epoch 362/500\n",
      "715/715 [==============================] - 0s 70us/step - loss: -1.3476 - val_loss: -0.1006\n",
      "Epoch 363/500\n",
      "715/715 [==============================] - 0s 60us/step - loss: -1.3476 - val_loss: -0.1006\n",
      "Epoch 364/500\n",
      "715/715 [==============================] - 0s 57us/step - loss: -1.3477 - val_loss: -0.1007\n",
      "Epoch 365/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -1.3477 - val_loss: -0.1007\n",
      "Epoch 366/500\n",
      "715/715 [==============================] - 0s 55us/step - loss: -1.3478 - val_loss: -0.1007\n",
      "Epoch 367/500\n",
      "715/715 [==============================] - 0s 60us/step - loss: -1.3478 - val_loss: -0.1008\n",
      "Epoch 368/500\n",
      "715/715 [==============================] - 0s 59us/step - loss: -1.3478 - val_loss: -0.1008\n",
      "Epoch 369/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -1.3479 - val_loss: -0.1008\n",
      "Epoch 370/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3479 - val_loss: -0.1008\n",
      "Epoch 371/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -1.3479 - val_loss: -0.1009\n",
      "Epoch 372/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3480 - val_loss: -0.1008\n",
      "Epoch 373/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3480 - val_loss: -0.1009\n",
      "Epoch 374/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3480 - val_loss: -0.1008\n",
      "Epoch 375/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -1.3481 - val_loss: -0.1009\n",
      "Epoch 376/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3481 - val_loss: -0.1009\n",
      "Epoch 377/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3481 - val_loss: -0.1010\n",
      "Epoch 378/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3482 - val_loss: -0.1010\n",
      "Epoch 379/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3482 - val_loss: -0.1010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 380/500\n",
      "715/715 [==============================] - 0s 57us/step - loss: -1.3482 - val_loss: -0.1010\n",
      "Epoch 381/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3482 - val_loss: -0.1011\n",
      "Epoch 382/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3483 - val_loss: -0.1011\n",
      "Epoch 383/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3483 - val_loss: -0.1011\n",
      "Epoch 384/500\n",
      "715/715 [==============================] - 0s 49us/step - loss: -1.3483 - val_loss: -0.1011\n",
      "Epoch 385/500\n",
      "715/715 [==============================] - 0s 80us/step - loss: -1.3484 - val_loss: -0.1011\n",
      "Epoch 386/500\n",
      "715/715 [==============================] - 0s 64us/step - loss: -1.3484 - val_loss: -0.1012\n",
      "Epoch 387/500\n",
      "715/715 [==============================] - 0s 114us/step - loss: -1.3484 - val_loss: -0.1012\n",
      "Epoch 388/500\n",
      "715/715 [==============================] - 0s 114us/step - loss: -1.3485 - val_loss: -0.1012\n",
      "Epoch 389/500\n",
      "715/715 [==============================] - 0s 77us/step - loss: -1.3485 - val_loss: -0.1012\n",
      "Epoch 390/500\n",
      "715/715 [==============================] - 0s 70us/step - loss: -1.3485 - val_loss: -0.1012\n",
      "Epoch 391/500\n",
      "715/715 [==============================] - 0s 99us/step - loss: -1.3485 - val_loss: -0.1012\n",
      "Epoch 392/500\n",
      "715/715 [==============================] - 0s 70us/step - loss: -1.3486 - val_loss: -0.1013\n",
      "Epoch 393/500\n",
      "715/715 [==============================] - 0s 70us/step - loss: -1.3486 - val_loss: -0.1013\n",
      "Epoch 394/500\n",
      "715/715 [==============================] - 0s 57us/step - loss: -1.3486 - val_loss: -0.1013\n",
      "Epoch 395/500\n",
      "715/715 [==============================] - 0s 89us/step - loss: -1.3486 - val_loss: -0.1013\n",
      "Epoch 396/500\n",
      "715/715 [==============================] - 0s 64us/step - loss: -1.3487 - val_loss: -0.1013\n",
      "Epoch 397/500\n",
      "715/715 [==============================] - 0s 66us/step - loss: -1.3487 - val_loss: -0.1014\n",
      "Epoch 398/500\n",
      "715/715 [==============================] - 0s 61us/step - loss: -1.3487 - val_loss: -0.1013\n",
      "Epoch 399/500\n",
      "715/715 [==============================] - 0s 70us/step - loss: -1.3487 - val_loss: -0.1014\n",
      "Epoch 400/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -1.3488 - val_loss: -0.1014\n",
      "Epoch 401/500\n",
      "715/715 [==============================] - 0s 60us/step - loss: -1.3488 - val_loss: -0.1013\n",
      "Epoch 402/500\n",
      "715/715 [==============================] - 0s 60us/step - loss: -1.3488 - val_loss: -0.1014\n",
      "Epoch 403/500\n",
      "715/715 [==============================] - 0s 70us/step - loss: -1.3488 - val_loss: -0.1014\n",
      "Epoch 404/500\n",
      "715/715 [==============================] - 0s 57us/step - loss: -1.3488 - val_loss: -0.1014\n",
      "Epoch 405/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -1.3489 - val_loss: -0.1015\n",
      "Epoch 406/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -1.3489 - val_loss: -0.1014\n",
      "Epoch 407/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -1.3489 - val_loss: -0.1015\n",
      "Epoch 408/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -1.3489 - val_loss: -0.1014\n",
      "Epoch 409/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -1.3489 - val_loss: -0.1015\n",
      "Epoch 410/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -1.3490 - val_loss: -0.1015\n",
      "Epoch 411/500\n",
      "715/715 [==============================] - 0s 61us/step - loss: -1.3490 - val_loss: -0.1015\n",
      "Epoch 412/500\n",
      "715/715 [==============================] - 0s 61us/step - loss: -1.3490 - val_loss: -0.1015\n",
      "Epoch 413/500\n",
      "715/715 [==============================] - 0s 57us/step - loss: -1.3490 - val_loss: -0.1015\n",
      "Epoch 414/500\n",
      "715/715 [==============================] - 0s 60us/step - loss: -1.3490 - val_loss: -0.1015\n",
      "Epoch 415/500\n",
      "715/715 [==============================] - 0s 57us/step - loss: -1.3491 - val_loss: -0.1015\n",
      "Epoch 416/500\n",
      "715/715 [==============================] - 0s 76us/step - loss: -1.3491 - val_loss: -0.1015\n",
      "Epoch 417/500\n",
      "715/715 [==============================] - 0s 59us/step - loss: -1.3491 - val_loss: -0.1015\n",
      "Epoch 418/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -1.3491 - val_loss: -0.1015\n",
      "Epoch 419/500\n",
      "715/715 [==============================] - 0s 55us/step - loss: -1.3491 - val_loss: -0.1015\n",
      "Epoch 420/500\n",
      "715/715 [==============================] - 0s 63us/step - loss: -1.3491 - val_loss: -0.1015\n",
      "Epoch 421/500\n",
      "715/715 [==============================] - 0s 67us/step - loss: -1.3492 - val_loss: -0.1015\n",
      "Epoch 422/500\n",
      "715/715 [==============================] - 0s 74us/step - loss: -1.3492 - val_loss: -0.1015\n",
      "Epoch 423/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -1.3492 - val_loss: -0.1015\n",
      "Epoch 424/500\n",
      "715/715 [==============================] - 0s 59us/step - loss: -1.3492 - val_loss: -0.1014\n",
      "Epoch 425/500\n",
      "715/715 [==============================] - 0s 59us/step - loss: -1.3492 - val_loss: -0.1015\n",
      "Epoch 426/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3492 - val_loss: -0.1015\n",
      "Epoch 427/500\n",
      "715/715 [==============================] - 0s 55us/step - loss: -1.3493 - val_loss: -0.1015\n",
      "Epoch 428/500\n",
      "715/715 [==============================] - 0s 55us/step - loss: -1.3493 - val_loss: -0.1014\n",
      "Epoch 429/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -1.3493 - val_loss: -0.1015\n",
      "Epoch 430/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -1.3493 - val_loss: -0.1014\n",
      "Epoch 431/500\n",
      "715/715 [==============================] - 0s 57us/step - loss: -1.3493 - val_loss: -0.1014\n",
      "Epoch 432/500\n",
      "715/715 [==============================] - 0s 57us/step - loss: -1.3493 - val_loss: -0.1014\n",
      "Epoch 433/500\n",
      "715/715 [==============================] - 0s 59us/step - loss: -1.3493 - val_loss: -0.1014\n",
      "Epoch 434/500\n",
      "715/715 [==============================] - 0s 57us/step - loss: -1.3494 - val_loss: -0.1014\n",
      "Epoch 435/500\n",
      "715/715 [==============================] - 0s 59us/step - loss: -1.3494 - val_loss: -0.1014\n",
      "Epoch 436/500\n",
      "715/715 [==============================] - 0s 66us/step - loss: -1.3494 - val_loss: -0.1014\n",
      "Epoch 437/500\n",
      "715/715 [==============================] - 0s 66us/step - loss: -1.3494 - val_loss: -0.1014\n",
      "Epoch 438/500\n",
      "715/715 [==============================] - 0s 66us/step - loss: -1.3494 - val_loss: -0.1014\n",
      "Epoch 439/500\n",
      "715/715 [==============================] - 0s 77us/step - loss: -1.3494 - val_loss: -0.1014\n",
      "Epoch 440/500\n",
      "715/715 [==============================] - 0s 70us/step - loss: -1.3494 - val_loss: -0.1014\n",
      "Epoch 441/500\n",
      "715/715 [==============================] - 0s 63us/step - loss: -1.3495 - val_loss: -0.1014\n",
      "Epoch 442/500\n",
      "715/715 [==============================] - 0s 63us/step - loss: -1.3494 - val_loss: -0.1014\n",
      "Epoch 443/500\n",
      "715/715 [==============================] - 0s 66us/step - loss: -1.3495 - val_loss: -0.1014\n",
      "Epoch 444/500\n",
      "715/715 [==============================] - 0s 57us/step - loss: -1.3495 - val_loss: -0.1014\n",
      "Epoch 445/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -1.3495 - val_loss: -0.1013\n",
      "Epoch 446/500\n",
      "715/715 [==============================] - 0s 60us/step - loss: -1.3495 - val_loss: -0.1014\n",
      "Epoch 447/500\n",
      "715/715 [==============================] - 0s 57us/step - loss: -1.3495 - val_loss: -0.1014\n",
      "Epoch 448/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -1.3495 - val_loss: -0.1014\n",
      "Epoch 449/500\n",
      "715/715 [==============================] - 0s 55us/step - loss: -1.3495 - val_loss: -0.1014\n",
      "Epoch 450/500\n",
      "715/715 [==============================] - 0s 55us/step - loss: -1.3496 - val_loss: -0.1014\n",
      "Epoch 451/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -1.3496 - val_loss: -0.1014\n",
      "Epoch 452/500\n",
      "715/715 [==============================] - 0s 55us/step - loss: -1.3496 - val_loss: -0.1014\n",
      "Epoch 453/500\n",
      "715/715 [==============================] - 0s 92us/step - loss: -1.3496 - val_loss: -0.1014\n",
      "Epoch 454/500\n",
      "715/715 [==============================] - 0s 66us/step - loss: -1.3496 - val_loss: -0.1014\n",
      "Epoch 455/500\n",
      "715/715 [==============================] - 0s 83us/step - loss: -1.3496 - val_loss: -0.1014\n",
      "Epoch 456/500\n",
      "715/715 [==============================] - 0s 67us/step - loss: -1.3496 - val_loss: -0.1014\n",
      "Epoch 457/500\n",
      "715/715 [==============================] - 0s 68us/step - loss: -1.3496 - val_loss: -0.1014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 458/500\n",
      "715/715 [==============================] - 0s 66us/step - loss: -1.3497 - val_loss: -0.1014\n",
      "Epoch 459/500\n",
      "715/715 [==============================] - 0s 69us/step - loss: -1.3497 - val_loss: -0.1014\n",
      "Epoch 460/500\n",
      "715/715 [==============================] - 0s 81us/step - loss: -1.3497 - val_loss: -0.1014\n",
      "Epoch 461/500\n",
      "715/715 [==============================] - 0s 80us/step - loss: -1.3497 - val_loss: -0.1014\n",
      "Epoch 462/500\n",
      "715/715 [==============================] - 0s 88us/step - loss: -1.3497 - val_loss: -0.1014\n",
      "Epoch 463/500\n",
      "715/715 [==============================] - 0s 70us/step - loss: -1.3497 - val_loss: -0.1014\n",
      "Epoch 464/500\n",
      "715/715 [==============================] - 0s 73us/step - loss: -1.3497 - val_loss: -0.1013\n",
      "Epoch 465/500\n",
      "715/715 [==============================] - 0s 77us/step - loss: -1.3497 - val_loss: -0.1014\n",
      "Epoch 466/500\n",
      "715/715 [==============================] - 0s 120us/step - loss: -1.3497 - val_loss: -0.1014\n",
      "Epoch 467/500\n",
      "715/715 [==============================] - 0s 101us/step - loss: -1.3498 - val_loss: -0.1014\n",
      "Epoch 468/500\n",
      "715/715 [==============================] - 0s 102us/step - loss: -1.3498 - val_loss: -0.1014\n",
      "Epoch 469/500\n",
      "715/715 [==============================] - 0s 63us/step - loss: -1.3498 - val_loss: -0.1014\n",
      "Epoch 470/500\n",
      "715/715 [==============================] - 0s 89us/step - loss: -1.3498 - val_loss: -0.1013\n",
      "Epoch 471/500\n",
      "715/715 [==============================] - 0s 126us/step - loss: -1.3498 - val_loss: -0.1013\n",
      "Epoch 472/500\n",
      "715/715 [==============================] - 0s 74us/step - loss: -1.3498 - val_loss: -0.1013\n",
      "Epoch 473/500\n",
      "715/715 [==============================] - 0s 134us/step - loss: -1.3498 - val_loss: -0.1013\n",
      "Epoch 474/500\n",
      "715/715 [==============================] - 0s 59us/step - loss: -1.3499 - val_loss: -0.1013\n",
      "Epoch 475/500\n",
      "715/715 [==============================] - 0s 138us/step - loss: -1.3499 - val_loss: -0.1012\n",
      "Epoch 476/500\n",
      "715/715 [==============================] - 0s 63us/step - loss: -1.3499 - val_loss: -0.1013\n",
      "Epoch 477/500\n",
      "715/715 [==============================] - 0s 78us/step - loss: -1.3499 - val_loss: -0.1013\n",
      "Epoch 478/500\n",
      "715/715 [==============================] - 0s 55us/step - loss: -1.3499 - val_loss: -0.1013\n",
      "Epoch 479/500\n",
      "715/715 [==============================] - 0s 61us/step - loss: -1.3499 - val_loss: -0.1013\n",
      "Epoch 480/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -1.3499 - val_loss: -0.1013\n",
      "Epoch 481/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -1.3499 - val_loss: -0.1013\n",
      "Epoch 482/500\n",
      "715/715 [==============================] - 0s 57us/step - loss: -1.3499 - val_loss: -0.1013\n",
      "Epoch 483/500\n",
      "715/715 [==============================] - 0s 62us/step - loss: -1.3499 - val_loss: -0.1013\n",
      "Epoch 484/500\n",
      "715/715 [==============================] - 0s 69us/step - loss: -1.3499 - val_loss: -0.1013\n",
      "Epoch 485/500\n",
      "715/715 [==============================] - 0s 71us/step - loss: -1.3500 - val_loss: -0.1013\n",
      "Epoch 486/500\n",
      "715/715 [==============================] - 0s 58us/step - loss: -1.3500 - val_loss: -0.1013\n",
      "Epoch 487/500\n",
      "715/715 [==============================] - 0s 59us/step - loss: -1.3500 - val_loss: -0.1013\n",
      "Epoch 488/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3500 - val_loss: -0.1013\n",
      "Epoch 489/500\n",
      "715/715 [==============================] - 0s 55us/step - loss: -1.3500 - val_loss: -0.1013\n",
      "Epoch 490/500\n",
      "715/715 [==============================] - 0s 50us/step - loss: -1.3500 - val_loss: -0.1013\n",
      "Epoch 491/500\n",
      "715/715 [==============================] - 0s 52us/step - loss: -1.3500 - val_loss: -0.1013\n",
      "Epoch 492/500\n",
      "715/715 [==============================] - 0s 55us/step - loss: -1.3500 - val_loss: -0.1012\n",
      "Epoch 493/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -1.3500 - val_loss: -0.1013\n",
      "Epoch 494/500\n",
      "715/715 [==============================] - 0s 55us/step - loss: -1.3500 - val_loss: -0.1013\n",
      "Epoch 495/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3501 - val_loss: -0.1013\n",
      "Epoch 496/500\n",
      "715/715 [==============================] - 0s 53us/step - loss: -1.3501 - val_loss: -0.1013\n",
      "Epoch 497/500\n",
      "715/715 [==============================] - 0s 56us/step - loss: -1.3500 - val_loss: -0.1013\n",
      "Epoch 498/500\n",
      "715/715 [==============================] - 0s 55us/step - loss: -1.3501 - val_loss: -0.1013\n",
      "Epoch 499/500\n",
      "715/715 [==============================] - 0s 57us/step - loss: -1.3501 - val_loss: -0.1013\n",
      "Epoch 500/500\n",
      "715/715 [==============================] - 0s 54us/step - loss: -1.3501 - val_loss: -0.1013\n"
     ]
    }
   ],
   "source": [
    "# import keras\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "window_length = 410\n",
    "encoding_dim = 200\n",
    "epochs = 500\n",
    "\n",
    "\n",
    "# this is our input placeholder\n",
    "input_window = Input(shape=(window_length,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_window)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(window_length, activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_window, decoded)\n",
    "\n",
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_window, encoded)\n",
    "\n",
    "\n",
    "autoencoder.summary()\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "history = autoencoder.fit(X_normal_scaled, X_normal_scaled,\n",
    "                epochs=epochs,\n",
    "                batch_size=1024,\n",
    "                shuffle=True,\n",
    "                validation_split = 0.2)\n",
    "\n",
    "# Separating the points encoded by the Auto-encoder as normal and fraud \n",
    "decoded_X_normal = autoencoder.predict(X_normal_scaled)\n",
    "decoded_X_deseased = autoencoder.predict(X_deseased_scaled)\n",
    "# Combining the encoded points into a single table  \n",
    "encoded_X = np.append(decoded_X_normal, decoded_X_deseased, axis = 0) \n",
    "y_normal = np.zeros(decoded_X_normal.shape[0]) \n",
    "y_deceased = np.ones(decoded_X_deseased.shape[0]) \n",
    "encoded_y = np.append(y_normal, y_deceased) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import keras\n",
    "from keras.layers import Input, Dense, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras import regularizers \n",
    "# Building the Input Layer \n",
    "window_length = 410\n",
    "input_layer =  Input(shape=(window_length,))\n",
    "\n",
    "# Building the Encoder network \n",
    "decoded = Dense(100, activation ='tanh')(input_layer) \n",
    "\n",
    "# Building the Decoder network \n",
    "\n",
    "# Building the Output Layer \n",
    "output_layer = Dense(X.shape[1], activation ='tanh')(decoded) \n",
    "#Step 8: Defining and Training the Auto-encoder\n",
    "# Defining the parameters of the Auto-encoder network \n",
    "autoencoder = Model(input_layer, output_layer) \n",
    "autoencoder.compile(optimizer =\"adam\", loss =\"mse\") \n",
    "# Training the Auto-encoder network \n",
    "autoencoder.fit(X_normal_scaled, X_normal_scaled,  \n",
    "                batch_size = 16, epochs = 300)\n",
    "\n",
    "hidden_representation = Sequential() \n",
    "hidden_representation.add(autoencoder.layers[0]) \n",
    "hidden_representation.add(autoencoder.layers[1]) \n",
    "\n",
    "\n",
    "# Separating the points encoded by the Auto-encoder as alive and deseased \n",
    "decoded_X_normal = hidden_representation.predict(X_normal_scaled)\n",
    "decoded_X_deseased = hidden_representation.predict(X_deseased_scaled)\n",
    "# Combining the encoded points into a single table  \n",
    "encoded_X = np.append(decoded_X_normal, decoded_X_deseased, axis = 0) \n",
    "y_normal = np.zeros(decoded_X_normal.shape[0]) \n",
    "y_deceased = np.ones(decoded_X_deseased.shape[0]) \n",
    "encoded_y = np.append(y_normal, y_deceased) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting features and the label: 20% test data and 80% assigned to training data\n",
    "# split into train/test sets with same class ratio\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(encoded_X, encoded_y, test_size=0.2, random_state=5, stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# balancing the data\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "# define oversampling strategy\n",
    "over = RandomOverSampler(sampling_strategy='minority')\n",
    "# fit and apply the transform\n",
    "X_train, y_train = over.fit_resample(X_train, y_train)\n",
    "# define undersampling strategy\n",
    "under = RandomUnderSampler(sampling_strategy='majority')\n",
    "# fit and apply the transform\n",
    "X_train, y_train = under.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.bagging module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "# transform the dataset\n",
    "oversample = SMOTE()\n",
    "X_train, y_train = oversample.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data\n",
    "\n",
    "#Scaling the data to standarize them\n",
    "#Here caling reduced the iteration number from 200 to 50\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train = sc.transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 _train= 0.8716690042075736\n",
      "R^2 _test= 0.7219251336898396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "#fitting the model and get the conversion probabilities. \n",
    "#predit_proba() function of our model assigns probability for each row:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(solver='lbfgs', penalty='l2',max_iter=300)\n",
    "model.fit(X_train,y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "lr_probs = model.predict_proba(X_test)[:,1]\n",
    "#Return the mean accuracy on the given test data and taraining data to see if we have overfitting.score clculates R^2\n",
    "print('R^2 _train=',model.score(X_train, y_train))\n",
    "print('R^2 _test=',model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy Scores [0.86713287 0.85314685 0.88111888 0.87412587 0.87412587 0.84615385\n",
      " 0.85211268 0.85915493 0.82394366 0.85915493]\n",
      "CV-scores_min =  0.823943661971831\n",
      "CV_scores_mean = 0.8590170392987295\n",
      "CV_scores_max = 0.8811188811188811\n"
     ]
    }
   ],
   "source": [
    "#Cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(model, X_train, y_train, cv=10)\n",
    "print('Cross-Validation Accuracy Scores', scores)\n",
    "scores = pd.Series(scores)\n",
    "print('CV-scores_min = ',scores.min())\n",
    "print('CV_scores_mean =', scores.mean())\n",
    "print('CV_scores_max =', scores.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Actual  Predicted\n",
       "0      0.0        1.0\n",
       "1      0.0        1.0\n",
       "2      0.0        1.0\n",
       "3      0.0        1.0\n",
       "4      0.0        0.0\n",
       "5      0.0        0.0\n",
       "6      0.0        1.0\n",
       "7      0.0        0.0\n",
       "8      1.0        1.0\n",
       "9      0.0        0.0\n",
       "10     0.0        0.0\n",
       "11     0.0        1.0\n",
       "12     0.0        0.0\n",
       "13     0.0        0.0\n",
       "14     0.0        0.0\n",
       "15     0.0        0.0\n",
       "16     0.0        0.0\n",
       "17     0.0        0.0\n",
       "18     0.0        1.0\n",
       "19     0.0        0.0\n",
       "20     0.0        0.0\n",
       "21     0.0        0.0\n",
       "22     0.0        0.0\n",
       "23     0.0        0.0\n",
       "24     0.0        0.0\n",
       "25     0.0        1.0\n",
       "26     0.0        0.0\n",
       "27     0.0        0.0\n",
       "28     0.0        0.0\n",
       "29     0.0        0.0\n",
       "30     0.0        0.0\n",
       "31     0.0        1.0\n",
       "32     0.0        1.0\n",
       "33     0.0        0.0\n",
       "34     0.0        0.0\n",
       "35     0.0        0.0\n",
       "36     0.0        0.0\n",
       "37     0.0        1.0\n",
       "38     0.0        0.0\n",
       "39     0.0        0.0\n",
       "40     0.0        0.0\n",
       "41     0.0        0.0\n",
       "42     0.0        1.0\n",
       "43     0.0        0.0\n",
       "44     0.0        0.0\n",
       "45     0.0        1.0\n",
       "46     0.0        0.0\n",
       "47     0.0        0.0\n",
       "48     0.0        1.0\n",
       "49     0.0        0.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'Actual': y_test, 'Predicted': y_hat})\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance measurement metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.27807486631016043\n",
      "Mean Squared Error: 0.27807486631016043\n",
      "Root Mean Squared Error: 0.5273280443046439\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_hat))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_hat))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc = 0.932780847145488\n"
     ]
    }
   ],
   "source": [
    "#Area Under ROC Curve (AUROC) metric\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# Keep only the positive class\n",
    "#lr_probs = [p[1] for p in lr_probs]\n",
    "lr_probs\n",
    "print( 'roc_auc =', roc_auc_score(y_test, lr_probs) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Skill: ROC AUC=0.500\n",
      "Logistic: ROC AUC=0.933\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "ns_probs = [0 for _ in range(len(y_test))]\n",
    "# predict probabilities\n",
    "lr_probs = model.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 1]\n",
    "# calculate scores\n",
    "ns_auc = roc_auc_score(y_test, ns_probs)\n",
    "lr_auc = roc_auc_score(y_test, lr_probs)\n",
    "# summarize scores\n",
    "print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "print('Logistic: ROC AUC=%.3f' % (lr_auc))\n",
    "\n",
    "\n",
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
    "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill',  linewidth=2,  color = '#333F4B')\n",
    "pyplot.plot(lr_fpr, lr_tpr, marker='.', label='Logistic regression', linewidth=2, color = 'darkred')\n",
    "# axis labels\n",
    "pyplot.xlabel('False Positive Rate',fontsize=15)\n",
    "pyplot.ylabel('True Positive Rate',fontsize=15)\n",
    "pyplot.title('Features: Viral Sequences',fontsize=15)\n",
    "# show the legend\n",
    "pyplot.legend(fontsize=15)\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7219251336898396"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "recall_score(y_test, y_hat, average='weighted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

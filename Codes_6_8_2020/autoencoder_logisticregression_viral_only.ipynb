{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing the host information including;age, location,gender and status for the possible prediction of outcome of recovery vs Death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data_SARS-CoV-2_TestMetadata_with_viral.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "record_date\n",
      "virus_name\n",
      "accession_id\n",
      "type\n",
      "lineage\n",
      "passage_details_history\n",
      "collection_date\n",
      "location\n",
      "host\n",
      "additional_location_info\n",
      "gender\n",
      "age\n",
      "status\n",
      "specimen_source\n",
      "additional_host_information\n",
      "outbreak\n",
      "last_vaccinated\n",
      "treatment\n",
      "sequencing_technology\n",
      "assembly_method\n",
      "coverage\n",
      "comment\n",
      "originating_lab\n",
      "originating_lab_address\n",
      "sample_id_given_by_sample_provider\n",
      "submitting_lab\n",
      "submitting_lab_address\n",
      "sample_id_given_by_submitting_lab\n",
      "authors\n",
      "submitter\n",
      "submission_date\n",
      "submitter_address\n",
      "Query\n",
      "Strand\n",
      "%N\n",
      "Length(nt)\n",
      "Length(aa)\n",
      "#Muts\n",
      "%Muts\n",
      "#UniqueMuts\n",
      "%UniqueMuts\n",
      "#ExistingMuts\n",
      "%ExistingMuts\n",
      "Comment\n",
      "Symbol\n",
      "Reference\n",
      "UniqueMutList\n",
      "ExistingMutList\n",
      "Clade\n",
      "IfExistSpecialChar\n"
     ]
    }
   ],
   "source": [
    "# iterating the columns \n",
    "for col in data.columns: \n",
    "    print(col) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#important features\n",
    "start_data_row = 76\n",
    "Final_data_row = 255\n",
    "Data = data.loc[ start_data_row:Final_data_row , ['status','%N','Length(nt)','Length(aa)',\n",
    "                                                   '%Muts','%UniqueMuts','%ExistingMuts','ExistingMutList','Clade']]\n",
    "#change the index of the data according to the length of the new data\n",
    "Data.index = range(len(Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.04\n",
       "1      0.05\n",
       "2      0.13\n",
       "3      0.03\n",
       "4      0.03\n",
       "       ... \n",
       "173    0.04\n",
       "174    0.05\n",
       "175    0.05\n",
       "176    0.05\n",
       "177    0.01\n",
       "Name: %ExistingMuts, Length: 178, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing the % from the data\n",
    "Data['%N'] = Data['%N'].str.replace('%', ' ') \n",
    "Data['%Muts'] = Data['%Muts'].str.replace('%', ' ') \n",
    "Data['%UniqueMuts'] = Data['%UniqueMuts'].str.replace('%', ' ') \n",
    "Data['%ExistingMuts'] = Data['%ExistingMuts'].str.replace('%', ' ') \n",
    "Data['%N'].astype(float)\n",
    "Data['%Muts'].astype(float)\n",
    "Data['%UniqueMuts'].astype(float)\n",
    "Data['%ExistingMuts'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting rid of NaN data\n",
    "Data.dropna(subset = ['status'], inplace=True)\n",
    "Data.dropna(subset = ['ExistingMutList'], inplace=True)\n",
    "Data.index = range(len(Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chacking if there is any null data in ExistingMutList\n",
    "for i in range(len(Data)):\n",
    "    if pd.isnull(Data.ExistingMutList[i]) is True:\n",
    "        print('True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting rid of NaN data\n",
    "Data.drop(Data.loc[Data['status']=='unknown'].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#Labeling\n",
    "Data.replace(['Deceased'],value= [1], inplace=True)\n",
    "Data.status[Data['status'] != 1]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ExistingMutList_(NS3_A23V,NS3_G251V)</th>\n",
       "      <th>ExistingMutList_(NS3_G251V)</th>\n",
       "      <th>ExistingMutList_(NS8_L84S)</th>\n",
       "      <th>ExistingMutList_(NSP12_P323L)</th>\n",
       "      <th>ExistingMutList_(NSP12_P323L,NS8_L84S)</th>\n",
       "      <th>ExistingMutList_(NSP12_P323L,NSP12_A185V,Spike_D614G)</th>\n",
       "      <th>ExistingMutList_(NSP12_P323L,NSP12_A449V,Spike_D614G,N_G204R,N_R203K)</th>\n",
       "      <th>ExistingMutList_(NSP12_P323L,NSP12_T252N,Spike_D614G)</th>\n",
       "      <th>ExistingMutList_(NSP12_P323L,NSP12_T26I,Spike_D614G,M_D3G)</th>\n",
       "      <th>ExistingMutList_(NSP12_P323L,NSP12_V880I,Spike_D614G,Spike_E583D,NS3_Q57H,N_S194L)</th>\n",
       "      <th>...</th>\n",
       "      <th>Clade_G</th>\n",
       "      <th>Clade_Other</th>\n",
       "      <th>Clade_S</th>\n",
       "      <th>Clade_V</th>\n",
       "      <th>status</th>\n",
       "      <th>%N</th>\n",
       "      <th>Length(nt)</th>\n",
       "      <th>Length(aa)</th>\n",
       "      <th>%Muts</th>\n",
       "      <th>%UniqueMuts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29890.0</td>\n",
       "      <td>9710.0</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29849.0</td>\n",
       "      <td>9710.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.41</td>\n",
       "      <td>29899.0</td>\n",
       "      <td>9685.0</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29845.0</td>\n",
       "      <td>9710.0</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29834.0</td>\n",
       "      <td>9710.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 116 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ExistingMutList_(NS3_A23V,NS3_G251V)  ExistingMutList_(NS3_G251V)  \\\n",
       "0                                     0                            0   \n",
       "1                                     0                            0   \n",
       "2                                     0                            0   \n",
       "3                                     0                            0   \n",
       "4                                     0                            0   \n",
       "\n",
       "   ExistingMutList_(NS8_L84S)  ExistingMutList_(NSP12_P323L)  \\\n",
       "0                           0                              0   \n",
       "1                           0                              0   \n",
       "2                           0                              0   \n",
       "3                           0                              0   \n",
       "4                           0                              0   \n",
       "\n",
       "   ExistingMutList_(NSP12_P323L,NS8_L84S)  \\\n",
       "0                                       0   \n",
       "1                                       0   \n",
       "2                                       0   \n",
       "3                                       0   \n",
       "4                                       0   \n",
       "\n",
       "   ExistingMutList_(NSP12_P323L,NSP12_A185V,Spike_D614G)  \\\n",
       "0                                                  0       \n",
       "1                                                  0       \n",
       "2                                                  0       \n",
       "3                                                  0       \n",
       "4                                                  0       \n",
       "\n",
       "   ExistingMutList_(NSP12_P323L,NSP12_A449V,Spike_D614G,N_G204R,N_R203K)  \\\n",
       "0                                                  0                       \n",
       "1                                                  0                       \n",
       "2                                                  0                       \n",
       "3                                                  0                       \n",
       "4                                                  0                       \n",
       "\n",
       "   ExistingMutList_(NSP12_P323L,NSP12_T252N,Spike_D614G)  \\\n",
       "0                                                  0       \n",
       "1                                                  0       \n",
       "2                                                  0       \n",
       "3                                                  0       \n",
       "4                                                  0       \n",
       "\n",
       "   ExistingMutList_(NSP12_P323L,NSP12_T26I,Spike_D614G,M_D3G)  \\\n",
       "0                                                  0            \n",
       "1                                                  0            \n",
       "2                                                  0            \n",
       "3                                                  0            \n",
       "4                                                  0            \n",
       "\n",
       "   ExistingMutList_(NSP12_P323L,NSP12_V880I,Spike_D614G,Spike_E583D,NS3_Q57H,N_S194L)  \\\n",
       "0                                                  0                                    \n",
       "1                                                  0                                    \n",
       "2                                                  0                                    \n",
       "3                                                  0                                    \n",
       "4                                                  0                                    \n",
       "\n",
       "   ...  Clade_G  Clade_Other  Clade_S  Clade_V  status     %N  Length(nt)  \\\n",
       "0  ...        1            0        0        0       0  0.00      29890.0   \n",
       "1  ...        1            0        0        0       0  0.00      29849.0   \n",
       "2  ...        0            1        0        0       0  2.41      29899.0   \n",
       "3  ...        1            0        0        0       0  0.00      29845.0   \n",
       "4  ...        1            0        0        0       0  0.00      29834.0   \n",
       "\n",
       "   Length(aa)  %Muts  %UniqueMuts  \n",
       "0      9710.0  0.04         0.00   \n",
       "1      9710.0  0.05         0.00   \n",
       "2      9685.0  0.19         0.05   \n",
       "3      9710.0  0.04         0.01   \n",
       "4      9710.0  0.03         0.00   \n",
       "\n",
       "[5 rows x 116 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using dummies instead of location data for ML input\n",
    "Data_model = pd.concat([pd.get_dummies(Data[['ExistingMutList']]),pd.get_dummies(Data[['Clade']]), Data[['status','%N','Length(nt)','Length(aa)',\n",
    "                                                   '%Muts','%UniqueMuts']]], axis=1)\n",
    "Data_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    137\n",
       "1     32\n",
       "Name: status, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the balance in the data\n",
    "Data_model['status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    137\n",
       "1     32\n",
       "Name: status, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Separating target (label) data\n",
    "X = Data_model.drop('status',axis=1).astype(float)\n",
    "y = Data_model.status\n",
    "y=y.astype(int)\n",
    "y.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel PCA diemnsion reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### from sklearn.decomposition import PCA, KernelPCA\n",
    "kpca = KernelPCA(kernel=\"rbf\")\n",
    "kpca.fit(X_train)\n",
    "#X_back = kpca.inverse_transform(X_kpca)\n",
    "X_train = kpca.transform(X_train)\n",
    "X_test = kpca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data to standarize them\n",
    "#Here caling reduced the iteration number from 200 to 50\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "sc = StandardScaler()\n",
    "#sc = MinMaxScaler()\n",
    "sc.fit(X)\n",
    "X_scaled = sc.transform(X)\n",
    "X_normal_scaled = X_scaled[y == 0] \n",
    "X_deseased_scaled = X_scaled[y == 1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple feedforward autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 115)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 348       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 115)               460       \n",
      "=================================================================\n",
      "Total params: 808\n",
      "Trainable params: 808\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 109 samples, validate on 28 samples\n",
      "Epoch 1/300\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 0.6988 - val_loss: 0.6852\n",
      "Epoch 2/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.6972 - val_loss: 0.6837\n",
      "Epoch 3/300\n",
      "109/109 [==============================] - 0s 51us/step - loss: 0.6957 - val_loss: 0.6824\n",
      "Epoch 4/300\n",
      "109/109 [==============================] - 0s 44us/step - loss: 0.6942 - val_loss: 0.6809\n",
      "Epoch 5/300\n",
      "109/109 [==============================] - 0s 64us/step - loss: 0.6927 - val_loss: 0.6795\n",
      "Epoch 6/300\n",
      "109/109 [==============================] - 0s 43us/step - loss: 0.6912 - val_loss: 0.6781\n",
      "Epoch 7/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.6897 - val_loss: 0.6767\n",
      "Epoch 8/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.6882 - val_loss: 0.6753\n",
      "Epoch 9/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.6867 - val_loss: 0.6739\n",
      "Epoch 10/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.6853 - val_loss: 0.6725\n",
      "Epoch 11/300\n",
      "109/109 [==============================] - 0s 33us/step - loss: 0.6838 - val_loss: 0.6711\n",
      "Epoch 12/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.6823 - val_loss: 0.6697\n",
      "Epoch 13/300\n",
      "109/109 [==============================] - 0s 91us/step - loss: 0.6809 - val_loss: 0.6683\n",
      "Epoch 14/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.6794 - val_loss: 0.6669\n",
      "Epoch 15/300\n",
      "109/109 [==============================] - 0s 63us/step - loss: 0.6780 - val_loss: 0.6655\n",
      "Epoch 16/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.6765 - val_loss: 0.6641\n",
      "Epoch 17/300\n",
      "109/109 [==============================] - 0s 51us/step - loss: 0.6751 - val_loss: 0.6627\n",
      "Epoch 18/300\n",
      "109/109 [==============================] - 0s 71us/step - loss: 0.6736 - val_loss: 0.6613\n",
      "Epoch 19/300\n",
      "109/109 [==============================] - 0s 35us/step - loss: 0.6722 - val_loss: 0.6600\n",
      "Epoch 20/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.6707 - val_loss: 0.6586\n",
      "Epoch 21/300\n",
      "109/109 [==============================] - 0s 45us/step - loss: 0.6692 - val_loss: 0.6572\n",
      "Epoch 22/300\n",
      "109/109 [==============================] - 0s 45us/step - loss: 0.6678 - val_loss: 0.6559\n",
      "Epoch 23/300\n",
      "109/109 [==============================] - 0s 36us/step - loss: 0.6663 - val_loss: 0.6545\n",
      "Epoch 24/300\n",
      "109/109 [==============================] - 0s 58us/step - loss: 0.6648 - val_loss: 0.6531\n",
      "Epoch 25/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.6633 - val_loss: 0.6518\n",
      "Epoch 26/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.6618 - val_loss: 0.6504\n",
      "Epoch 27/300\n",
      "109/109 [==============================] - 0s 47us/step - loss: 0.6603 - val_loss: 0.6491\n",
      "Epoch 28/300\n",
      "109/109 [==============================] - 0s 28us/step - loss: 0.6587 - val_loss: 0.6477\n",
      "Epoch 29/300\n",
      "109/109 [==============================] - 0s 31us/step - loss: 0.6572 - val_loss: 0.6464\n",
      "Epoch 30/300\n",
      "109/109 [==============================] - 0s 28us/step - loss: 0.6556 - val_loss: 0.6450\n",
      "Epoch 31/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.6541 - val_loss: 0.6437\n",
      "Epoch 32/300\n",
      "109/109 [==============================] - 0s 47us/step - loss: 0.6525 - val_loss: 0.6423\n",
      "Epoch 33/300\n",
      "109/109 [==============================] - 0s 42us/step - loss: 0.6509 - val_loss: 0.6410\n",
      "Epoch 34/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.6493 - val_loss: 0.6396\n",
      "Epoch 35/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.6477 - val_loss: 0.6383\n",
      "Epoch 36/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.6460 - val_loss: 0.6369\n",
      "Epoch 37/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.6443 - val_loss: 0.6356\n",
      "Epoch 38/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.6427 - val_loss: 0.6342\n",
      "Epoch 39/300\n",
      "109/109 [==============================] - 0s 32us/step - loss: 0.6410 - val_loss: 0.6329\n",
      "Epoch 40/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.6393 - val_loss: 0.6316\n",
      "Epoch 41/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.6375 - val_loss: 0.6302\n",
      "Epoch 42/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.6358 - val_loss: 0.6289\n",
      "Epoch 43/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.6340 - val_loss: 0.6275\n",
      "Epoch 44/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.6322 - val_loss: 0.6262\n",
      "Epoch 45/300\n",
      "109/109 [==============================] - 0s 36us/step - loss: 0.6304 - val_loss: 0.6248\n",
      "Epoch 46/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.6286 - val_loss: 0.6235\n",
      "Epoch 47/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.6268 - val_loss: 0.6221\n",
      "Epoch 48/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.6249 - val_loss: 0.6208\n",
      "Epoch 49/300\n",
      "109/109 [==============================] - 0s 28us/step - loss: 0.6230 - val_loss: 0.6194\n",
      "Epoch 50/300\n",
      "109/109 [==============================] - 0s 42us/step - loss: 0.6211 - val_loss: 0.6181\n",
      "Epoch 51/300\n",
      "109/109 [==============================] - 0s 30us/step - loss: 0.6192 - val_loss: 0.6167\n",
      "Epoch 52/300\n",
      "109/109 [==============================] - 0s 28us/step - loss: 0.6173 - val_loss: 0.6154\n",
      "Epoch 53/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.6154 - val_loss: 0.6140\n",
      "Epoch 54/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.6134 - val_loss: 0.6127\n",
      "Epoch 55/300\n",
      "109/109 [==============================] - 0s 54us/step - loss: 0.6114 - val_loss: 0.6113\n",
      "Epoch 56/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.6094 - val_loss: 0.6100\n",
      "Epoch 57/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.6073 - val_loss: 0.6086\n",
      "Epoch 58/300\n",
      "109/109 [==============================] - 0s 26us/step - loss: 0.6053 - val_loss: 0.6073\n",
      "Epoch 59/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.6032 - val_loss: 0.6059\n",
      "Epoch 60/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.6011 - val_loss: 0.6046\n",
      "Epoch 61/300\n",
      "109/109 [==============================] - 0s 40us/step - loss: 0.5990 - val_loss: 0.6032\n",
      "Epoch 62/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.5969 - val_loss: 0.6018\n",
      "Epoch 63/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.5948 - val_loss: 0.6005\n",
      "Epoch 64/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.5926 - val_loss: 0.5991\n",
      "Epoch 65/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.5904 - val_loss: 0.5978\n",
      "Epoch 66/300\n",
      "109/109 [==============================] - 0s 41us/step - loss: 0.5882 - val_loss: 0.5964\n",
      "Epoch 67/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.5860 - val_loss: 0.5951\n",
      "Epoch 68/300\n",
      "109/109 [==============================] - 0s 45us/step - loss: 0.5838 - val_loss: 0.5937\n",
      "Epoch 69/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.5815 - val_loss: 0.5924\n",
      "Epoch 70/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.5793 - val_loss: 0.5910\n",
      "Epoch 71/300\n",
      "109/109 [==============================] - 0s 36us/step - loss: 0.5770 - val_loss: 0.5897\n",
      "Epoch 72/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.5747 - val_loss: 0.5883\n",
      "Epoch 73/300\n",
      "109/109 [==============================] - 0s 54us/step - loss: 0.5724 - val_loss: 0.5870\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.5700 - val_loss: 0.5856\n",
      "Epoch 75/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.5677 - val_loss: 0.5843\n",
      "Epoch 76/300\n",
      "109/109 [==============================] - 0s 33us/step - loss: 0.5653 - val_loss: 0.5829\n",
      "Epoch 77/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.5629 - val_loss: 0.5816\n",
      "Epoch 78/300\n",
      "109/109 [==============================] - 0s 28us/step - loss: 0.5606 - val_loss: 0.5802\n",
      "Epoch 79/300\n",
      "109/109 [==============================] - 0s 41us/step - loss: 0.5581 - val_loss: 0.5789\n",
      "Epoch 80/300\n",
      "109/109 [==============================] - 0s 39us/step - loss: 0.5557 - val_loss: 0.5776\n",
      "Epoch 81/300\n",
      "109/109 [==============================] - 0s 28us/step - loss: 0.5533 - val_loss: 0.5762\n",
      "Epoch 82/300\n",
      "109/109 [==============================] - 0s 45us/step - loss: 0.5508 - val_loss: 0.5749\n",
      "Epoch 83/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.5484 - val_loss: 0.5736\n",
      "Epoch 84/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.5459 - val_loss: 0.5722\n",
      "Epoch 85/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.5434 - val_loss: 0.5709\n",
      "Epoch 86/300\n",
      "109/109 [==============================] - 0s 61us/step - loss: 0.5409 - val_loss: 0.5695\n",
      "Epoch 87/300\n",
      "109/109 [==============================] - 0s 28us/step - loss: 0.5384 - val_loss: 0.5682\n",
      "Epoch 88/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.5358 - val_loss: 0.5669\n",
      "Epoch 89/300\n",
      "109/109 [==============================] - 0s 59us/step - loss: 0.5333 - val_loss: 0.5655\n",
      "Epoch 90/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.5307 - val_loss: 0.5642\n",
      "Epoch 91/300\n",
      "109/109 [==============================] - 0s 42us/step - loss: 0.5282 - val_loss: 0.5629\n",
      "Epoch 92/300\n",
      "109/109 [==============================] - 0s 45us/step - loss: 0.5256 - val_loss: 0.5616\n",
      "Epoch 93/300\n",
      "109/109 [==============================] - 0s 36us/step - loss: 0.5230 - val_loss: 0.5603\n",
      "Epoch 94/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.5204 - val_loss: 0.5590\n",
      "Epoch 95/300\n",
      "109/109 [==============================] - 0s 35us/step - loss: 0.5178 - val_loss: 0.5577\n",
      "Epoch 96/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.5152 - val_loss: 0.5564\n",
      "Epoch 97/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.5126 - val_loss: 0.5551\n",
      "Epoch 98/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.5099 - val_loss: 0.5537\n",
      "Epoch 99/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.5073 - val_loss: 0.5524\n",
      "Epoch 100/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.5046 - val_loss: 0.5511\n",
      "Epoch 101/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.5020 - val_loss: 0.5498\n",
      "Epoch 102/300\n",
      "109/109 [==============================] - 0s 64us/step - loss: 0.4993 - val_loss: 0.5486\n",
      "Epoch 103/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.4966 - val_loss: 0.5473\n",
      "Epoch 104/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.4939 - val_loss: 0.5460\n",
      "Epoch 105/300\n",
      "109/109 [==============================] - 0s 57us/step - loss: 0.4912 - val_loss: 0.5447\n",
      "Epoch 106/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.4885 - val_loss: 0.5435\n",
      "Epoch 107/300\n",
      "109/109 [==============================] - 0s 36us/step - loss: 0.4858 - val_loss: 0.5422\n",
      "Epoch 108/300\n",
      "109/109 [==============================] - 0s 43us/step - loss: 0.4832 - val_loss: 0.5409\n",
      "Epoch 109/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.4804 - val_loss: 0.5396\n",
      "Epoch 110/300\n",
      "109/109 [==============================] - 0s 44us/step - loss: 0.4777 - val_loss: 0.5383\n",
      "Epoch 111/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.4750 - val_loss: 0.5371\n",
      "Epoch 112/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.4723 - val_loss: 0.5358\n",
      "Epoch 113/300\n",
      "109/109 [==============================] - 0s 36us/step - loss: 0.4696 - val_loss: 0.5346\n",
      "Epoch 114/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.4669 - val_loss: 0.5333\n",
      "Epoch 115/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.4641 - val_loss: 0.5321\n",
      "Epoch 116/300\n",
      "109/109 [==============================] - 0s 51us/step - loss: 0.4614 - val_loss: 0.5308\n",
      "Epoch 117/300\n",
      "109/109 [==============================] - 0s 36us/step - loss: 0.4587 - val_loss: 0.5296\n",
      "Epoch 118/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.4559 - val_loss: 0.5284\n",
      "Epoch 119/300\n",
      "109/109 [==============================] - 0s 35us/step - loss: 0.4532 - val_loss: 0.5272\n",
      "Epoch 120/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.4505 - val_loss: 0.5260\n",
      "Epoch 121/300\n",
      "109/109 [==============================] - 0s 28us/step - loss: 0.4478 - val_loss: 0.5248\n",
      "Epoch 122/300\n",
      "109/109 [==============================] - 0s 32us/step - loss: 0.4450 - val_loss: 0.5236\n",
      "Epoch 123/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.4423 - val_loss: 0.5224\n",
      "Epoch 124/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.4396 - val_loss: 0.5212\n",
      "Epoch 125/300\n",
      "109/109 [==============================] - 0s 35us/step - loss: 0.4368 - val_loss: 0.5200\n",
      "Epoch 126/300\n",
      "109/109 [==============================] - 0s 32us/step - loss: 0.4341 - val_loss: 0.5189\n",
      "Epoch 127/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.4314 - val_loss: 0.5177\n",
      "Epoch 128/300\n",
      "109/109 [==============================] - 0s 36us/step - loss: 0.4286 - val_loss: 0.5165\n",
      "Epoch 129/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.4259 - val_loss: 0.5154\n",
      "Epoch 130/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.4232 - val_loss: 0.5142\n",
      "Epoch 131/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.4205 - val_loss: 0.5130\n",
      "Epoch 132/300\n",
      "109/109 [==============================] - 0s 28us/step - loss: 0.4178 - val_loss: 0.5119\n",
      "Epoch 133/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.4150 - val_loss: 0.5107\n",
      "Epoch 134/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.4123 - val_loss: 0.5096\n",
      "Epoch 135/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.4096 - val_loss: 0.5085\n",
      "Epoch 136/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.4069 - val_loss: 0.5074\n",
      "Epoch 137/300\n",
      "109/109 [==============================] - 0s 28us/step - loss: 0.4042 - val_loss: 0.5062\n",
      "Epoch 138/300\n",
      "109/109 [==============================] - 0s 28us/step - loss: 0.4015 - val_loss: 0.5051\n",
      "Epoch 139/300\n",
      "109/109 [==============================] - 0s 58us/step - loss: 0.3988 - val_loss: 0.5041\n",
      "Epoch 140/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.3961 - val_loss: 0.5030\n",
      "Epoch 141/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.3934 - val_loss: 0.5019\n",
      "Epoch 142/300\n",
      "109/109 [==============================] - 0s 29us/step - loss: 0.3908 - val_loss: 0.5008\n",
      "Epoch 143/300\n",
      "109/109 [==============================] - 0s 74us/step - loss: 0.3881 - val_loss: 0.4998\n",
      "Epoch 144/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.3854 - val_loss: 0.4987\n",
      "Epoch 145/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.3827 - val_loss: 0.4976\n",
      "Epoch 146/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.3801 - val_loss: 0.4966\n",
      "Epoch 147/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.3774 - val_loss: 0.4955\n",
      "Epoch 148/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.3748 - val_loss: 0.4945\n",
      "Epoch 149/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.3721 - val_loss: 0.4934\n",
      "Epoch 150/300\n",
      "109/109 [==============================] - 0s 26us/step - loss: 0.3695 - val_loss: 0.4924\n",
      "Epoch 151/300\n",
      "109/109 [==============================] - 0s 28us/step - loss: 0.3668 - val_loss: 0.4914\n",
      "Epoch 152/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.3642 - val_loss: 0.4904\n",
      "Epoch 153/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.3616 - val_loss: 0.4893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.3590 - val_loss: 0.4883\n",
      "Epoch 155/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.3564 - val_loss: 0.4874\n",
      "Epoch 156/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.3538 - val_loss: 0.4864\n",
      "Epoch 157/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.3512 - val_loss: 0.4854\n",
      "Epoch 158/300\n",
      "109/109 [==============================] - 0s 28us/step - loss: 0.3486 - val_loss: 0.4845\n",
      "Epoch 159/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.3460 - val_loss: 0.4835\n",
      "Epoch 160/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.3434 - val_loss: 0.4826\n",
      "Epoch 161/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.3409 - val_loss: 0.4817\n",
      "Epoch 162/300\n",
      "109/109 [==============================] - 0s 42us/step - loss: 0.3383 - val_loss: 0.4807\n",
      "Epoch 163/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.3358 - val_loss: 0.4798\n",
      "Epoch 164/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.3332 - val_loss: 0.4789\n",
      "Epoch 165/300\n",
      "109/109 [==============================] - 0s 34us/step - loss: 0.3307 - val_loss: 0.4780\n",
      "Epoch 166/300\n",
      "109/109 [==============================] - 0s 36us/step - loss: 0.3282 - val_loss: 0.4771\n",
      "Epoch 167/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.3256 - val_loss: 0.4761\n",
      "Epoch 168/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.3231 - val_loss: 0.4752\n",
      "Epoch 169/300\n",
      "109/109 [==============================] - 0s 73us/step - loss: 0.3206 - val_loss: 0.4742\n",
      "Epoch 170/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.3181 - val_loss: 0.4733\n",
      "Epoch 171/300\n",
      "109/109 [==============================] - 0s 52us/step - loss: 0.3156 - val_loss: 0.4723\n",
      "Epoch 172/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.3131 - val_loss: 0.4714\n",
      "Epoch 173/300\n",
      "109/109 [==============================] - 0s 45us/step - loss: 0.3106 - val_loss: 0.4705\n",
      "Epoch 174/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.3081 - val_loss: 0.4695\n",
      "Epoch 175/300\n",
      "109/109 [==============================] - 0s 63us/step - loss: 0.3057 - val_loss: 0.4686\n",
      "Epoch 176/300\n",
      "109/109 [==============================] - 0s 28us/step - loss: 0.3032 - val_loss: 0.4677\n",
      "Epoch 177/300\n",
      "109/109 [==============================] - 0s 41us/step - loss: 0.3008 - val_loss: 0.4668\n",
      "Epoch 178/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.2983 - val_loss: 0.4659\n",
      "Epoch 179/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.2959 - val_loss: 0.4651\n",
      "Epoch 180/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.2935 - val_loss: 0.4641\n",
      "Epoch 181/300\n",
      "109/109 [==============================] - 0s 54us/step - loss: 0.2910 - val_loss: 0.4632\n",
      "Epoch 182/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.2886 - val_loss: 0.4623\n",
      "Epoch 183/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.2862 - val_loss: 0.4614\n",
      "Epoch 184/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.2838 - val_loss: 0.4605\n",
      "Epoch 185/300\n",
      "109/109 [==============================] - 0s 36us/step - loss: 0.2814 - val_loss: 0.4597\n",
      "Epoch 186/300\n",
      "109/109 [==============================] - 0s 42us/step - loss: 0.2790 - val_loss: 0.4588\n",
      "Epoch 187/300\n",
      "109/109 [==============================] - 0s 78us/step - loss: 0.2767 - val_loss: 0.4579\n",
      "Epoch 188/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.2743 - val_loss: 0.4571\n",
      "Epoch 189/300\n",
      "109/109 [==============================] - 0s 71us/step - loss: 0.2719 - val_loss: 0.4562\n",
      "Epoch 190/300\n",
      "109/109 [==============================] - 0s 45us/step - loss: 0.2696 - val_loss: 0.4554\n",
      "Epoch 191/300\n",
      "109/109 [==============================] - 0s 44us/step - loss: 0.2672 - val_loss: 0.4545\n",
      "Epoch 192/300\n",
      "109/109 [==============================] - 0s 202us/step - loss: 0.2649 - val_loss: 0.4537\n",
      "Epoch 193/300\n",
      "109/109 [==============================] - 0s 45us/step - loss: 0.2625 - val_loss: 0.4529\n",
      "Epoch 194/300\n",
      "109/109 [==============================] - 0s 41us/step - loss: 0.2602 - val_loss: 0.4520\n",
      "Epoch 195/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.2579 - val_loss: 0.4512\n",
      "Epoch 196/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.2556 - val_loss: 0.4504\n",
      "Epoch 197/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.2532 - val_loss: 0.4495\n",
      "Epoch 198/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.2509 - val_loss: 0.4487\n",
      "Epoch 199/300\n",
      "109/109 [==============================] - 0s 31us/step - loss: 0.2487 - val_loss: 0.4479\n",
      "Epoch 200/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.2464 - val_loss: 0.4471\n",
      "Epoch 201/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.2441 - val_loss: 0.4463\n",
      "Epoch 202/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.2418 - val_loss: 0.4455\n",
      "Epoch 203/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.2395 - val_loss: 0.4447\n",
      "Epoch 204/300\n",
      "109/109 [==============================] - 0s 40us/step - loss: 0.2373 - val_loss: 0.4439\n",
      "Epoch 205/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.2350 - val_loss: 0.4432\n",
      "Epoch 206/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.2328 - val_loss: 0.4424\n",
      "Epoch 207/300\n",
      "109/109 [==============================] - 0s 47us/step - loss: 0.2306 - val_loss: 0.4416\n",
      "Epoch 208/300\n",
      "109/109 [==============================] - 0s 36us/step - loss: 0.2283 - val_loss: 0.4409\n",
      "Epoch 209/300\n",
      "109/109 [==============================] - 0s 28us/step - loss: 0.2261 - val_loss: 0.4401\n",
      "Epoch 210/300\n",
      "109/109 [==============================] - 0s 31us/step - loss: 0.2239 - val_loss: 0.4394\n",
      "Epoch 211/300\n",
      "109/109 [==============================] - 0s 28us/step - loss: 0.2216 - val_loss: 0.4386\n",
      "Epoch 212/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.2194 - val_loss: 0.4378\n",
      "Epoch 213/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.2172 - val_loss: 0.4371\n",
      "Epoch 214/300\n",
      "109/109 [==============================] - 0s 28us/step - loss: 0.2150 - val_loss: 0.4363\n",
      "Epoch 215/300\n",
      "109/109 [==============================] - 0s 29us/step - loss: 0.2129 - val_loss: 0.4356\n",
      "Epoch 216/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.2107 - val_loss: 0.4348\n",
      "Epoch 217/300\n",
      "109/109 [==============================] - 0s 28us/step - loss: 0.2085 - val_loss: 0.4341\n",
      "Epoch 218/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.2063 - val_loss: 0.4334\n",
      "Epoch 219/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.2042 - val_loss: 0.4327\n",
      "Epoch 220/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.2020 - val_loss: 0.4319\n",
      "Epoch 221/300\n",
      "109/109 [==============================] - 0s 36us/step - loss: 0.1999 - val_loss: 0.4312\n",
      "Epoch 222/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.1977 - val_loss: 0.4305\n",
      "Epoch 223/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.1956 - val_loss: 0.4298\n",
      "Epoch 224/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.1934 - val_loss: 0.4291\n",
      "Epoch 225/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.1913 - val_loss: 0.4284\n",
      "Epoch 226/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.1892 - val_loss: 0.4277\n",
      "Epoch 227/300\n",
      "109/109 [==============================] - 0s 28us/step - loss: 0.1871 - val_loss: 0.4271\n",
      "Epoch 228/300\n",
      "109/109 [==============================] - 0s 45us/step - loss: 0.1849 - val_loss: 0.4264\n",
      "Epoch 229/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.1828 - val_loss: 0.4257\n",
      "Epoch 230/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.1807 - val_loss: 0.4250\n",
      "Epoch 231/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.1786 - val_loss: 0.4244\n",
      "Epoch 232/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.1765 - val_loss: 0.4237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233/300\n",
      "109/109 [==============================] - 0s 38us/step - loss: 0.1744 - val_loss: 0.4230\n",
      "Epoch 234/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.1723 - val_loss: 0.4224\n",
      "Epoch 235/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.1703 - val_loss: 0.4217\n",
      "Epoch 236/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.1682 - val_loss: 0.4210\n",
      "Epoch 237/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.1661 - val_loss: 0.4204\n",
      "Epoch 238/300\n",
      "109/109 [==============================] - 0s 42us/step - loss: 0.1641 - val_loss: 0.4197\n",
      "Epoch 239/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.1620 - val_loss: 0.4190\n",
      "Epoch 240/300\n",
      "109/109 [==============================] - 0s 36us/step - loss: 0.1599 - val_loss: 0.4184\n",
      "Epoch 241/300\n",
      "109/109 [==============================] - 0s 28us/step - loss: 0.1579 - val_loss: 0.4177\n",
      "Epoch 242/300\n",
      "109/109 [==============================] - 0s 45us/step - loss: 0.1558 - val_loss: 0.4171\n",
      "Epoch 243/300\n",
      "109/109 [==============================] - 0s 45us/step - loss: 0.1538 - val_loss: 0.4164\n",
      "Epoch 244/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.1518 - val_loss: 0.4158\n",
      "Epoch 245/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.1497 - val_loss: 0.4151\n",
      "Epoch 246/300\n",
      "109/109 [==============================] - 0s 36us/step - loss: 0.1477 - val_loss: 0.4145\n",
      "Epoch 247/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.1457 - val_loss: 0.4138\n",
      "Epoch 248/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.1436 - val_loss: 0.4132\n",
      "Epoch 249/300\n",
      "109/109 [==============================] - 0s 36us/step - loss: 0.1416 - val_loss: 0.4125\n",
      "Epoch 250/300\n",
      "109/109 [==============================] - 0s 36us/step - loss: 0.1396 - val_loss: 0.4119\n",
      "Epoch 251/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.1376 - val_loss: 0.4112\n",
      "Epoch 252/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.1356 - val_loss: 0.4106\n",
      "Epoch 253/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.1336 - val_loss: 0.4100\n",
      "Epoch 254/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.1316 - val_loss: 0.4094\n",
      "Epoch 255/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.1296 - val_loss: 0.4087\n",
      "Epoch 256/300\n",
      "109/109 [==============================] - 0s 47us/step - loss: 0.1276 - val_loss: 0.4081\n",
      "Epoch 257/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.1256 - val_loss: 0.4075\n",
      "Epoch 258/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.1236 - val_loss: 0.4069\n",
      "Epoch 259/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.1216 - val_loss: 0.4063\n",
      "Epoch 260/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.1197 - val_loss: 0.4057\n",
      "Epoch 261/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.1177 - val_loss: 0.4051\n",
      "Epoch 262/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.1157 - val_loss: 0.4045\n",
      "Epoch 263/300\n",
      "109/109 [==============================] - 0s 48us/step - loss: 0.1137 - val_loss: 0.4039\n",
      "Epoch 264/300\n",
      "109/109 [==============================] - 0s 45us/step - loss: 0.1118 - val_loss: 0.4033\n",
      "Epoch 265/300\n",
      "109/109 [==============================] - 0s 44us/step - loss: 0.1098 - val_loss: 0.4027\n",
      "Epoch 266/300\n",
      "109/109 [==============================] - 0s 82us/step - loss: 0.1079 - val_loss: 0.4020\n",
      "Epoch 267/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.1059 - val_loss: 0.4014\n",
      "Epoch 268/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.1039 - val_loss: 0.4008\n",
      "Epoch 269/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.1020 - val_loss: 0.4002\n",
      "Epoch 270/300\n",
      "109/109 [==============================] - 0s 35us/step - loss: 0.1000 - val_loss: 0.3996\n",
      "Epoch 271/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.0981 - val_loss: 0.3991\n",
      "Epoch 272/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.0962 - val_loss: 0.3985\n",
      "Epoch 273/300\n",
      "109/109 [==============================] - 0s 76us/step - loss: 0.0942 - val_loss: 0.3979\n",
      "Epoch 274/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.0922 - val_loss: 0.3973\n",
      "Epoch 275/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.0904 - val_loss: 0.3967\n",
      "Epoch 276/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.0884 - val_loss: 0.3961\n",
      "Epoch 277/300\n",
      "109/109 [==============================] - 0s 31us/step - loss: 0.0865 - val_loss: 0.3955\n",
      "Epoch 278/300\n",
      "109/109 [==============================] - 0s 63us/step - loss: 0.0845 - val_loss: 0.3949\n",
      "Epoch 279/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.0826 - val_loss: 0.3943\n",
      "Epoch 280/300\n",
      "109/109 [==============================] - 0s 36us/step - loss: 0.0806 - val_loss: 0.3937\n",
      "Epoch 281/300\n",
      "109/109 [==============================] - 0s 45us/step - loss: 0.0787 - val_loss: 0.3931\n",
      "Epoch 282/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.0769 - val_loss: 0.3925\n",
      "Epoch 283/300\n",
      "109/109 [==============================] - 0s 36us/step - loss: 0.0748 - val_loss: 0.3919\n",
      "Epoch 284/300\n",
      "109/109 [==============================] - 0s 46us/step - loss: 0.0729 - val_loss: 0.3913\n",
      "Epoch 285/300\n",
      "109/109 [==============================] - 0s 31us/step - loss: 0.0711 - val_loss: 0.3907\n",
      "Epoch 286/300\n",
      "109/109 [==============================] - 0s 57us/step - loss: 0.0690 - val_loss: 0.3901\n",
      "Epoch 287/300\n",
      "109/109 [==============================] - 0s 55us/step - loss: 0.0671 - val_loss: 0.3895\n",
      "Epoch 288/300\n",
      "109/109 [==============================] - 0s 45us/step - loss: 0.0655 - val_loss: 0.3889\n",
      "Epoch 289/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.0637 - val_loss: 0.3884\n",
      "Epoch 290/300\n",
      "109/109 [==============================] - 0s 28us/step - loss: 0.0615 - val_loss: 0.3878\n",
      "Epoch 291/300\n",
      "109/109 [==============================] - 0s 36us/step - loss: 0.0597 - val_loss: 0.3872\n",
      "Epoch 292/300\n",
      "109/109 [==============================] - 0s 64us/step - loss: 0.0577 - val_loss: 0.3866\n",
      "Epoch 293/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.0559 - val_loss: 0.3860\n",
      "Epoch 294/300\n",
      "109/109 [==============================] - 0s 36us/step - loss: 0.0545 - val_loss: 0.3855\n",
      "Epoch 295/300\n",
      "109/109 [==============================] - 0s 64us/step - loss: 0.0525 - val_loss: 0.3849\n",
      "Epoch 296/300\n",
      "109/109 [==============================] - 0s 36us/step - loss: 0.0506 - val_loss: 0.3843\n",
      "Epoch 297/300\n",
      "109/109 [==============================] - 0s 37us/step - loss: 0.0490 - val_loss: 0.3838\n",
      "Epoch 298/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.0472 - val_loss: 0.3832\n",
      "Epoch 299/300\n",
      "109/109 [==============================] - 0s 27us/step - loss: 0.0456 - val_loss: 0.3826\n",
      "Epoch 300/300\n",
      "109/109 [==============================] - 0s 45us/step - loss: 0.0439 - val_loss: 0.3821\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "window_length = 115\n",
    "encoding_dim = 3\n",
    "epochs = 300\n",
    "test_samples = 2000\n",
    "\n",
    "X_simple = X.values.reshape((len(X), np.prod(X.shape[1:])))\n",
    "\n",
    "# this is our input placeholder\n",
    "input_window = Input(shape=(window_length,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_window)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(window_length, activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_window, decoded)\n",
    "\n",
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_window, encoded)\n",
    "\n",
    "\n",
    "autoencoder.summary()\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "history = autoencoder.fit(X_normal_scaled, X_normal_scaled,\n",
    "                epochs=epochs,\n",
    "                batch_size=1024,\n",
    "                shuffle=True,\n",
    "                validation_split = 0.20)\n",
    "\n",
    "# Separating the points encoded by the Auto-encoder as normal and fraud \n",
    "decoded_X_normal = autoencoder.predict(X_normal_scaled)\n",
    "decoded_X_deseased = autoencoder.predict(X_deseased_scaled)\n",
    "# Combining the encoded points into a single table  \n",
    "encoded_X = np.append(decoded_X_normal, decoded_X_deseased, axis = 0) \n",
    "y_normal = np.zeros(decoded_X_normal.shape[0]) \n",
    "y_deceased = np.ones(decoded_X_deseased.shape[0]) \n",
    "encoded_y = np.append(y_normal, y_deceased) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep autoencoder NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 109 samples, validate on 28 samples\n",
      "Epoch 1/10\n",
      "109/109 [==============================] - 2s 19ms/step - loss: 0.3033 - val_loss: 0.1544\n",
      "Epoch 2/10\n",
      "109/109 [==============================] - 0s 367us/step - loss: 0.0453 - val_loss: 0.2149\n",
      "Epoch 3/10\n",
      "109/109 [==============================] - 0s 329us/step - loss: -0.0887 - val_loss: 0.1867\n",
      "Epoch 4/10\n",
      "109/109 [==============================] - 0s 399us/step - loss: -0.1546 - val_loss: 0.2093\n",
      "Epoch 5/10\n",
      "109/109 [==============================] - 0s 375us/step - loss: -0.2069 - val_loss: 0.1905\n",
      "Epoch 6/10\n",
      "109/109 [==============================] - 0s 366us/step - loss: -0.2405 - val_loss: 0.1677\n",
      "Epoch 7/10\n",
      "109/109 [==============================] - 0s 366us/step - loss: -0.2682 - val_loss: 0.1273\n",
      "Epoch 8/10\n",
      "109/109 [==============================] - 0s 384us/step - loss: -0.2582 - val_loss: 0.1511\n",
      "Epoch 9/10\n",
      "109/109 [==============================] - 0s 398us/step - loss: -0.2897 - val_loss: 0.1462\n",
      "Epoch 10/10\n",
      "109/109 [==============================] - 0s 423us/step - loss: -0.2676 - val_loss: 0.1551\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras import regularizers \n",
    "# Building the Input Layer \n",
    "input_layer = Input(shape =(X.shape[1], )) \n",
    "  \n",
    "# Building the Encoder network \n",
    "encoded = Dense(100, activation ='tanh', \n",
    "                activity_regularizer = regularizers.l1(10e-5))(input_layer) \n",
    "encoded = Dense(50, activation ='tanh', \n",
    "                activity_regularizer = regularizers.l1(10e-5))(encoded) \n",
    "encoded = Dense(25, activation ='tanh', \n",
    "                activity_regularizer = regularizers.l1(10e-5))(encoded) \n",
    "encoded = Dense(12, activation ='tanh', \n",
    "                activity_regularizer = regularizers.l1(10e-5))(encoded) \n",
    "encoded = Dense(6, activation ='relu')(encoded) \n",
    "  \n",
    "# Building the Decoder network \n",
    "decoded = Dense(12, activation ='tanh')(encoded) \n",
    "decoded = Dense(25, activation ='tanh')(decoded) \n",
    "decoded = Dense(50, activation ='tanh')(decoded) \n",
    "decoded = Dense(100, activation ='tanh')(decoded) \n",
    "  \n",
    "# Building the Output Layer \n",
    "output_layer = Dense(X.shape[1], activation ='relu')(decoded) \n",
    "#Step 8: Defining and Training the Auto-encoder\n",
    "# Defining the parameters of the Auto-encoder network \n",
    "autoencoder = Model(input_layer, output_layer) \n",
    "autoencoder.compile(optimizer =\"adadelta\", loss =\"mse\") \n",
    "# Training the Auto-encoder network \n",
    "autoencoder.fit(X_normal_scaled, X_normal_scaled,  \n",
    "                batch_size = 16, epochs = 10,  \n",
    "                shuffle = True, validation_split = 0.20) \n",
    "\n",
    "hidden_representation = Sequential() \n",
    "hidden_representation.add(autoencoder.layers[0]) \n",
    "hidden_representation.add(autoencoder.layers[1]) \n",
    "hidden_representation.add(autoencoder.layers[2]) \n",
    "hidden_representation.add(autoencoder.layers[3]) \n",
    "hidden_representation.add(autoencoder.layers[4]) \n",
    "\n",
    "# Separating the points encoded by the Auto-encoder as normal and fraud \n",
    "decoded_X_normal = hidden_representation.predict(X_normal_scaled)\n",
    "decoded_X_deseased = hidden_representation.predict(X_deseased_scaled)\n",
    "# Combining the encoded points into a single table  \n",
    "encoded_X = np.append(decoded_X_normal, decoded_X_deseased, axis = 0) \n",
    "y_normal = np.zeros(decoded_X_normal.shape[0]) \n",
    "y_deceased = np.ones(decoded_X_deseased.shape[0]) \n",
    "encoded_y = np.append(y_normal, y_deceased) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second deep autoencoder NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_61\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_36 (InputLayer)        (None, 115)               0         \n",
      "_________________________________________________________________\n",
      "dense_136 (Dense)            (None, 6)                 696       \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 6)                 24        \n",
      "_________________________________________________________________\n",
      "dense_137 (Dense)            (None, 3)                 21        \n",
      "_________________________________________________________________\n",
      "dense_138 (Dense)            (None, 6)                 24        \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 6)                 24        \n",
      "_________________________________________________________________\n",
      "dense_139 (Dense)            (None, 115)               805       \n",
      "=================================================================\n",
      "Total params: 1,594\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 24\n",
      "_________________________________________________________________\n",
      "Train on 109 samples, validate on 28 samples\n",
      "Epoch 1/300\n",
      "109/109 [==============================] - 2s 23ms/step - loss: 0.7005 - val_loss: 0.6895\n",
      "Epoch 2/300\n",
      "109/109 [==============================] - 0s 355us/step - loss: 0.6952 - val_loss: 0.6862\n",
      "Epoch 3/300\n",
      "109/109 [==============================] - 0s 311us/step - loss: 0.6870 - val_loss: 0.6828\n",
      "Epoch 4/300\n",
      "109/109 [==============================] - 0s 355us/step - loss: 0.6837 - val_loss: 0.6793\n",
      "Epoch 5/300\n",
      "109/109 [==============================] - 0s 312us/step - loss: 0.6773 - val_loss: 0.6758\n",
      "Epoch 6/300\n",
      "109/109 [==============================] - 0s 321us/step - loss: 0.6717 - val_loss: 0.6722\n",
      "Epoch 7/300\n",
      "109/109 [==============================] - 0s 393us/step - loss: 0.6671 - val_loss: 0.6685\n",
      "Epoch 8/300\n",
      "109/109 [==============================] - 0s 302us/step - loss: 0.6625 - val_loss: 0.6646\n",
      "Epoch 9/300\n",
      "109/109 [==============================] - 0s 339us/step - loss: 0.6556 - val_loss: 0.6604\n",
      "Epoch 10/300\n",
      "109/109 [==============================] - 0s 339us/step - loss: 0.6496 - val_loss: 0.6561\n",
      "Epoch 11/300\n",
      "109/109 [==============================] - 0s 413us/step - loss: 0.6426 - val_loss: 0.6516\n",
      "Epoch 12/300\n",
      "109/109 [==============================] - 0s 332us/step - loss: 0.6384 - val_loss: 0.6467\n",
      "Epoch 13/300\n",
      "109/109 [==============================] - 0s 348us/step - loss: 0.6303 - val_loss: 0.6416\n",
      "Epoch 14/300\n",
      "109/109 [==============================] - 0s 312us/step - loss: 0.6226 - val_loss: 0.6364\n",
      "Epoch 15/300\n",
      "109/109 [==============================] - 0s 366us/step - loss: 0.6173 - val_loss: 0.6308\n",
      "Epoch 16/300\n",
      "109/109 [==============================] - 0s 349us/step - loss: 0.6080 - val_loss: 0.6246\n",
      "Epoch 17/300\n",
      "109/109 [==============================] - 0s 311us/step - loss: 0.6021 - val_loss: 0.6183\n",
      "Epoch 18/300\n",
      "109/109 [==============================] - 0s 314us/step - loss: 0.5939 - val_loss: 0.6120\n",
      "Epoch 19/300\n",
      "109/109 [==============================] - 0s 357us/step - loss: 0.5848 - val_loss: 0.6056\n",
      "Epoch 20/300\n",
      "109/109 [==============================] - 0s 347us/step - loss: 0.5756 - val_loss: 0.5991\n",
      "Epoch 21/300\n",
      "109/109 [==============================] - 0s 339us/step - loss: 0.5659 - val_loss: 0.5923\n",
      "Epoch 22/300\n",
      "109/109 [==============================] - 0s 355us/step - loss: 0.5574 - val_loss: 0.5850\n",
      "Epoch 23/300\n",
      "109/109 [==============================] - 0s 311us/step - loss: 0.5492 - val_loss: 0.5775\n",
      "Epoch 24/300\n",
      "109/109 [==============================] - 0s 335us/step - loss: 0.5367 - val_loss: 0.5701\n",
      "Epoch 25/300\n",
      "109/109 [==============================] - 0s 356us/step - loss: 0.5261 - val_loss: 0.5623\n",
      "Epoch 26/300\n",
      "109/109 [==============================] - 0s 357us/step - loss: 0.5159 - val_loss: 0.5544\n",
      "Epoch 27/300\n",
      "109/109 [==============================] - 0s 357us/step - loss: 0.5058 - val_loss: 0.5463\n",
      "Epoch 28/300\n",
      "109/109 [==============================] - 0s 346us/step - loss: 0.4974 - val_loss: 0.5381\n",
      "Epoch 29/300\n",
      "109/109 [==============================] - 0s 349us/step - loss: 0.4870 - val_loss: 0.5293\n",
      "Epoch 30/300\n",
      "109/109 [==============================] - 0s 348us/step - loss: 0.4748 - val_loss: 0.5205\n",
      "Epoch 31/300\n",
      "109/109 [==============================] - 0s 381us/step - loss: 0.4647 - val_loss: 0.5122\n",
      "Epoch 32/300\n",
      "109/109 [==============================] - 0s 357us/step - loss: 0.4512 - val_loss: 0.5038\n",
      "Epoch 33/300\n",
      "109/109 [==============================] - 0s 339us/step - loss: 0.4409 - val_loss: 0.4947\n",
      "Epoch 34/300\n",
      "109/109 [==============================] - 0s 288us/step - loss: 0.4296 - val_loss: 0.4858\n",
      "Epoch 35/300\n",
      "109/109 [==============================] - 0s 339us/step - loss: 0.4196 - val_loss: 0.4770\n",
      "Epoch 36/300\n",
      "109/109 [==============================] - 0s 398us/step - loss: 0.4064 - val_loss: 0.4684\n",
      "Epoch 37/300\n",
      "109/109 [==============================] - 0s 348us/step - loss: 0.3946 - val_loss: 0.4601\n",
      "Epoch 38/300\n",
      "109/109 [==============================] - 0s 554us/step - loss: 0.3837 - val_loss: 0.4516\n",
      "Epoch 39/300\n",
      "109/109 [==============================] - 0s 394us/step - loss: 0.3738 - val_loss: 0.4433\n",
      "Epoch 40/300\n",
      "109/109 [==============================] - 0s 336us/step - loss: 0.3627 - val_loss: 0.4353\n",
      "Epoch 41/300\n",
      "109/109 [==============================] - 0s 403us/step - loss: 0.3501 - val_loss: 0.4266\n",
      "Epoch 42/300\n",
      "109/109 [==============================] - 0s 397us/step - loss: 0.3382 - val_loss: 0.4179\n",
      "Epoch 43/300\n",
      "109/109 [==============================] - 0s 312us/step - loss: 0.3326 - val_loss: 0.4094\n",
      "Epoch 44/300\n",
      "109/109 [==============================] - 0s 329us/step - loss: 0.3138 - val_loss: 0.4011\n",
      "Epoch 45/300\n",
      "109/109 [==============================] - 0s 315us/step - loss: 0.3083 - val_loss: 0.3924\n",
      "Epoch 46/300\n",
      "109/109 [==============================] - 0s 330us/step - loss: 0.2956 - val_loss: 0.3838\n",
      "Epoch 47/300\n",
      "109/109 [==============================] - 0s 384us/step - loss: 0.2853 - val_loss: 0.3761\n",
      "Epoch 48/300\n",
      "109/109 [==============================] - 0s 385us/step - loss: 0.2730 - val_loss: 0.3694\n",
      "Epoch 49/300\n",
      "109/109 [==============================] - 0s 412us/step - loss: 0.2647 - val_loss: 0.3630\n",
      "Epoch 50/300\n",
      "109/109 [==============================] - 0s 377us/step - loss: 0.2561 - val_loss: 0.3568\n",
      "Epoch 51/300\n",
      "109/109 [==============================] - 0s 376us/step - loss: 0.2403 - val_loss: 0.3501\n",
      "Epoch 52/300\n",
      "109/109 [==============================] - 0s 265us/step - loss: 0.2341 - val_loss: 0.3434\n",
      "Epoch 53/300\n",
      "109/109 [==============================] - 0s 278us/step - loss: 0.2251 - val_loss: 0.3363\n",
      "Epoch 54/300\n",
      "109/109 [==============================] - 0s 284us/step - loss: 0.2176 - val_loss: 0.3300\n",
      "Epoch 55/300\n",
      "109/109 [==============================] - 0s 302us/step - loss: 0.2077 - val_loss: 0.3236\n",
      "Epoch 56/300\n",
      "109/109 [==============================] - 0s 289us/step - loss: 0.1956 - val_loss: 0.3180\n",
      "Epoch 57/300\n",
      "109/109 [==============================] - 0s 274us/step - loss: 0.1866 - val_loss: 0.3126\n",
      "Epoch 58/300\n",
      "109/109 [==============================] - 0s 274us/step - loss: 0.1756 - val_loss: 0.3070\n",
      "Epoch 59/300\n",
      "109/109 [==============================] - 0s 266us/step - loss: 0.1706 - val_loss: 0.3017\n",
      "Epoch 60/300\n",
      "109/109 [==============================] - 0s 311us/step - loss: 0.1626 - val_loss: 0.2964\n",
      "Epoch 61/300\n",
      "109/109 [==============================] - 0s 265us/step - loss: 0.1561 - val_loss: 0.2918\n",
      "Epoch 62/300\n",
      "109/109 [==============================] - 0s 271us/step - loss: 0.1422 - val_loss: 0.2874\n",
      "Epoch 63/300\n",
      "109/109 [==============================] - 0s 311us/step - loss: 0.1384 - val_loss: 0.2826\n",
      "Epoch 64/300\n",
      "109/109 [==============================] - 0s 350us/step - loss: 0.1284 - val_loss: 0.2782\n",
      "Epoch 65/300\n",
      "109/109 [==============================] - 0s 348us/step - loss: 0.1218 - val_loss: 0.2726\n",
      "Epoch 66/300\n",
      "109/109 [==============================] - 0s 293us/step - loss: 0.1148 - val_loss: 0.2676\n",
      "Epoch 67/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109/109 [==============================] - 0s 295us/step - loss: 0.1040 - val_loss: 0.2629\n",
      "Epoch 68/300\n",
      "109/109 [==============================] - 0s 284us/step - loss: 0.0969 - val_loss: 0.2584\n",
      "Epoch 69/300\n",
      "109/109 [==============================] - 0s 301us/step - loss: 0.0912 - val_loss: 0.2548\n",
      "Epoch 70/300\n",
      "109/109 [==============================] - 0s 288us/step - loss: 0.0809 - val_loss: 0.2513\n",
      "Epoch 71/300\n",
      "109/109 [==============================] - 0s 283us/step - loss: 0.0766 - val_loss: 0.2485\n",
      "Epoch 72/300\n",
      "109/109 [==============================] - 0s 356us/step - loss: 0.0682 - val_loss: 0.2451\n",
      "Epoch 73/300\n",
      "109/109 [==============================] - 0s 269us/step - loss: 0.0628 - val_loss: 0.2415\n",
      "Epoch 74/300\n",
      "109/109 [==============================] - 0s 284us/step - loss: 0.0599 - val_loss: 0.2395\n",
      "Epoch 75/300\n",
      "109/109 [==============================] - 0s 293us/step - loss: 0.0508 - val_loss: 0.2372\n",
      "Epoch 76/300\n",
      "109/109 [==============================] - 0s 370us/step - loss: 0.0476 - val_loss: 0.2352\n",
      "Epoch 77/300\n",
      "109/109 [==============================] - 0s 320us/step - loss: 0.0367 - val_loss: 0.2320\n",
      "Epoch 78/300\n",
      "109/109 [==============================] - 0s 283us/step - loss: 0.0312 - val_loss: 0.2286\n",
      "Epoch 79/300\n",
      "109/109 [==============================] - 0s 280us/step - loss: 0.0240 - val_loss: 0.2257\n",
      "Epoch 80/300\n",
      "109/109 [==============================] - 0s 302us/step - loss: 0.0184 - val_loss: 0.2242\n",
      "Epoch 81/300\n",
      "109/109 [==============================] - 0s 311us/step - loss: 0.0113 - val_loss: 0.2229\n",
      "Epoch 82/300\n",
      "109/109 [==============================] - 0s 267us/step - loss: 0.0041 - val_loss: 0.2212\n",
      "Epoch 83/300\n",
      "109/109 [==============================] - 0s 312us/step - loss: 0.0011 - val_loss: 0.2195\n",
      "Epoch 84/300\n",
      "109/109 [==============================] - 0s 302us/step - loss: -0.0063 - val_loss: 0.2180\n",
      "Epoch 85/300\n",
      "109/109 [==============================] - 0s 339us/step - loss: -0.0065 - val_loss: 0.2165\n",
      "Epoch 86/300\n",
      "109/109 [==============================] - 0s 326us/step - loss: -0.0193 - val_loss: 0.2152\n",
      "Epoch 87/300\n",
      "109/109 [==============================] - 0s 335us/step - loss: -0.0225 - val_loss: 0.2138\n",
      "Epoch 88/300\n",
      "109/109 [==============================] - 0s 274us/step - loss: -0.0312 - val_loss: 0.2126\n",
      "Epoch 89/300\n",
      "109/109 [==============================] - 0s 287us/step - loss: -0.0273 - val_loss: 0.2119\n",
      "Epoch 90/300\n",
      "109/109 [==============================] - 0s 256us/step - loss: -0.0404 - val_loss: 0.2110\n",
      "Epoch 91/300\n",
      "109/109 [==============================] - 0s 293us/step - loss: -0.0403 - val_loss: 0.2094\n",
      "Epoch 92/300\n",
      "109/109 [==============================] - 0s 265us/step - loss: -0.0472 - val_loss: 0.2076\n",
      "Epoch 93/300\n",
      "109/109 [==============================] - 0s 275us/step - loss: -0.0566 - val_loss: 0.2069\n",
      "Epoch 94/300\n",
      "109/109 [==============================] - 0s 283us/step - loss: -0.0615 - val_loss: 0.2062\n",
      "Epoch 95/300\n",
      "109/109 [==============================] - 0s 283us/step - loss: -0.0621 - val_loss: 0.2057\n",
      "Epoch 96/300\n",
      "109/109 [==============================] - 0s 284us/step - loss: -0.0640 - val_loss: 0.2043\n",
      "Epoch 97/300\n",
      "109/109 [==============================] - 0s 311us/step - loss: -0.0747 - val_loss: 0.2035\n",
      "Epoch 98/300\n",
      "109/109 [==============================] - 0s 366us/step - loss: -0.0803 - val_loss: 0.2033\n",
      "Epoch 99/300\n",
      "109/109 [==============================] - 0s 341us/step - loss: -0.0897 - val_loss: 0.2033\n",
      "Epoch 100/300\n",
      "109/109 [==============================] - 0s 311us/step - loss: -0.0872 - val_loss: 0.2027\n",
      "Epoch 101/300\n",
      "109/109 [==============================] - 0s 347us/step - loss: -0.0949 - val_loss: 0.2026\n",
      "Epoch 102/300\n",
      "109/109 [==============================] - 0s 399us/step - loss: -0.0959 - val_loss: 0.2024\n",
      "Epoch 103/300\n",
      "109/109 [==============================] - 0s 402us/step - loss: -0.1085 - val_loss: 0.2024\n",
      "Epoch 104/300\n",
      "109/109 [==============================] - 0s 323us/step - loss: -0.1100 - val_loss: 0.2021\n",
      "Epoch 105/300\n",
      "109/109 [==============================] - 0s 365us/step - loss: -0.1118 - val_loss: 0.2022\n",
      "Epoch 106/300\n",
      "109/109 [==============================] - 0s 347us/step - loss: -0.1172 - val_loss: 0.2023\n",
      "Epoch 107/300\n",
      "109/109 [==============================] - 0s 339us/step - loss: -0.1284 - val_loss: 0.2032\n",
      "Epoch 108/300\n",
      "109/109 [==============================] - 0s 338us/step - loss: -0.1317 - val_loss: 0.2034\n",
      "Epoch 109/300\n",
      "109/109 [==============================] - 0s 325us/step - loss: -0.1365 - val_loss: 0.2033\n",
      "Epoch 110/300\n",
      "109/109 [==============================] - 0s 302us/step - loss: -0.1404 - val_loss: 0.2038\n",
      "Epoch 111/300\n",
      "109/109 [==============================] - 0s 421us/step - loss: -0.1463 - val_loss: 0.2052\n",
      "Epoch 112/300\n",
      "109/109 [==============================] - 0s 362us/step - loss: -0.1520 - val_loss: 0.2060\n",
      "Epoch 113/300\n",
      "109/109 [==============================] - 0s 302us/step - loss: -0.1546 - val_loss: 0.2063\n",
      "Epoch 114/300\n",
      "109/109 [==============================] - 0s 345us/step - loss: -0.1592 - val_loss: 0.2066\n",
      "Epoch 115/300\n",
      "109/109 [==============================] - 0s 311us/step - loss: -0.1599 - val_loss: 0.2066\n",
      "Epoch 116/300\n",
      "109/109 [==============================] - ETA: 0s - loss: -0.15 - 0s 293us/step - loss: -0.1728 - val_loss: 0.2062\n",
      "Epoch 117/300\n",
      "109/109 [==============================] - 0s 304us/step - loss: -0.1745 - val_loss: 0.2070\n",
      "Epoch 118/300\n",
      "109/109 [==============================] - 0s 321us/step - loss: -0.1737 - val_loss: 0.2075\n",
      "Epoch 119/300\n",
      "109/109 [==============================] - 0s 321us/step - loss: -0.1848 - val_loss: 0.2080\n",
      "Epoch 120/300\n",
      "109/109 [==============================] - 0s 351us/step - loss: -0.1858 - val_loss: 0.2093\n",
      "Epoch 121/300\n",
      "109/109 [==============================] - 0s 348us/step - loss: -0.1983 - val_loss: 0.2097\n",
      "Epoch 122/300\n",
      "109/109 [==============================] - 0s 296us/step - loss: -0.2022 - val_loss: 0.2106\n",
      "Epoch 123/300\n",
      "109/109 [==============================] - 0s 312us/step - loss: -0.1952 - val_loss: 0.2136\n",
      "Epoch 124/300\n",
      "109/109 [==============================] - 0s 320us/step - loss: -0.2069 - val_loss: 0.2169\n",
      "Epoch 125/300\n",
      "109/109 [==============================] - 0s 289us/step - loss: -0.2088 - val_loss: 0.2199\n",
      "Epoch 126/300\n",
      "109/109 [==============================] - 0s 339us/step - loss: -0.2197 - val_loss: 0.2214\n",
      "Epoch 127/300\n",
      "109/109 [==============================] - 0s 398us/step - loss: -0.2220 - val_loss: 0.2222\n",
      "Epoch 128/300\n",
      "109/109 [==============================] - 0s 284us/step - loss: -0.2315 - val_loss: 0.2218\n",
      "Epoch 129/300\n",
      "109/109 [==============================] - 0s 275us/step - loss: -0.2382 - val_loss: 0.2217\n",
      "Epoch 130/300\n",
      "109/109 [==============================] - 0s 362us/step - loss: -0.2444 - val_loss: 0.2223\n",
      "Epoch 131/300\n",
      "109/109 [==============================] - 0s 364us/step - loss: -0.2413 - val_loss: 0.2228\n",
      "Epoch 132/300\n",
      "109/109 [==============================] - 0s 320us/step - loss: -0.2497 - val_loss: 0.2231\n",
      "Epoch 133/300\n",
      "109/109 [==============================] - 0s 286us/step - loss: -0.2506 - val_loss: 0.2242\n",
      "Epoch 134/300\n",
      "109/109 [==============================] - 0s 284us/step - loss: -0.2634 - val_loss: 0.2246\n",
      "Epoch 135/300\n",
      "109/109 [==============================] - 0s 329us/step - loss: -0.2549 - val_loss: 0.2273\n",
      "Epoch 136/300\n",
      "109/109 [==============================] - 0s 276us/step - loss: -0.2711 - val_loss: 0.2295\n",
      "Epoch 137/300\n",
      "109/109 [==============================] - 0s 311us/step - loss: -0.2749 - val_loss: 0.2327\n",
      "Epoch 138/300\n",
      "109/109 [==============================] - 0s 362us/step - loss: -0.2829 - val_loss: 0.2352\n",
      "Epoch 139/300\n",
      "109/109 [==============================] - 0s 284us/step - loss: -0.2941 - val_loss: 0.2369\n",
      "Epoch 140/300\n",
      "109/109 [==============================] - 0s 320us/step - loss: -0.2948 - val_loss: 0.2394\n",
      "Epoch 141/300\n",
      "109/109 [==============================] - 0s 353us/step - loss: -0.2950 - val_loss: 0.2407\n",
      "Epoch 142/300\n",
      "109/109 [==============================] - 0s 299us/step - loss: -0.3026 - val_loss: 0.2416\n",
      "Epoch 143/300\n",
      "109/109 [==============================] - 0s 293us/step - loss: -0.3097 - val_loss: 0.2428\n",
      "Epoch 144/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109/109 [==============================] - 0s 329us/step - loss: -0.2950 - val_loss: 0.2424\n",
      "Epoch 145/300\n",
      "109/109 [==============================] - 0s 316us/step - loss: -0.3142 - val_loss: 0.2431\n",
      "Epoch 146/300\n",
      "109/109 [==============================] - 0s 336us/step - loss: -0.3204 - val_loss: 0.2450\n",
      "Epoch 147/300\n",
      "109/109 [==============================] - 0s 328us/step - loss: -0.3264 - val_loss: 0.2477\n",
      "Epoch 148/300\n",
      "109/109 [==============================] - 0s 302us/step - loss: -0.3283 - val_loss: 0.2493\n",
      "Epoch 149/300\n",
      "109/109 [==============================] - 0s 326us/step - loss: -0.3386 - val_loss: 0.2512\n",
      "Epoch 150/300\n",
      "109/109 [==============================] - 0s 320us/step - loss: -0.3404 - val_loss: 0.2536\n",
      "Epoch 151/300\n",
      "109/109 [==============================] - 0s 293us/step - loss: -0.3576 - val_loss: 0.2568\n",
      "Epoch 152/300\n",
      "109/109 [==============================] - 0s 257us/step - loss: -0.3517 - val_loss: 0.2577\n",
      "Epoch 153/300\n",
      "109/109 [==============================] - 0s 329us/step - loss: -0.3638 - val_loss: 0.2589\n",
      "Epoch 154/300\n",
      "109/109 [==============================] - 0s 311us/step - loss: -0.3778 - val_loss: 0.2601\n",
      "Epoch 155/300\n",
      "109/109 [==============================] - 0s 329us/step - loss: -0.3808 - val_loss: 0.2609\n",
      "Epoch 156/300\n",
      "109/109 [==============================] - 0s 320us/step - loss: -0.3738 - val_loss: 0.2634\n",
      "Epoch 157/300\n",
      "109/109 [==============================] - 0s 294us/step - loss: -0.3731 - val_loss: 0.2667\n",
      "Epoch 158/300\n",
      "109/109 [==============================] - 0s 344us/step - loss: -0.3998 - val_loss: 0.2707\n",
      "Epoch 159/300\n",
      "109/109 [==============================] - 0s 440us/step - loss: -0.4024 - val_loss: 0.2751\n",
      "Epoch 160/300\n",
      "109/109 [==============================] - 0s 328us/step - loss: -0.4059 - val_loss: 0.2767\n",
      "Epoch 161/300\n",
      "109/109 [==============================] - 0s 393us/step - loss: -0.4070 - val_loss: 0.2753\n",
      "Epoch 162/300\n",
      "109/109 [==============================] - 0s 344us/step - loss: -0.4170 - val_loss: 0.2758\n",
      "Epoch 163/300\n",
      "109/109 [==============================] - 0s 329us/step - loss: -0.4187 - val_loss: 0.2776\n",
      "Epoch 164/300\n",
      "109/109 [==============================] - 0s 374us/step - loss: -0.4188 - val_loss: 0.2798\n",
      "Epoch 165/300\n",
      "109/109 [==============================] - 0s 348us/step - loss: -0.4212 - val_loss: 0.2835\n",
      "Epoch 166/300\n",
      "109/109 [==============================] - 0s 311us/step - loss: -0.4438 - val_loss: 0.2899\n",
      "Epoch 167/300\n",
      "109/109 [==============================] - 0s 336us/step - loss: -0.4546 - val_loss: 0.2942\n",
      "Epoch 168/300\n",
      "109/109 [==============================] - 0s 302us/step - loss: -0.4467 - val_loss: 0.2945\n",
      "Epoch 169/300\n",
      "109/109 [==============================] - 0s 302us/step - loss: -0.4637 - val_loss: 0.2962\n",
      "Epoch 170/300\n",
      "109/109 [==============================] - 0s 316us/step - loss: -0.4632 - val_loss: 0.3015\n",
      "Epoch 171/300\n",
      "109/109 [==============================] - 0s 320us/step - loss: -0.4776 - val_loss: 0.3077\n",
      "Epoch 172/300\n",
      "109/109 [==============================] - 0s 332us/step - loss: -0.4737 - val_loss: 0.3119\n",
      "Epoch 173/300\n",
      "109/109 [==============================] - 0s 329us/step - loss: -0.4824 - val_loss: 0.3165\n",
      "Epoch 174/300\n",
      "109/109 [==============================] - 0s 347us/step - loss: -0.4972 - val_loss: 0.3178\n",
      "Epoch 175/300\n",
      "109/109 [==============================] - 0s 319us/step - loss: -0.4875 - val_loss: 0.3196\n",
      "Epoch 176/300\n",
      "109/109 [==============================] - 0s 325us/step - loss: -0.5029 - val_loss: 0.3236\n",
      "Epoch 177/300\n",
      "109/109 [==============================] - 0s 320us/step - loss: -0.4955 - val_loss: 0.3259\n",
      "Epoch 178/300\n",
      "109/109 [==============================] - 0s 393us/step - loss: -0.5275 - val_loss: 0.3284\n",
      "Epoch 179/300\n",
      "109/109 [==============================] - 0s 284us/step - loss: -0.5253 - val_loss: 0.3309\n",
      "Epoch 180/300\n",
      "109/109 [==============================] - 0s 313us/step - loss: -0.5063 - val_loss: 0.3295\n",
      "Epoch 181/300\n",
      "109/109 [==============================] - 0s 291us/step - loss: -0.5410 - val_loss: 0.3340\n",
      "Epoch 182/300\n",
      "109/109 [==============================] - 0s 284us/step - loss: -0.5329 - val_loss: 0.3406\n",
      "Epoch 183/300\n",
      "109/109 [==============================] - 0s 335us/step - loss: -0.5451 - val_loss: 0.3446\n",
      "Epoch 184/300\n",
      "109/109 [==============================] - 0s 355us/step - loss: -0.5406 - val_loss: 0.3493\n",
      "Epoch 185/300\n",
      "109/109 [==============================] - 0s 283us/step - loss: -0.5647 - val_loss: 0.3484\n",
      "Epoch 186/300\n",
      "109/109 [==============================] - 0s 302us/step - loss: -0.5719 - val_loss: 0.3483\n",
      "Epoch 187/300\n",
      "109/109 [==============================] - 0s 284us/step - loss: -0.5738 - val_loss: 0.3468\n",
      "Epoch 188/300\n",
      "109/109 [==============================] - 0s 320us/step - loss: -0.5761 - val_loss: 0.3496\n",
      "Epoch 189/300\n",
      "109/109 [==============================] - 0s 311us/step - loss: -0.5833 - val_loss: 0.3538\n",
      "Epoch 190/300\n",
      "109/109 [==============================] - 0s 339us/step - loss: -0.5990 - val_loss: 0.3577\n",
      "Epoch 191/300\n",
      "109/109 [==============================] - 0s 354us/step - loss: -0.5956 - val_loss: 0.3571\n",
      "Epoch 192/300\n",
      "109/109 [==============================] - 0s 293us/step - loss: -0.5842 - val_loss: 0.3586\n",
      "Epoch 193/300\n",
      "109/109 [==============================] - 0s 377us/step - loss: -0.5965 - val_loss: 0.3513\n",
      "Epoch 194/300\n",
      "109/109 [==============================] - 0s 291us/step - loss: -0.6199 - val_loss: 0.3564\n",
      "Epoch 195/300\n",
      "109/109 [==============================] - 0s 293us/step - loss: -0.6208 - val_loss: 0.3578\n",
      "Epoch 196/300\n",
      "109/109 [==============================] - 0s 302us/step - loss: -0.6193 - val_loss: 0.3577\n",
      "Epoch 197/300\n",
      "109/109 [==============================] - 0s 301us/step - loss: -0.6372 - val_loss: 0.3594\n",
      "Epoch 198/300\n",
      "109/109 [==============================] - 0s 266us/step - loss: -0.6509 - val_loss: 0.3616\n",
      "Epoch 199/300\n",
      "109/109 [==============================] - 0s 246us/step - loss: -0.6443 - val_loss: 0.3590\n",
      "Epoch 200/300\n",
      "109/109 [==============================] - 0s 244us/step - loss: -0.6265 - val_loss: 0.3605\n",
      "Epoch 201/300\n",
      "109/109 [==============================] - 0s 284us/step - loss: -0.6332 - val_loss: 0.3653\n",
      "Epoch 202/300\n",
      "109/109 [==============================] - 0s 320us/step - loss: -0.6540 - val_loss: 0.3638\n",
      "Epoch 203/300\n",
      "109/109 [==============================] - 0s 302us/step - loss: -0.6565 - val_loss: 0.3610\n",
      "Epoch 204/300\n",
      "109/109 [==============================] - 0s 302us/step - loss: -0.6724 - val_loss: 0.3578\n",
      "Epoch 205/300\n",
      "109/109 [==============================] - 0s 293us/step - loss: -0.6888 - val_loss: 0.3575\n",
      "Epoch 206/300\n",
      "109/109 [==============================] - 0s 302us/step - loss: -0.6679 - val_loss: 0.3595\n",
      "Epoch 207/300\n",
      "109/109 [==============================] - 0s 311us/step - loss: -0.6807 - val_loss: 0.3620\n",
      "Epoch 208/300\n",
      "109/109 [==============================] - 0s 288us/step - loss: -0.6633 - val_loss: 0.3683\n",
      "Epoch 209/300\n",
      "109/109 [==============================] - 0s 311us/step - loss: -0.7107 - val_loss: 0.3660\n",
      "Epoch 210/300\n",
      "109/109 [==============================] - 0s 329us/step - loss: -0.7058 - val_loss: 0.3689\n",
      "Epoch 211/300\n",
      "109/109 [==============================] - 0s 306us/step - loss: -0.7098 - val_loss: 0.3735\n",
      "Epoch 212/300\n",
      "109/109 [==============================] - 0s 274us/step - loss: -0.7116 - val_loss: 0.3676\n",
      "Epoch 213/300\n",
      "109/109 [==============================] - 0s 302us/step - loss: -0.7274 - val_loss: 0.3745\n",
      "Epoch 214/300\n",
      "109/109 [==============================] - 0s 343us/step - loss: -0.7134 - val_loss: 0.3769\n",
      "Epoch 215/300\n",
      "109/109 [==============================] - 0s 284us/step - loss: -0.7459 - val_loss: 0.3812\n",
      "Epoch 216/300\n",
      "109/109 [==============================] - 0s 290us/step - loss: -0.7433 - val_loss: 0.3829\n",
      "Epoch 217/300\n",
      "109/109 [==============================] - 0s 293us/step - loss: -0.7527 - val_loss: 0.3802\n",
      "Epoch 218/300\n",
      "109/109 [==============================] - 0s 312us/step - loss: -0.7134 - val_loss: 0.3795\n",
      "Epoch 219/300\n",
      "109/109 [==============================] - 0s 357us/step - loss: -0.7585 - val_loss: 0.3875\n",
      "Epoch 220/300\n",
      "109/109 [==============================] - 0s 284us/step - loss: -0.7438 - val_loss: 0.3892\n",
      "Epoch 221/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109/109 [==============================] - 0s 292us/step - loss: -0.7714 - val_loss: 0.3868\n",
      "Epoch 222/300\n",
      "109/109 [==============================] - 0s 305us/step - loss: -0.7665 - val_loss: 0.3919\n",
      "Epoch 223/300\n",
      "109/109 [==============================] - 0s 339us/step - loss: -0.7685 - val_loss: 0.3942\n",
      "Epoch 224/300\n",
      "109/109 [==============================] - 0s 357us/step - loss: -0.7658 - val_loss: 0.3892\n",
      "Epoch 225/300\n",
      "109/109 [==============================] - 0s 295us/step - loss: -0.7849 - val_loss: 0.3950\n",
      "Epoch 226/300\n",
      "109/109 [==============================] - 0s 302us/step - loss: -0.7671 - val_loss: 0.3932\n",
      "Epoch 227/300\n",
      "109/109 [==============================] - 0s 298us/step - loss: -0.7799 - val_loss: 0.3930\n",
      "Epoch 228/300\n",
      "109/109 [==============================] - 0s 308us/step - loss: -0.7753 - val_loss: 0.3949\n",
      "Epoch 229/300\n",
      "109/109 [==============================] - 0s 329us/step - loss: -0.7910 - val_loss: 0.3893\n",
      "Epoch 230/300\n",
      "109/109 [==============================] - 0s 298us/step - loss: -0.8058 - val_loss: 0.3945\n",
      "Epoch 231/300\n",
      "109/109 [==============================] - 0s 311us/step - loss: -0.7831 - val_loss: 0.3983\n",
      "Epoch 232/300\n",
      "109/109 [==============================] - 0s 311us/step - loss: -0.8064 - val_loss: 0.3869\n",
      "Epoch 233/300\n",
      "109/109 [==============================] - 0s 291us/step - loss: -0.8109 - val_loss: 0.3902\n",
      "Epoch 234/300\n",
      "109/109 [==============================] - 0s 346us/step - loss: -0.7874 - val_loss: 0.3855\n",
      "Epoch 235/300\n",
      "109/109 [==============================] - 0s 293us/step - loss: -0.8239 - val_loss: 0.3895\n",
      "Epoch 236/300\n",
      "109/109 [==============================] - 0s 302us/step - loss: -0.8097 - val_loss: 0.3840\n",
      "Epoch 237/300\n",
      "109/109 [==============================] - 0s 312us/step - loss: -0.8185 - val_loss: 0.3853\n",
      "Epoch 238/300\n",
      "109/109 [==============================] - 0s 312us/step - loss: -0.8236 - val_loss: 0.3768\n",
      "Epoch 239/300\n",
      "109/109 [==============================] - 0s 325us/step - loss: -0.8135 - val_loss: 0.3724\n",
      "Epoch 240/300\n",
      "109/109 [==============================] - 0s 302us/step - loss: -0.8042 - val_loss: 0.3722\n",
      "Epoch 241/300\n",
      "109/109 [==============================] - 0s 289us/step - loss: -0.8349 - val_loss: 0.3723\n",
      "Epoch 242/300\n",
      "109/109 [==============================] - 0s 301us/step - loss: -0.8331 - val_loss: 0.3762\n",
      "Epoch 243/300\n",
      "109/109 [==============================] - 0s 311us/step - loss: -0.8276 - val_loss: 0.3701\n",
      "Epoch 244/300\n",
      "109/109 [==============================] - 0s 325us/step - loss: -0.8502 - val_loss: 0.3712\n",
      "Epoch 245/300\n",
      "109/109 [==============================] - 0s 347us/step - loss: -0.8550 - val_loss: 0.3664\n",
      "Epoch 246/300\n",
      "109/109 [==============================] - 0s 339us/step - loss: -0.8508 - val_loss: 0.3654\n",
      "Epoch 247/300\n",
      "109/109 [==============================] - 0s 285us/step - loss: -0.8459 - val_loss: 0.3622\n",
      "Epoch 248/300\n",
      "109/109 [==============================] - 0s 311us/step - loss: -0.8374 - val_loss: 0.3539\n",
      "Epoch 249/300\n",
      "109/109 [==============================] - 0s 319us/step - loss: -0.8657 - val_loss: 0.3556\n",
      "Epoch 250/300\n",
      "109/109 [==============================] - 0s 301us/step - loss: -0.8374 - val_loss: 0.3567\n",
      "Epoch 251/300\n",
      "109/109 [==============================] - 0s 339us/step - loss: -0.8392 - val_loss: 0.3516\n",
      "Epoch 252/300\n",
      "109/109 [==============================] - 0s 307us/step - loss: -0.8490 - val_loss: 0.3488\n",
      "Epoch 253/300\n",
      "109/109 [==============================] - 0s 293us/step - loss: -0.8733 - val_loss: 0.3427\n",
      "Epoch 254/300\n",
      "109/109 [==============================] - 0s 329us/step - loss: -0.8588 - val_loss: 0.3422\n",
      "Epoch 255/300\n",
      "109/109 [==============================] - 0s 303us/step - loss: -0.8679 - val_loss: 0.3423\n",
      "Epoch 256/300\n",
      "109/109 [==============================] - 0s 310us/step - loss: -0.8501 - val_loss: 0.3420\n",
      "Epoch 257/300\n",
      "109/109 [==============================] - 0s 302us/step - loss: -0.8856 - val_loss: 0.3393\n",
      "Epoch 258/300\n",
      "109/109 [==============================] - 0s 332us/step - loss: -0.8526 - val_loss: 0.3371\n",
      "Epoch 259/300\n",
      "109/109 [==============================] - 0s 311us/step - loss: -0.8933 - val_loss: 0.3365\n",
      "Epoch 260/300\n",
      "109/109 [==============================] - 0s 293us/step - loss: -0.8761 - val_loss: 0.3404\n",
      "Epoch 261/300\n",
      "109/109 [==============================] - 0s 320us/step - loss: -0.8551 - val_loss: 0.3413\n",
      "Epoch 262/300\n",
      "109/109 [==============================] - 0s 348us/step - loss: -0.8928 - val_loss: 0.3429\n",
      "Epoch 263/300\n",
      "109/109 [==============================] - 0s 293us/step - loss: -0.8894 - val_loss: 0.3449\n",
      "Epoch 264/300\n",
      "109/109 [==============================] - 0s 329us/step - loss: -0.8753 - val_loss: 0.3374\n",
      "Epoch 265/300\n",
      "109/109 [==============================] - 0s 289us/step - loss: -0.8770 - val_loss: 0.3344\n",
      "Epoch 266/300\n",
      "109/109 [==============================] - 0s 311us/step - loss: -0.8901 - val_loss: 0.3335\n",
      "Epoch 267/300\n",
      "109/109 [==============================] - 0s 293us/step - loss: -0.8615 - val_loss: 0.3340\n",
      "Epoch 268/300\n",
      "109/109 [==============================] - 0s 302us/step - loss: -0.9282 - val_loss: 0.3312\n",
      "Epoch 269/300\n",
      "109/109 [==============================] - 0s 283us/step - loss: -0.8992 - val_loss: 0.3257\n",
      "Epoch 270/300\n",
      "109/109 [==============================] - 0s 302us/step - loss: -0.9180 - val_loss: 0.3226\n",
      "Epoch 271/300\n",
      "109/109 [==============================] - 0s 302us/step - loss: -0.8880 - val_loss: 0.3210\n",
      "Epoch 272/300\n",
      "109/109 [==============================] - 0s 330us/step - loss: -0.9149 - val_loss: 0.3160\n",
      "Epoch 273/300\n",
      "109/109 [==============================] - 0s 347us/step - loss: -0.9194 - val_loss: 0.3150\n",
      "Epoch 274/300\n",
      "109/109 [==============================] - 0s 347us/step - loss: -0.9169 - val_loss: 0.3141\n",
      "Epoch 275/300\n",
      "109/109 [==============================] - 0s 329us/step - loss: -0.9324 - val_loss: 0.3131\n",
      "Epoch 276/300\n",
      "109/109 [==============================] - 0s 303us/step - loss: -0.9233 - val_loss: 0.3125\n",
      "Epoch 277/300\n",
      "109/109 [==============================] - 0s 297us/step - loss: -0.9201 - val_loss: 0.3123\n",
      "Epoch 278/300\n",
      "109/109 [==============================] - 0s 334us/step - loss: -0.9115 - val_loss: 0.3106\n",
      "Epoch 279/300\n",
      "109/109 [==============================] - 0s 335us/step - loss: -0.9297 - val_loss: 0.3090\n",
      "Epoch 280/300\n",
      "109/109 [==============================] - 0s 339us/step - loss: -0.9347 - val_loss: 0.3063\n",
      "Epoch 281/300\n",
      "109/109 [==============================] - 0s 357us/step - loss: -0.9423 - val_loss: 0.3049\n",
      "Epoch 282/300\n",
      "109/109 [==============================] - 0s 426us/step - loss: -0.9322 - val_loss: 0.3042\n",
      "Epoch 283/300\n",
      "109/109 [==============================] - 0s 329us/step - loss: -0.9563 - val_loss: 0.3031\n",
      "Epoch 284/300\n",
      "109/109 [==============================] - 0s 293us/step - loss: -0.9284 - val_loss: 0.3034\n",
      "Epoch 285/300\n",
      "109/109 [==============================] - 0s 311us/step - loss: -0.9605 - val_loss: 0.3001\n",
      "Epoch 286/300\n",
      "109/109 [==============================] - 0s 320us/step - loss: -0.9398 - val_loss: 0.2986\n",
      "Epoch 287/300\n",
      "109/109 [==============================] - 0s 274us/step - loss: -0.9574 - val_loss: 0.2991\n",
      "Epoch 288/300\n",
      "109/109 [==============================] - 0s 302us/step - loss: -0.9506 - val_loss: 0.2945\n",
      "Epoch 289/300\n",
      "109/109 [==============================] - 0s 329us/step - loss: -0.9452 - val_loss: 0.2928\n",
      "Epoch 290/300\n",
      "109/109 [==============================] - 0s 324us/step - loss: -0.9302 - val_loss: 0.2930\n",
      "Epoch 291/300\n",
      "109/109 [==============================] - 0s 256us/step - loss: -0.9590 - val_loss: 0.2872\n",
      "Epoch 292/300\n",
      "109/109 [==============================] - 0s 302us/step - loss: -0.9551 - val_loss: 0.2888\n",
      "Epoch 293/300\n",
      "109/109 [==============================] - ETA: 0s - loss: -0.84 - 0s 334us/step - loss: -0.9422 - val_loss: 0.2866\n",
      "Epoch 294/300\n",
      "109/109 [==============================] - 0s 292us/step - loss: -0.9519 - val_loss: 0.2847\n",
      "Epoch 295/300\n",
      "109/109 [==============================] - 0s 312us/step - loss: -0.9444 - val_loss: 0.2777\n",
      "Epoch 296/300\n",
      "109/109 [==============================] - 0s 302us/step - loss: -0.9655 - val_loss: 0.2738\n",
      "Epoch 297/300\n",
      "109/109 [==============================] - 0s 301us/step - loss: -0.9660 - val_loss: 0.2726\n",
      "Epoch 298/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109/109 [==============================] - 0s 303us/step - loss: -0.9685 - val_loss: 0.2716\n",
      "Epoch 299/300\n",
      "109/109 [==============================] - 0s 317us/step - loss: -0.9589 - val_loss: 0.2792\n",
      "Epoch 300/300\n",
      "109/109 [==============================] - 0s 366us/step - loss: -0.9707 - val_loss: 0.2773\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Building the Encoder network \n",
    "input_window = Input(shape=(window_length,))\n",
    "x = Dense(6, activation='relu')(input_window)\n",
    "x = BatchNormalization()(x)\n",
    "encoded = Dense(encoding_dim, activation='relu')(x)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "x = Dense(6, activation='relu')(encoded)\n",
    "x = BatchNormalization()(x)\n",
    "decoded = Dense(window_length, activation='sigmoid')(x)\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_window, decoded)\n",
    "\n",
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_window, encoded)\n",
    "autoencoder.summary()\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "autoencoder.fit(X_normal_scaled, X_normal_scaled,\n",
    "                epochs=epochs,\n",
    "                batch_size=16,\n",
    "                shuffle=True,\n",
    "                validation_split = 0.20)\n",
    "# Separating the points encoded by the Auto-encoder as normal and fraud \n",
    "decoded_X_normal = autoencoder.predict(X_normal_scaled)\n",
    "decoded_X_deseased = autoencoder.predict(X_deseased_scaled)\n",
    "# Combining the encoded points into a single table  \n",
    "encoded_X = np.append(decoded_X_normal, decoded_X_deseased, axis = 0) \n",
    "y_normal = np.zeros(decoded_X_normal.shape[0]) \n",
    "y_deceased = np.ones(decoded_X_deseased.shape[0]) \n",
    "encoded_y = np.append(y_normal, y_deceased) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting features and the label: 20% test data and 80% assigned to training data\n",
    "# split into train/test sets with same class ratio\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(encoded_X, encoded_y, test_size=0.2, random_state=5, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.bagging module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\Azadeh\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#balancing the data\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "# define oversampling strategy\n",
    "over = RandomOverSampler(sampling_strategy='minority')\n",
    "# fit and apply the transform\n",
    "X_train, y_train = over.fit_resample(X_train, y_train)\n",
    "# define undersampling strategy\n",
    "under = RandomUnderSampler(sampling_strategy='majority')\n",
    "# fit and apply the transform\n",
    "X_train, y_train = under.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 _train= 0.7990654205607477\n",
      "R^2 _test= 0.6764705882352942\n"
     ]
    }
   ],
   "source": [
    "#fitting the model and get the conversion probabilities. \n",
    "#predit_proba() function of our model assigns probability for each row:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(solver='lbfgs', penalty='l2',C= 1, max_iter=200, n_jobs=1, tol=0.0001)\n",
    "model.fit(X_train,y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "lr_probs = model.predict_proba(X_test)\n",
    "#Return the mean accuracy on the given test data and taraining data to see if we have overfitting.score clculates R^2\n",
    "print('R^2 _train=',model.score(X_train, y_train))\n",
    "print('R^2 _test=',model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy Scores [0.72727273 0.81818182 0.86363636 0.81818182 0.80952381 0.85714286\n",
      " 0.71428571 0.76190476 0.80952381 0.85714286]\n",
      "CV-scores_min =  0.7142857142857143\n",
      "CV_scores_mean = 0.8036796536796537\n",
      "CV_scores_max = 0.8636363636363636\n"
     ]
    }
   ],
   "source": [
    "#Cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(model, X_train, y_train, cv=10)\n",
    "print('Cross-Validation Accuracy Scores', scores)\n",
    "scores = pd.Series(scores)\n",
    "print('CV-scores_min = ',scores.min())\n",
    "print('CV_scores_mean =', scores.mean())\n",
    "print('CV_scores_max =', scores.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Actual  Predicted\n",
       "0      0.0        1.0\n",
       "1      0.0        0.0\n",
       "2      0.0        0.0\n",
       "3      0.0        1.0\n",
       "4      0.0        0.0\n",
       "5      0.0        0.0\n",
       "6      1.0        1.0\n",
       "7      0.0        0.0\n",
       "8      0.0        0.0\n",
       "9      1.0        1.0\n",
       "10     0.0        0.0\n",
       "11     0.0        1.0\n",
       "12     0.0        0.0\n",
       "13     0.0        0.0\n",
       "14     0.0        0.0\n",
       "15     0.0        1.0\n",
       "16     1.0        1.0\n",
       "17     0.0        0.0\n",
       "18     0.0        1.0\n",
       "19     0.0        0.0\n",
       "20     0.0        1.0\n",
       "21     0.0        1.0\n",
       "22     0.0        0.0\n",
       "23     0.0        0.0\n",
       "24     0.0        1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'Actual': y_test, 'Predicted': y_hat})\n",
    "df.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance measurement metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.3235294117647059\n",
      "Mean Squared Error: 0.3235294117647059\n",
      "Root Mean Squared Error: 0.568796458994521\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_hat))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_hat))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "# calculate precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score = 0.4210526315789474\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "# calculate F1 score\n",
    "f1 = f1_score(y_test, y_hat)\n",
    "print('f1 score =', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc = 0.85\n"
     ]
    }
   ],
   "source": [
    "#Area Under ROC Curve (AUROC) metric\n",
    "from sklearn.metrics import roc_auc_score\n",
    "lr_probs = model.predict_proba(X_test)\n",
    "# Keep only the positive class\n",
    "#lr_probs = [p[1] for p in lr_probs]\n",
    "lr_probs\n",
    "print( 'roc_auc =', roc_auc_score(y_test, lr_probs[:,1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Skill: ROC AUC=0.500\n",
      "Logistic: ROC AUC=0.850\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "ns_probs = [0 for _ in range(len(y_test))]\n",
    "# predict probabilities\n",
    "lr_probs = model.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 1]\n",
    "# calculate scores\n",
    "ns_auc = roc_auc_score(y_test, ns_probs)\n",
    "lr_auc = roc_auc_score(y_test, lr_probs)\n",
    "# summarize scores\n",
    "print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "print('Logistic: ROC AUC=%.3f' % (lr_auc))\n",
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
    "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "pyplot.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic: f1=0.421 auc=0.578\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import auc\n",
    "lr_probs = model.predict_proba(X_test)\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(y_test, lr_probs[:,1])\n",
    "lr_f1, lr_auc = f1_score(y_test, y_hat), auc(lr_recall, lr_precision)\n",
    "# summarize scores\n",
    "print('Logistic: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n",
    "# plot the precision-recall curves\n",
    "no_skill = len(y_test[y_test==1]) / len(y_test)\n",
    "pyplot.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "pyplot.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
